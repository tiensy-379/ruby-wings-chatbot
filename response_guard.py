#!/usr/bin/env python3
"""
response_guard.py v5.2 - Ruby Wings Chatbot

Enhanced "expert guard" to validate & format final answers before sending to user.
Fully integrated with entities.py v5.2 and tour_entities.json structure.

Responsibilities:
- Ensure answers cite sources (e.g., [1], [2]) or attach retrieved snippets if LLM hallucinated
- Ensure answer content is consistent with retrieved evidence (token overlap check)
- Ensure requested_field is respected (if provided) by preferring passages for that field
- Enforce friendly "healing travel" tone with sanitization heuristics
- Provide deterministic fallback using only retrieved passages when LLM output fails checks
- State-based response templates for different conversation stages
- Location-aware response formatting with region suggestions
- Tour response formatting with labels (üèÜ, ‚≠ê, üí∞)
- Intent-specific response templates

ƒê·ªíNG B·ªò: entities.py v5.2, knowledge.json, tour_entities.json, app.py v5.2

Usage:
  from response_guard import validate_and_format_answer
  out = validate_and_format_answer(
      llm_text=llm_text,
      top_passages=top_passages,            # List[Tuple[score, mapping_entry]]
      requested_field=requested_field,      # optional string
      tour_indices=tour_indices,            # optional list[int]
      max_tokens=700,
      context={}                            # conversation context
  )

Return value:
  {
    "answer": "<final text to send user>",
    "sources": ["root.tours[2].price", ...],
    "guard_passed": True/False,
    "reason": "ok" | "no_evidence" | "mismatch_field" | ...,
    "state": "explore" | "suggest" | ...,
    "tour_labels": [],
    "location_filtered": False
  }
"""

import re
import html
import time
import random
import logging
from typing import List, Tuple, Dict, Any, Optional, Union
from collections import Counter
from datetime import datetime

# Import from entities.py (ƒê·ªíNG B·ªò)
try:
    from entities import (
        ConversationStage,
        Intent,
        extract_location_from_query,
        get_region_from_location
    )
except ImportError:
    # Fallback definitions if entities.py not available
    logging.warning("‚ö†Ô∏è Could not import from entities.py, using fallback definitions")
    
    class ConversationStage:
        """Fallback ConversationStage"""
        EXPLORE = "explore"
        SUGGEST = "suggest"
        COMPARE = "compare"
        SELECT = "select"
        BOOK = "book"
        LEAD = "lead"
        CALLBACK = "callback"
    
    class Intent:
        """Fallback Intent"""
        PROVIDE_PHONE = "provide_phone"
        CALLBACK_REQUEST = "callback_request"
        BOOKING_CONFIRM = "booking_confirm"
        MODIFY_REQUEST = "modify_request"
        SMALLTALK = "smalltalk"
        LEAD_CAPTURED = "lead_captured"
        GREETING = "greeting"
        FAREWELL = "farewell"
        TOUR_INQUIRY = "tour_inquiry"
        TOUR_COMPARISON = "tour_comparison"
        TOUR_RECOMMENDATION = "tour_recommendation"
        PRICE_ASK = "price_ask"
        BOOKING_INQUIRY = "booking_inquiry"
        UNKNOWN = "unknown"
    
    def extract_location_from_query(text: str) -> Optional[str]:
        return None
    
    def get_region_from_location(location: str) -> Optional[str]:
        location_lower = location.lower()
        if any(k in location_lower for k in ["hu·∫ø", "qu·∫£ng tr·ªã", "b·∫°ch m√£", "ƒë√† n·∫µng", "h·ªôi an"]):
            return "Mi·ªÅn Trung"
        elif any(k in location_lower for k in ["h√† n·ªôi", "h·∫° long", "sapa", "ninh b√¨nh"]):
            return "Mi·ªÅn B·∫Øc"
        elif any(k in location_lower for k in ["h·ªì ch√≠ minh", "s√†i g√≤n", "c·∫ßn th∆°", "ph√∫ qu·ªëc"]):
            return "Mi·ªÅn Nam"
        return None

# Setup logging
logger = logging.getLogger("response_guard")

# ==================== CONSTANTS ====================

# Citation regex
SRC_RE = re.compile(r"\[\d+\]")  # detect [1], [2] style citations

# Guard parameters
MIN_OVERLAP_RATIO = 0.12   # minimal overlap between LLM text and evidence to accept
MIN_FIELD_MENTION_RATIO = 0.02  # small threshold to allow field-specific match via text overlap
MAX_ANSWER_CHARS = 1500
BANNED_PHRASES = ["i think", "i guess", "maybe", "probably", "as far as i know", "i'm not sure"]

# ==================== RESPONSE TEMPLATES ====================

# State-based templates (ƒê·ªíNG B·ªò V·ªöI ENTITIES.PY)
STATE_TEMPLATES = {
    ConversationStage.EXPLORE: [
        "T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n v·ªÅ tour du l·ªãch tr·∫£i nghi·ªám Ruby Wings? üåø",
        "B·∫°n mu·ªën t√¨m hi·ªÉu v·ªÅ tour du l·ªãch n√†o c·ªßa Ruby Wings? üòä",
        "Ch√†o b·∫°n! T√¥i c√≥ th·ªÉ t∆∞ v·∫•n cho b·∫°n v·ªÅ c√°c h√†nh tr√¨nh tr·∫£i nghi·ªám c·ªßa Ruby Wings.",
        "Ruby Wings c√≥ nhi·ªÅu tour tr·∫£i nghi·ªám ƒë·ªôc ƒë√°o. B·∫°n quan t√¢m ƒë·∫øn tour n√†o?"
    ],
    
    ConversationStage.SUGGEST: [
        "D·ª±a tr√™n y√™u c·∫ßu c·ªßa b·∫°n, t√¥i ƒë·ªÅ xu·∫•t c√°c tour sau:",
        "T√¥i t√¨m th·∫•y m·ªôt s·ªë tour ph√π h·ª£p v·ªõi b·∫°n:",
        "D∆∞·ªõi ƒë√¢y l√† c√°c tour Ruby Wings b·∫°n c√≥ th·ªÉ quan t√¢m:",
        "ƒê√¢y l√† nh·ªØng g·ª£i √Ω tour ph√π h·ª£p nh·∫•t cho b·∫°n:"
    ],
    
    ConversationStage.COMPARE: [
        "ƒê·ªÉ so s√°nh c√°c tour, t√¥i t√≥m t·∫Øt th√¥ng tin ch√≠nh:",
        "D∆∞·ªõi ƒë√¢y l√† th√¥ng tin so s√°nh gi·ªØa c√°c tour:",
        "T√¥i s·∫Ω gi√∫p b·∫°n so s√°nh c√°c tour ƒë·ªÉ ch·ªçn ph√π h·ª£p nh·∫•t:",
        "So s√°nh chi ti·∫øt c√°c tour:"
    ],
    
    ConversationStage.SELECT: [
        "B·∫°n ƒë√£ ch·ªçn tour **{tour_name}**. B·∫°n mu·ªën ƒë·∫∑t tour n√†y kh√¥ng?",
        "Tour **{tour_name}** r·∫•t ph√π h·ª£p v·ªõi b·∫°n! B·∫°n mu·ªën ti·∫øp t·ª•c ƒë·∫∑t tour kh√¥ng?",
        "Tuy·ªát v·ªùi! Tour **{tour_name}** ƒë√£ ƒë∆∞·ª£c ch·ªçn. B·∫°n c√≥ mu·ªën ƒë·∫∑t ngay kh√¥ng?",
        "**{tour_name}** l√† l·ª±a ch·ªçn tuy·ªát v·ªùi! B·∫°n c·∫ßn th√™m th√¥ng tin g√¨ v·ªÅ tour n√†y?"
    ],
    
    ConversationStage.BOOK: [
        "Tour **{tour_name}** ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t. Vui l√≤ng cung c·∫•p s·ªë ƒëi·ªán tho·∫°i ƒë·ªÉ ch√∫ng t√¥i li√™n h·ªá x√°c nh·∫≠n.",
        "Booking th√†nh c√¥ng! Ch√∫ng t√¥i s·∫Ω li√™n h·ªá v·ªõi b·∫°n qua s·ªë ƒëi·ªán tho·∫°i ƒë·ªÉ x√°c nh·∫≠n chi ti·∫øt.",
        "ƒê√£ x√°c nh·∫≠n ƒë·∫∑t tour **{tour_name}**. Vui l√≤ng cho ch√∫ng t√¥i s·ªë ƒëi·ªán tho·∫°i ƒë·ªÉ ho√†n t·∫•t th·ªß t·ª•c.",
        "C·∫£m ∆°n b·∫°n ƒë√£ ch·ªçn **{tour_name}**! ƒê·ªÉ ho√†n t·∫•t ƒë·∫∑t tour, vui l√≤ng cung c·∫•p s·ªë ƒëi·ªán tho·∫°i."
    ],
    
    ConversationStage.LEAD: [
        "ƒê√£ l∆∞u s·ªë **{phone}**. Ch√∫ng t√¥i s·∫Ω g·ªçi l·∫°i cho b·∫°n trong 30 ph√∫t. üìû",
        "C·∫£m ∆°n b·∫°n ƒë√£ cung c·∫•p s·ªë ƒëi·ªán tho·∫°i **{phone}**. ƒê·ªôi ng≈© Ruby Wings s·∫Ω li√™n h·ªá s·ªõm nh·∫•t!",
        "S·ªë ƒëi·ªán tho·∫°i **{phone}** ƒë√£ ƒë∆∞·ª£c ghi nh·∫≠n. Ch√∫ng t√¥i s·∫Ω li√™n h·ªá t∆∞ v·∫•n cho b·∫°n s·ªõm.",
        "ƒê√£ nh·∫≠n s·ªë **{phone}**. B·ªô ph·∫≠n t∆∞ v·∫•n Ruby Wings s·∫Ω li√™n h·ªá b·∫°n trong th·ªùi gian s·ªõm nh·∫•t."
    ],
    
    ConversationStage.CALLBACK: [
        "ƒê√£ ghi nh·∫≠n y√™u c·∫ßu g·ªçi l·∫°i. Ch√∫ng t√¥i s·∫Ω li√™n h·ªá s·ªë **{phone}** trong ng√†y h√¥m nay.",
        "Y√™u c·∫ßu g·ªçi l·∫°i ƒë√£ ƒë∆∞·ª£c x√°c nh·∫≠n. Ch√∫ng t√¥i s·∫Ω g·ªçi s·ªë **{phone}** trong v√≤ng 2 gi·ªù.",
        "Ch√∫ng t√¥i ƒë√£ ghi nh·∫≠n c·∫ßn g·ªçi l·∫°i s·ªë **{phone}**. S·∫Ω li√™n h·ªá v·ªõi b·∫°n s·ªõm nh·∫•t c√≥ th·ªÉ.",
        "C·∫£m ∆°n b·∫°n! Ch√∫ng t√¥i s·∫Ω g·ªçi l·∫°i s·ªë **{phone}** theo y√™u c·∫ßu c·ªßa b·∫°n."
    ]
}

# Intent-based templates (ƒê·ªíNG B·ªò V·ªöI ENTITIES.PY)
INTENT_TEMPLATES = {
    Intent.PROVIDE_PHONE: [
        "C·∫£m ∆°n b·∫°n ƒë√£ cung c·∫•p s·ªë ƒëi·ªán tho·∫°i **{phone}**. Ch√∫ng t√¥i s·∫Ω li√™n h·ªá s·ªõm nh·∫•t! üìû",
        "ƒê√£ nh·∫≠n s·ªë ƒëi·ªán tho·∫°i **{phone}**. ƒê·ªôi ng≈© Ruby Wings s·∫Ω g·ªçi t∆∞ v·∫•n cho b·∫°n!",
        "C·∫£m ∆°n b·∫°n! S·ªë **{phone}** ƒë√£ ƒë∆∞·ª£c l∆∞u l·∫°i. Ch√∫ng t√¥i s·∫Ω li√™n h·ªá trong th·ªùi gian s·ªõm nh·∫•t.",
        "S·ªë **{phone}** ƒë√£ ƒë∆∞·ª£c ghi nh·∫≠n. Ch√∫ng t√¥i r·∫•t mong ƒë∆∞·ª£c t∆∞ v·∫•n tr·ª±c ti·∫øp cho b·∫°n!"
    ],
    
    Intent.CALLBACK_REQUEST: [
        "B·∫°n mu·ªën ch√∫ng t√¥i g·ªçi l·∫°i khi n√†o? (s√°ng/chi·ªÅu/t·ªëi)",
        "Vui l√≤ng cho bi·∫øt khung gi·ªù ph√π h·ª£p ƒë·ªÉ ch√∫ng t√¥i g·ªçi l·∫°i cho b·∫°n?",
        "ƒê·ªÉ thu·∫≠n ti·ªán cho b·∫°n, b·∫°n mu·ªën ƒë∆∞·ª£c g·ªçi l·∫°i v√†o kho·∫£ng th·ªùi gian n√†o trong ng√†y?",
        "Khung gi·ªù n√†o thu·∫≠n ti·ªán ƒë·ªÉ ch√∫ng t√¥i li√™n h·ªá v·ªõi b·∫°n?"
    ],
    
    Intent.BOOKING_CONFIRM: [
        "Tuy·ªát v·ªùi! ƒê·ªÉ x√°c nh·∫≠n ƒë·∫∑t tour **{tour_name}**, vui l√≤ng cung c·∫•p s·ªë ƒëi·ªán tho·∫°i.",
        "B·∫°n ƒë√£ s·∫µn s√†ng ƒë·∫∑t tour **{tour_name}**. Vui l√≤ng cho ch√∫ng t√¥i s·ªë ƒëi·ªán tho·∫°i ƒë·ªÉ x√°c nh·∫≠n.",
        "C·∫£m ∆°n b·∫°n ƒë√£ ch·ªçn **{tour_name}**! ƒê·ªÉ ho√†n t·∫•t booking, vui l√≤ng cung c·∫•p s·ªë ƒëi·ªán tho·∫°i.",
        "ƒê·∫∑t tour **{tour_name}** ngay! Vui l√≤ng cung c·∫•p s·ªë ƒëi·ªán tho·∫°i ƒë·ªÉ ch√∫ng t√¥i x√°c nh·∫≠n."
    ],
    
    Intent.MODIFY_REQUEST: [
        "B·∫°n mu·ªën thay ƒë·ªïi th√¥ng tin tour? Vui l√≤ng cho bi·∫øt chi ti·∫øt.",
        "T√¥i s·∫Ω gi√∫p b·∫°n ch·ªânh s·ª≠a th√¥ng tin. B·∫°n mu·ªën thay ƒë·ªïi g√¨?",
        "ƒê·ªÉ h·ªó tr·ª£ b·∫°n thay ƒë·ªïi, vui l√≤ng cho bi·∫øt c·ª• th·ªÉ b·∫°n mu·ªën ƒëi·ªÅu ch·ªânh g√¨?",
        "B·∫°n c·∫ßn thay ƒë·ªïi th√¥ng tin n√†o? T√¥i s·∫Ω h·ªó tr·ª£ ngay."
    ],
    
    Intent.SMALLTALK: [
        "Xin ch√†o! T√¥i l√† Ruby Wings AI, r·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n. üòä",
        "Ch√†o b·∫°n! T√¥i ·ªü ƒë√¢y ƒë·ªÉ gi√∫p b·∫°n t√¨m tour tr·∫£i nghi·ªám ph√π h·ª£p nh·∫•t.",
        "R·∫•t vui ƒë∆∞·ª£c tr√≤ chuy·ªán v·ªõi b·∫°n! B·∫°n c·∫ßn t∆∞ v·∫•n v·ªÅ tour n√†o kh√¥ng?",
        "Hello! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n v·ªÅ c√°c tour Ruby Wings?"
    ],
    
    Intent.GREETING: [
        "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω AI c·ªßa Ruby Wings, chuy√™n t∆∞ v·∫•n tour tr·∫£i nghi·ªám thi√™n nhi√™n v√† ch·ªØa l√†nh. üåø",
        "Ch√†o b·∫°n! R·∫•t vui ƒë∆∞·ª£c g·∫∑p b·∫°n. T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n v·ªÅ c√°c tour Ruby Wings?",
        "Hello! T√¥i l√† chatbot Ruby Wings, s·∫µn s√†ng h·ªó tr·ª£ b·∫°n t√¨m tour ph√π h·ª£p nh·∫•t.",
        "Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi Ruby Wings! T√¥i c√≥ th·ªÉ t∆∞ v·∫•n tour n√†o cho b·∫°n?"
    ],
    
    Intent.FAREWELL: [
        "C·∫£m ∆°n b·∫°n ƒë√£ tr√≤ chuy·ªán! Hy v·ªçng s·ªõm ƒë∆∞·ª£c ƒë·ªìng h√†nh c√πng b·∫°n trong h√†nh tr√¨nh tr·∫£i nghi·ªám. ‚ú®",
        "T·∫°m bi·ªát b·∫°n! Li√™n h·ªá hotline **0332510486** n·∫øu c·∫ßn h·ªó tr·ª£ th√™m nh√©!",
        "Ch√∫c b·∫°n m·ªôt ng√†y t·ªët l√†nh! Mong s·ªõm ƒë∆∞·ª£c g·∫∑p l·∫°i b·∫°n trong tour Ruby Wings.",
        "H·∫πn g·∫∑p l·∫°i b·∫°n! ƒê·ª´ng qu√™n hotline **0332510486** khi c·∫ßn t∆∞ v·∫•n."
    ],
    
    Intent.TOUR_INQUIRY: [
        "T√¥i c√≥ th·ªÉ gi√∫p b·∫°n t√¨m hi·ªÉu v·ªÅ tour n√†o?",
        "B·∫°n quan t√¢m ƒë·∫øn lo·∫°i tour n√†o? T√¥i s·∫Ω t∆∞ v·∫•n chi ti·∫øt.",
        "Ruby Wings c√≥ nhi·ªÅu tour ƒë·ªôc ƒë√°o. B·∫°n mu·ªën bi·∫øt v·ªÅ tour n√†o?",
        "Cho t√¥i bi·∫øt b·∫°n quan t√¢m tour g√¨, t√¥i s·∫Ω gi·ªõi thi·ªáu chi ti·∫øt."
    ],
    
    Intent.PRICE_ASK: [
        "T√¥i s·∫Ω cung c·∫•p th√¥ng tin gi√° tour cho b·∫°n:",
        "D∆∞·ªõi ƒë√¢y l√† th√¥ng tin v·ªÅ gi√° c√°c tour:",
        "Gi√° tour nh∆∞ sau:",
        "Th√¥ng tin chi ph√≠ tour:"
    ]
}

# Location templates
LOCATION_TEMPLATES = {
    "no_tour_exact": [
        "Hi·ªán ch∆∞a c√≥ tour ch√≠nh x√°c t·∫°i **{location}**. B·∫°n c√≥ mu·ªën tham kh·∫£o c√°c tour t∆∞∆°ng t·ª± t·∫°i **{region}** kh√¥ng?",
        "Ruby Wings ch∆∞a c√≥ tour ·ªü **{location}**. T√¥i c√≥ th·ªÉ ƒë·ªÅ xu·∫•t tour ·ªü khu v·ª±c **{region}** cho b·∫°n.",
        "Kh√¥ng t√¨m th·∫•y tour t·∫°i **{location}**. B·∫°n c√≥ quan t√¢m ƒë·∫øn c√°c tour t·∫°i **{region}** kh√¥ng?",
        "Tour t·∫°i **{location}** hi·ªán ch∆∞a c√≥. T√¥i c√≥ th·ªÉ gi·ªõi thi·ªáu tour ·ªü **{region}** thay th·∫ø?"
    ],
    
    "tour_found": [
        "T√¨m th·∫•y **{count}** tour t·∫°i **{location}**:",
        "D∆∞·ªõi ƒë√¢y l√† **{count}** tour Ruby Wings t·∫°i **{location}**:",
        "C√≥ **{count}** tour ph√π h·ª£p t·∫°i **{location}** b·∫°n c√≥ th·ªÉ tham kh·∫£o:",
        "**{count}** tour t·∫°i **{location}** ƒëang ch·ªù b·∫°n kh√°m ph√°:"
    ],
    
    "region_fallback": [
        "C√°c tour t·∫°i khu v·ª±c **{region}**:",
        "Tour ·ªü **{region}** b·∫°n c√≥ th·ªÉ quan t√¢m:",
        "G·ª£i √Ω tour t·∫°i **{region}**:",
        "Kh√°m ph√° **{region}** c√πng c√°c tour:"
    ]
}

# ==================== HELPER FUNCTIONS ====================

def extract_source_tokens(text: str) -> List[str]:
    """Return list of citation tokens like [1] found in text."""
    return SRC_RE.findall(text or "")

def normalize_for_overlap(s: str) -> List[str]:
    """Normalize text for overlap comparison."""
    if not s:
        return []
    s = s.lower()
    s = re.sub(r"[^\w\s]", " ", s)
    toks = [t for t in s.split() if len(t) > 1]
    return toks

def overlap_ratio(a_tokens: List[str], b_tokens: List[str]) -> float:
    """Calculate overlap ratio between two token lists."""
    if not a_tokens or not b_tokens:
        return 0.0
    ca = Counter(a_tokens)
    cb = Counter(b_tokens)
    common = sum(min(ca[t], cb.get(t, 0)) for t in ca)
    return common / max(len(a_tokens), 1)

def collect_passage_texts(top_passages: List[Tuple[float, Dict]]) -> List[str]:
    """Collect text from passages."""
    return [m.get("text", "") for _, m in (top_passages or [])]

def collect_passage_paths(top_passages: List[Tuple[float, Dict]]) -> List[str]:
    """Collect paths from passages."""
    return [m.get("path", "") for _, m in (top_passages or [])]

def safe_shorten(text: str, max_chars: int = 1200) -> str:
    """Safely shorten text to max_chars, trying to cut at sentence boundary."""
    if not text:
        return ""
    t = text.strip()
    if len(t) <= max_chars:
        return t
    # Try to cut at sentence boundary
    cut = t[:max_chars].rfind(".")
    if cut > int(max_chars * 0.5):
        return t[:cut + 1]
    return t[:max_chars].rstrip() + "..."

def get_random_template(template_dict: Dict[str, List[str]], key: str, default: str = "") -> str:
    """Get random template from dict."""
    templates = template_dict.get(key, [default])
    return random.choice(templates) if templates else default

def mask_phone(phone: str) -> str:
    """Mask phone number for display."""
    if not phone or len(phone) < 4:
        return phone
    return f"{phone[:4]}***{phone[-2:]}"

# ==================== TOUR FORMATTING ====================

def extract_tour_info_from_passages(passages: List[Tuple[float, Dict[str, Any]]]) -> List[Dict[str, Any]]:
    """
    Extract structured tour information from passages.
    Compatible with tour_entities.json structure.
    """
    tours = {}
    
    for score, passage in passages:
        text = passage.get("text", "")
        path = passage.get("path", "")
        
        # Extract tour index from path
        tour_match = re.search(r'tours\[(\d+)\]', path)
        if not tour_match:
            continue
        
        tour_idx = int(tour_match.group(1))
        
        # Initialize tour dict if not exists
        if tour_idx not in tours:
            tours[tour_idx] = {
                "index": tour_idx,
                "tour_name": "",
                "location": "",
                "duration": "",
                "price": "",
                "summary": "",
                "score": 0.0
            }
        
        # Update tour info based on text content
        lines = text.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith("T√™n tour:"):
                tours[tour_idx]["tour_name"] = line.replace("T√™n tour:", "").strip()
            elif line.startswith("ƒê·ªãa ƒëi·ªÉm:"):
                tours[tour_idx]["location"] = line.replace("ƒê·ªãa ƒëi·ªÉm:", "").strip()
            elif line.startswith("Th·ªùi l∆∞·ª£ng:"):
                tours[tour_idx]["duration"] = line.replace("Th·ªùi l∆∞·ª£ng:", "").strip()
            elif line.startswith("Gi√°:"):
                price_text = line.replace("Gi√°:", "").strip()
                # Truncate long prices
                if len(price_text) > 100:
                    price_text = price_text[:100] + "..."
                tours[tour_idx]["price"] = price_text
            elif line.startswith("T√≥m t·∫Øt:"):
                tours[tour_idx]["summary"] = line.replace("T√≥m t·∫Øt:", "").strip()
        
        # Update score (highest score for this tour)
        tours[tour_idx]["score"] = max(tours[tour_idx]["score"], score)
    
    # Convert to list and sort by score
    tour_list = list(tours.values())
    tour_list.sort(key=lambda x: x["score"], reverse=True)
    
    return tour_list

def format_tour_response(tours: List[Dict[str, Any]], max_tours: int = 3, 
                        include_summary: bool = False) -> Tuple[str, List[str]]:
    """
    Format tours with labels and structured information.
    Returns: (formatted_text, tour_labels)
    """
    if not tours:
        return "", []
    
    # Limit to max_tours
    tours = tours[:max_tours]
    tour_labels = []
    formatted_parts = []
    
    # Define labels based on position (ƒê·ªíNG B·ªò V·ªöI APP.PY)
    label_map = {
        0: "üèÜ Ph√π h·ª£p nh·∫•t",
        1: "‚≠ê Ph·ªï bi·∫øn",
        2: "üí∞ Gi√° t·ªët"
    }
    
    for i, tour in enumerate(tours):
        if not tour:
            continue
        
        # Get label
        label = label_map.get(i, f"**{i+1}.**")
        tour_labels.append(label)
        
        # Build tour block
        tour_block = f"{label} **{tour.get('tour_name', 'Tour')}**\n"
        
        # Add details if available
        if tour.get('location'):
            tour_block += f"   üìç ƒê·ªãa ƒëi·ªÉm: {tour['location']}\n"
        if tour.get('duration'):
            tour_block += f"   ‚è±Ô∏è Th·ªùi l∆∞·ª£ng: {tour['duration']}\n"
        if tour.get('price'):
            tour_block += f"   üí∞ Gi√°: {tour['price']}\n"
        if include_summary and tour.get('summary'):
            summary = tour['summary'][:150] + "..." if len(tour['summary']) > 150 else tour['summary']
            tour_block += f"   üìù {summary}\n"
        
        formatted_parts.append(tour_block)
    
    return "\n".join(formatted_parts), tour_labels

# ==================== TEMPLATE GENERATION ====================

def generate_intent_response(intent: str, context: Dict[str, Any]) -> Optional[str]:
    """Generate intent-specific response."""
    if intent not in INTENT_TEMPLATES:
        return None
    
    template = random.choice(INTENT_TEMPLATES[intent])
    
    # Fill template variables
    phone = context.get("phone") or context.get("lead_phone") or ""
    tour_name = context.get("selected_tour_name") or context.get("tour_name") or "tour ƒë√£ ch·ªçn"
    
    # Format phone for display
    if phone:
        phone_display = mask_phone(phone)
    else:
        phone_display = ""
    
    try:
        # Try to format template
        if "{phone}" in template and phone_display:
            return template.format(phone=phone_display)
        elif "{tour_name}" in template and tour_name:
            return template.format(tour_name=tour_name)
        else:
            return template
    except KeyError:
        # If template has variables but context doesn't have them, return plain template
        return template

def generate_state_fallback(state: str, context: Dict[str, Any], 
                           top_passages: List[Tuple[float, Dict[str, Any]]], 
                           requested_field: Optional[str] = None) -> str:
    """Generate state-based fallback response."""
    
    # Try to get state template
    if state in STATE_TEMPLATES:
        template = random.choice(STATE_TEMPLATES[state])
        
        # Fill template variables
        phone = context.get("phone") or context.get("lead_phone") or ""
        tour_name = context.get("selected_tour_name") or context.get("tour_name") or ""
        location = context.get("location") or ""
        
        # Format phone
        if phone:
            phone = mask_phone(phone)
        
        try:
            if "{tour_name}" in template and tour_name:
                template = template.format(tour_name=tour_name)
            if "{phone}" in template and phone:
                template = template.format(phone=phone)
            if "{location}" in template and location:
                template = template.format(location=location)
        except KeyError:
            pass  # Use template as-is if formatting fails
        
        # Add tour information for SUGGEST/COMPARE states
        if state in [ConversationStage.SUGGEST, ConversationStage.COMPARE]:
            tours_info = extract_tour_info_from_passages(top_passages)
            if tours_info:
                formatted_tours, _ = format_tour_response(tours_info, max_tours=3)
                if formatted_tours:
                    return template + "\n\n" + formatted_tours + "\n\nüí° *Li√™n h·ªá hotline **0332510486** ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt*"
        
        return template + "\n\nüí° *Li√™n h·ªá hotline **0332510486** ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt*"
    
    # Default to deterministic fallback
    return deterministic_fallback_answer(top_passages, requested_field, context=context)

def add_state_template(text: str, state: str, context: Dict[str, Any]) -> str:
    """Add state-appropriate template prefix to text."""
    if state not in STATE_TEMPLATES:
        return text
    
    # Only add template for certain states
    if state in [ConversationStage.SUGGEST, ConversationStage.COMPARE]:
        template = random.choice(STATE_TEMPLATES[state])
        
        # Check if template already present
        if not any(template_part in text for template_part in STATE_TEMPLATES[state]):
            text = template + "\n\n" + text
    
    return text

def add_location_context(text: str, location: str, tour_count: int, region: Optional[str] = None) -> str:
    """Add location context to response."""
    if not location:
        return text
    
    # Get region if not provided
    if not region:
        region = get_region_from_location(location) or "khu v·ª±c t∆∞∆°ng t·ª±"
    
    # Check if location info already in text
    location_lower = location.lower()
    text_lower = text.lower()
    
    if location_lower not in text_lower and "ƒë·ªãa ƒëi·ªÉm" not in text_lower:
        if tour_count > 0:
            template = random.choice(LOCATION_TEMPLATES["tour_found"])
            prefix = template.format(count=tour_count, location=location)
        else:
            template = random.choice(LOCATION_TEMPLATES["no_tour_exact"])
            prefix = template.format(location=location, region=region)
        
        text = prefix + "\n\n" + text
    
    return text

# ==================== DETERMINISTIC FALLBACK ====================

def deterministic_fallback_answer(
    top_passages: List[Tuple[float, Dict[str, Any]]], 
    requested_field: Optional[str] = None, 
    max_snippets: int = 3,
    context: Optional[Dict[str, Any]] = None
) -> str:
    """
    Build a safe answer using only retrieved passages.
    Short, friendly, cites indexed sources [1],[2].
    If requested_field provided, prioritize passages whose path mentions that field.
    """
    context = context or {}
    
    if not top_passages:
        return "Xin l·ªói ‚Äî hi·ªán kh√¥ng c√≥ th√¥ng tin trong t√†i li·ªáu v·ªÅ y√™u c·∫ßu c·ªßa b·∫°n.\n\nüí° *Li√™n h·ªá hotline **0332510486** ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n tr·ª±c ti·∫øp*"

    # Prioritize field passages
    prioritized = []
    others = []
    
    for score, m in top_passages:
        p = m.get("path", "")
        if requested_field and (p.endswith(f".{requested_field}") or f".{requested_field}" in p):
            prioritized.append((score, m))
        else:
            others.append((score, m))
    
    chosen = (prioritized + others)[:max_snippets]

    pieces = []
    for i, (score, m) in enumerate(chosen, start=1):
        text = m.get("text", "").strip()
        text = safe_shorten(text, 800)
        pieces.append(f"[{i}] {text}")

    # Build header
    header = ""
    if requested_field:
        header = f'V·ªÅ "{requested_field}", t√¥i t√¨m th·∫•y th√¥ng tin sau (tr√≠ch t·ª´ t√†i li·ªáu Ruby Wings):\n\n'
    else:
        header = "T√¥i t√¨m th·∫•y th√¥ng tin sau t·ª´ d·ªØ li·ªáu Ruby Wings:\n\n"

    footer = "\n\nüí° *Li√™n h·ªá hotline **0332510486** ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt v√† ƒë·∫∑t tour*"
    
    return header + "\n\n".join(pieces) + footer

# ==================== MAIN VALIDATION FUNCTION ====================

def validate_and_format_answer(
    llm_text: str,
    top_passages: List[Tuple[float, Dict[str, Any]]],
    requested_field: Optional[str] = None,
    tour_indices: Optional[List[int]] = None,
    max_chars: int = MAX_ANSWER_CHARS,
    context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Validate LLM answer against retrieved top_passages.
    If fails safety checks, return deterministic aggregated snippets instead.
    
    Enhanced with state-based templates, location-aware responses, and improved formatting.
    Fully integrated with entities.py v5.2 and tour_entities.json structure.
    
    Parameters:
      - llm_text: text returned by LLM (may be empty)
      - top_passages: list of (score, mapping_entry) where mapping_entry has 'path' and 'text'
      - requested_field: if provided, ensure answer addresses that field
      - tour_indices: list of tour indices in context (optional)
      - max_chars: maximum characters for answer
      - context: conversation context dict with state, intent, location, etc.
    
    Returns:
      - dict with answer, sources, guard_passed, reason, state, etc.
    """
    start = time.time()
    context = context or {}
    
    # Extract context values (ƒê·ªíNG B·ªò V·ªöI ENTITIES.PY)
    state = context.get("stage", ConversationStage.EXPLORE)
    intent = context.get("intent")
    location = context.get("location")
    location_filtered = context.get("location_filtered", False)
    has_phone = context.get("has_phone", False)
    phone = context.get("phone") or context.get("lead_phone")
    selected_tour_name = context.get("selected_tour_name") or context.get("tour_name")
    
    passages = collect_passage_texts(top_passages)
    paths = collect_passage_paths(top_passages)

    # Sanitize LLM text first
    candidate = (llm_text or "").strip()
    candidate = html.unescape(candidate)
    candidate = re.sub(r"\s+\n", "\n", candidate)
    candidate = safe_shorten(candidate, max_chars)

    # NEW: Handle intent-specific responses first
    if intent and intent in INTENT_TEMPLATES:
        intent_response = generate_intent_response(intent, context)
        if intent_response:
            return {
                "answer": intent_response,
                "sources": [],
                "guard_passed": True,
                "reason": "intent_template",
                "state": state,
                "intent": intent,
                "elapsed": time.time() - start
            }

    # 1) If no retrieved evidence at all -> state-based fallback
    if not passages:
        fallback = generate_state_fallback(state, context, top_passages, requested_field)
        return {
            "answer": fallback,
            "sources": [],
            "guard_passed": False,
            "reason": "no_evidence",
            "state": state,
            "elapsed": time.time() - start
        }

    # 2) Check for explicit citation tokens in LLM text
    cited_tokens = extract_source_tokens(candidate)
    if cited_tokens:
        # Map numeric citation tokens to mapping paths: [1] -> top_passages[0], etc.
        cited_paths = []
        for tok in cited_tokens:
            try:
                idx = int(tok.strip("[]")) - 1
                if 0 <= idx < len(top_passages):
                    cited_paths.append(paths[idx])
            except Exception:
                pass
        
        # Basic evidence overlap check
        evidence_concat = " ".join(passages[:5])
        if overlap_ratio(normalize_for_overlap(candidate), normalize_for_overlap(evidence_concat)) >= MIN_OVERLAP_RATIO:
            # Add state template if appropriate
            if state in [ConversationStage.SUGGEST, ConversationStage.COMPARE]:
                candidate = add_state_template(candidate, state, context)
            
            # Add location context if filtered
            if location_filtered and location:
                candidate = add_location_context(candidate, location, len(passages))
            
            return {
                "answer": candidate,
                "sources": cited_paths or paths[:3],
                "guard_passed": True,
                "reason": "ok_with_citations",
                "state": state,
                "location_filtered": location_filtered,
                "elapsed": time.time() - start
            }

    # 3) Token-overlap heuristic between LLM output and evidence
    evidence_concat = " ".join(passages[:5])
    ov = overlap_ratio(normalize_for_overlap(candidate), normalize_for_overlap(evidence_concat))
    
    if ov >= MIN_OVERLAP_RATIO:
        # 3a) If requested_field is provided, ensure candidate mentions field-specific content
        if requested_field:
            # Find passages matching requested_field by path suffix
            field_passages = [
                m.get("text", "") for _, m in top_passages 
                if (m.get("path", "").endswith(f".{requested_field}") or f".{requested_field}" in m.get("path", ""))
            ]
            
            if field_passages:
                field_ov = overlap_ratio(
                    normalize_for_overlap(candidate), 
                    normalize_for_overlap(" ".join(field_passages[:4]))
                )
                
                if field_ov < MIN_FIELD_MENTION_RATIO:
                    # Mismatch: LLM didn't address requested field sufficiently
                    fallback = generate_state_fallback(state, context, top_passages, requested_field)
                    return {
                        "answer": fallback,
                        "sources": collect_passage_paths(top_passages)[:3],
                        "guard_passed": False,
                        "reason": "mismatch_field",
                        "state": state,
                        "elapsed": time.time() - start
                    }
        
        # 3b) Ban hedging phrases to enforce professional tone
        low = candidate.lower()
        banned_found = []
        for banned in BANNED_PHRASES:
            if banned in low:
                banned_found.append(banned)
                low = low.replace(banned, "")
        
        # If too many banned phrases, use fallback
        if len(banned_found) > 2:
            logger.warning(f"Too many banned phrases in LLM response: {banned_found}")
            fallback = generate_state_fallback(state, context, top_passages, requested_field)
            return {
                "answer": fallback,
                "sources": collect_passage_paths(top_passages)[:3],
                "guard_passed": False,
                "reason": "too_many_banned_phrases",
                "state": state,
                "elapsed": time.time() - start
            }
        
        candidate = safe_shorten(candidate, max_chars)
        
        # Add location context if applicable
        if location_filtered and location:
            candidate = add_location_context(candidate, location, len(passages))
        
        # Add state template
        candidate = add_state_template(candidate, state, context)
        
        return {
            "answer": candidate,
            "sources": collect_passage_paths(top_passages)[:3],
            "guard_passed": True,
            "reason": "ok",
            "overlap": round(ov, 3),
            "state": state,
            "location_filtered": location_filtered,
            "elapsed": time.time() - start
        }

    # 4) Low overlap -> LLM likely hallucinated -> state-based deterministic fallback
    logger.warning(f"Low overlap detected: {ov:.3f} < {MIN_OVERLAP_RATIO}")
    
    fallback = generate_state_fallback(state, context, top_passages, requested_field)
    
    # Extract tour info for formatting
    tours_info = extract_tour_info_from_passages(top_passages)
    formatted_tours, tour_labels = format_tour_response(tours_info, max_tours=3)
    
    # Add formatted tours to fallback if available
    if formatted_tours and state in [ConversationStage.SUGGEST, ConversationStage.COMPARE, ConversationStage.EXPLORE]:
        if not fallback.endswith("\n\n"):
            fallback += "\n\n"
        fallback += formatted_tours
        fallback += "\n\nüí° *Li√™n h·ªá hotline **0332510486** ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt*"
    
    return {
        "answer": fallback,
        "sources": collect_passage_paths(top_passages)[:3],
        "guard_passed": False,
        "reason": "low_overlap",
        "overlap": round(ov, 3),
        "state": state,
        "tour_labels": tour_labels,
        "location_filtered": location_filtered,
        "elapsed": time.time() - start
    }

# ==================== UTILITY FUNCTIONS ====================

def sanitize_answer(text: str) -> str:
    """Sanitize answer text for safe output."""
    if not text:
        return ""
    
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    
    # Unescape HTML entities
    text = html.unescape(text)
    
    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\n\s+\n', '\n\n', text)
    
    # Trim
    text = text.strip()
    
    return text

def add_hotline_cta(text: str) -> str:
    """Add hotline CTA if not already present."""
    if "0332510486" in text or "hotline" in text.lower():
        return text
    
    return text + "\n\nüí° *Li√™n h·ªá hotline **0332510486** ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt*"

def format_price_text(price_text: str, max_length: int = 100) -> str:
    """Format price text for display."""
    if not price_text:
        return ""
    
    # Truncate if too long
    if len(price_text) > max_length:
        price_text = price_text[:max_length] + "..."
    
    return price_text

# ==================== TESTING ====================

def test_response_guard():
    """Test response guard functionality."""
    print("=" * 60)
    print("TESTING RESPONSE GUARD v5.2")
    print("=" * 60)
    
    # Sample passages
    sample_passages = [
        (1.0, {"path": "root.tours[0].price", "text": "T√™n tour: Non n∆∞·ªõc B·∫°ch M√£\nƒê·ªãa ƒëi·ªÉm: V∆∞·ªùn qu·ªëc gia B·∫°ch M√£\nTh·ªùi l∆∞·ª£ng: 1 ng√†y\nGi√°: 890.000 VNƒê/kh√°ch"}),
        (0.9, {"path": "root.tours[0].transport", "text": "Ph∆∞∆°ng ti·ªán: Xe 7-16 ch·ªó ƒë·ªùi m·ªõi"}),
        (0.8, {"path": "root.tours[1].tour_name", "text": "T√™n tour: M∆∞a ƒê·ªè v√† Tr∆∞·ªùng S∆°n ‚Äì H√†nh Tr√¨nh Kh√°t V·ªçng\nƒê·ªãa ƒëi·ªÉm: Qu·∫£ng Tr·ªã\nTh·ªùi l∆∞·ª£ng: 2 ng√†y 1 ƒë√™m\nGi√°: 1.700.000 ‚Äì 2.300.000 VNƒê/ng∆∞·ªùi"})
    ]
    
    # Test 1: With context
    print("\n### Test 1: State-based response (SUGGEST)")
    context1 = {
        "stage": ConversationStage.SUGGEST,
        "intent": Intent.TOUR_INQUIRY,
        "location": "Hu·∫ø",
        "location_filtered": True
    }
    
    llm_good = "Gi√° tour B·∫°ch M√£ l√† 890.000 VNƒê/kh√°ch. [1]"
    result1 = validate_and_format_answer(llm_good, sample_passages, context=context1)
    print(f"Guard passed: {result1['guard_passed']}")
    print(f"Reason: {result1['reason']}")
    print(f"Answer preview: {result1['answer'][:200]}...")
    
    # Test 2: Intent template
    print("\n### Test 2: Intent-based response (PROVIDE_PHONE)")
    context2 = {
        "intent": Intent.PROVIDE_PHONE,
        "phone": "0909123456",
        "stage": ConversationStage.LEAD
    }
    
    result2 = validate_and_format_answer("", sample_passages, context=context2)
    print(f"Guard passed: {result2['guard_passed']}")
    print(f"Answer: {result2['answer']}")
    
    # Test 3: Tour formatting
    print("\n### Test 3: Tour formatting")
    tours_info = extract_tour_info_from_passages(sample_passages)
    formatted, labels = format_tour_response(tours_info)
    print(f"Formatted tours:\n{formatted}")
    print(f"Labels: {labels}")
    
    # Test 4: Low overlap (hallucination)
    print("\n### Test 4: Low overlap detection")
    llm_bad = "B·∫°n ch·ªâ c·∫ßn mang 10 tri·ªáu v√† m·ªçi th·ª© s·∫Ω ·ªïn."
    result4 = validate_and_format_answer(llm_bad, sample_passages, context=context1)
    print(f"Guard passed: {result4['guard_passed']}")
    print(f"Reason: {result4['reason']}")
    print(f"Overlap: {result4.get('overlap', 'N/A')}")
    
    print("\n" + "=" * 60)
    print("TESTING COMPLETE")
    print("=" * 60)

# ==================== EXPORTS ====================

__all__ = [
    # Main function
    'validate_and_format_answer',
    
    # Helper functions
    'extract_tour_info_from_passages',
    'format_tour_response',
    'generate_intent_response',
    'generate_state_fallback',
    'add_state_template',
    'add_location_context',
    'deterministic_fallback_answer',
    'sanitize_answer',
    'add_hotline_cta',
    
    # Test function
    'test_response_guard'
]

# ==================== MAIN ====================

if __name__ == "__main__":
    # Run tests
    test_response_guard()