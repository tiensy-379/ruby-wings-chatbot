def safe_validate(reply):
    try:
        if not isinstance(reply, dict):
            return reply
        # Only validate when tour is selected
        if not reply.get("tour_name"):
            return reply
        return AutoValidator.validate_response(reply)
    except Exception as e:
        try:
            reply.setdefault("warnings", []).append(str(e))
        except:
            pass
        return reply


# app.py - Ruby Wings Chatbot v4.0 (Complete Rewrite with Dataclasses)
# =========== IMPORTS ===========
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("ruby-wings")
import os
import sys
import json
import threading
import logging
import re
import unicodedata
import traceback
import hashlib
import time
import random
from functools import lru_cache, wraps
from typing import List, Tuple, Dict, Optional, Any, Set, Union, Callable
from datetime import datetime, timedelta
from collections import defaultdict, deque
from difflib import SequenceMatcher
from enum import Enum
# Try to import numpy with detailed error handling
try:
    
    import numpy as np
    NUMPY_AVAILABLE = True
    logger.info("âœ… NumPy available")
except ImportError as e:
    logger.error(f"âŒ NumPy import failed: {e}")
    # Create a minimal numpy-like fallback for basic operations
    class NumpyFallback:
        def __init__(self):
            self.float32 = float
            self.int64 = int
            
        def array(self, data, dtype=None):
            # Simple list wrapper
            class SimpleArray:
                def __init__(self, data):
                    self.data = list(data)
                    self.shape = (len(data),) if isinstance(data[0], (int, float)) else (len(data), len(data[0]))
                
                def astype(self, dtype):
                    return self
                
                def reshape(self, shape):
                    return self
                
                def __getitem__(self, idx):
                    return self.data[idx]
                
                def __len__(self):
                    return len(self.data)
            
            return SimpleArray(data)
        
        def empty(self, shape, dtype):
            if len(shape) == 1:
                return [0.0] * shape[0]
            else:
                return [[0.0] * shape[1] for _ in range(shape[0])]
        
        def vstack(self, arrays):
            result = []
            for arr in arrays:
                if hasattr(arr, 'data'):
                    result.extend(arr.data)
                else:
                    result.extend(arr)
            return result
        
        def load(self, path):
            # Mock load function
            class MockNpz:
                def __init__(self):
                    self.files = ['mat']
                
                def __getitem__(self, key):
                    if key == 'mat':
                        # Return empty array
                        return self.array([[0.0]])
                    return None
            
            return MockNpz()
        
        def savez_compressed(self, path, **kwargs):
            # Mock save function
            logger.warning(f"âš ï¸ NumPy fallback: Mock saving to {path}")
            return None
    
    np = NumpyFallback()
    NUMPY_AVAILABLE = False
    logger.warning("âš ï¸ Using NumPy fallback - limited functionality")
from flask import Flask, request, jsonify, g
from flask_cors import CORS

# =========== ENTITY IMPORTS ===========
from entities import (
    QuestionType,
    ConversationState,
    PriceLevel,
    DurationType,
    Tour,
    UserProfile,
    SearchResult,
    ConversationContext,
    FilterSet,
    LLMRequest,
    ChatResponse,
    LeadData,
    CacheEntry,
    EnhancedJSONEncoder
)

# =========== CONFIGURATION ===========
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ruby_wings.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("rbw_v4")

# =========== IMPORTS WITH FALLBACKS ===========
HAS_FAISS = False
try:
    import faiss
    HAS_FAISS = True
    logger.info("âœ… FAISS available")
except ImportError:
    logger.warning("âš ï¸ FAISS not available, using numpy fallback")

HAS_OPENAI = False
client = None
try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    logger.warning("âš ï¸ OpenAI not available, using fallback responses")

# Google Sheets
try:
    import gspread
    from google.oauth2.service_account import Credentials
    from google.auth.exceptions import GoogleAuthError
    from gspread.exceptions import APIError, SpreadsheetNotFound, WorksheetNotFound
    HAS_GOOGLE_SHEETS = True
except ImportError:
    HAS_GOOGLE_SHEETS = False
    logger.warning("âš ï¸ Google Sheets not available")

# Meta CAPI
try:
    from meta_capi import send_meta_pageview, send_meta_lead, send_meta_call_button
    HAS_META_CAPI = True
    logger.info("âœ… Meta CAPI available")
except ImportError:
    HAS_META_CAPI = False
    logger.warning("âš ï¸ Meta CAPI not available")

# =========== ENVIRONMENT VARIABLES ===========
# Memory Profile
RAM_PROFILE = os.environ.get("RAM_PROFILE", "512").strip()
IS_LOW_RAM = RAM_PROFILE == "512"
IS_HIGH_RAM = RAM_PROFILE == "2048"

# Core API Keys
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "").strip()
OPENAI_BASE_URL = os.environ.get("OPENAI_BASE_URL", "").strip()
GOOGLE_SERVICE_ACCOUNT_JSON = os.environ.get("GOOGLE_SERVICE_ACCOUNT_JSON", "").strip()

# Knowledge & Index
KNOWLEDGE_PATH = os.environ.get("KNOWLEDGE_PATH", "knowledge.json")
FAISS_INDEX_PATH = os.environ.get("FAISS_INDEX_PATH", "faiss_index.bin")
FAISS_MAPPING_PATH = os.environ.get("FAISS_MAPPING_PATH", "faiss_mapping.json")
FALLBACK_VECTORS_PATH = os.environ.get("FALLBACK_VECTORS_PATH", "vectors.npz")

# Models
EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
CHAT_MODEL = os.environ.get("CHAT_MODEL", "gpt-4o-mini")
TOP_K = int(os.environ.get("TOP_K", "5"))

# FAISS
FAISS_ENABLED = os.environ.get("FAISS_ENABLED", "true").lower() in ("1", "true", "yes") and not IS_LOW_RAM

# Google Sheets
GOOGLE_SHEET_ID = os.environ.get("GOOGLE_SHEET_ID", "1SdVbwkuxb8l1meEW--ddyfh4WmUvSXXMOPQ5bCyPkdk")
GOOGLE_SHEET_NAME = os.environ.get("GOOGLE_SHEET_NAME", "RBW_Lead_Raw_Inbox")
ENABLE_GOOGLE_SHEETS = os.environ.get("ENABLE_GOOGLE_SHEETS", "true").lower() in ("1", "true", "yes")

# Storage
ENABLE_FALLBACK_STORAGE = os.environ.get("ENABLE_FALLBACK_STORAGE", "true").lower() in ("1", "true", "yes")
FALLBACK_STORAGE_PATH = os.environ.get("FALLBACK_STORAGE_PATH", "leads_fallback.json")

# Meta CAPI
META_CAPI_TOKEN = os.environ.get("META_CAPI_TOKEN", "").strip()
META_PIXEL_ID = os.environ.get("META_PIXEL_ID", "").strip()
META_CAPI_ENDPOINT = os.environ.get("META_CAPI_ENDPOINT", "https://graph.facebook.com/v17.0/")
ENABLE_META_CAPI_CALL = os.environ.get("ENABLE_META_CAPI_CALL", "true").lower() in ("1", "true", "yes")

# Server
FLASK_ENV = os.environ.get("FLASK_ENV", "production")
DEBUG = os.environ.get("DEBUG", "false").lower() in ("1", "true", "yes")
SECRET_KEY = os.environ.get("SECRET_KEY", "ruby-wings-secret-key-2024")
CORS_ORIGINS = os.environ.get("CORS_ORIGINS", "https://www.rubywings.vn,http://localhost:3000").split(",")
HOST = os.environ.get("HOST", "0.0.0.0")
PORT = int(os.environ.get("PORT", "10000"))

# =========== STATS TRACKING (FIX Lá»–I STATE) ===========
# ThÃªm global stats tracking system
STATS_LOCK = threading.Lock()
GLOBAL_STATS = {
    'meta_capi_calls': 0,
    'meta_capi_errors': 0,
    'leads': 0,
    'errors': 0,
    'total_requests': 0
}

def increment_stat(stat_name: str, amount: int = 1):
    """Thread-safe stat increment"""
    with STATS_LOCK:
        if stat_name in GLOBAL_STATS:
            GLOBAL_STATS[stat_name] += amount
        else:
            GLOBAL_STATS[stat_name] = amount

def get_stats() -> dict:
    """Get current stats"""
    with STATS_LOCK:
        return GLOBAL_STATS.copy()

# =========== UPGRADE FEATURE FLAGS ===========
class UpgradeFlags:
    """Control all 10 upgrades with environment variables"""
    
    @staticmethod
    def get_all_flags():
        return {
            # CORE UPGRADES (Essential fixes)
            "UPGRADE_1_MANDATORY_FILTER": os.environ.get("UPGRADE_1_MANDATORY_FILTER", "true").lower() == "true",
            "UPGRADE_2_DEDUPLICATION": os.environ.get("UPGRADE_2_DEDUPLICATION", "true").lower() == "true",
            "UPGRADE_3_ENHANCED_FIELDS": os.environ.get("UPGRADE_3_ENHANCED_FIELDS", "true").lower() == "true",
            "UPGRADE_4_QUESTION_PIPELINE": os.environ.get("UPGRADE_4_QUESTION_PIPELINE", "true").lower() == "true",
            
            # ADVANCED UPGRADES
            "UPGRADE_5_QUERY_SPLITTER": os.environ.get("UPGRADE_5_QUERY_SPLITTER", "true").lower() == "true",
            "UPGRADE_6_FUZZY_MATCHING": os.environ.get("UPGRADE_6_FUZZY_MATCHING", "true").lower() == "true",
            "UPGRADE_7_STATE_MACHINE": os.environ.get("UPGRADE_7_STATE_MACHINE", "true").lower() == "true",
            "UPGRADE_8_SEMANTIC_ANALYSIS": os.environ.get("UPGRADE_8_SEMANTIC_ANALYSIS", "true").lower() == "true",
            "UPGRADE_9_AUTO_VALIDATION": os.environ.get("UPGRADE_9_AUTO_VALIDATION", "true").lower() == "true",
            "UPGRADE_10_TEMPLATE_SYSTEM": os.environ.get("UPGRADE_10_TEMPLATE_SYSTEM", "true").lower() == "true",
            
            # PERFORMANCE OPTIONS
            "ENABLE_CACHING": os.environ.get("ENABLE_CACHING", "true").lower() == "true",
            "CACHE_TTL_SECONDS": int(os.environ.get("CACHE_TTL_SECONDS", "300")),
            "ENABLE_QUERY_LOGGING": os.environ.get("ENABLE_QUERY_LOGGING", "true").lower() == "true",
            
            # MEMORY OPTIMIZATION
            "EMBEDDING_CACHE_SIZE": 100 if IS_LOW_RAM else 1000,
            "TOUR_CACHE_ENABLED": not IS_LOW_RAM,
            "PRELOAD_EMBEDDINGS": not IS_LOW_RAM,
        }
    
    @staticmethod
    def is_enabled(upgrade_name: str) -> bool:
        flags = UpgradeFlags.get_all_flags()
        return flags.get(f"UPGRADE_{upgrade_name}", False)

# =========== FLASK APP CONFIG ===========
app = Flask(__name__)
app.json_encoder = EnhancedJSONEncoder  # Use custom JSON encoder
CORS(app, origins=CORS_ORIGINS, supports_credentials=True)

# =========== GLOBAL STATE (USING DATACLASSES) ===========
# Initialize OpenAI client
# ==== OpenAI client (SDK 1.x safe, Render compatible) ====
from openai import OpenAI
import httpx
import os

def create_openai_client():
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY is not set")

    # Render-safe HTTP client (no proxies param in OpenAI 1.x)
    http_client = httpx.Client(
        timeout=60.0,
        follow_redirects=True
    )

    return OpenAI(
        api_key=api_key,
        http_client=http_client
    )

# Create global OpenAI client
client = create_openai_client()


# Knowledge base state
KNOW: Dict = {}                      # Raw knowledge.json data
FLAT_TEXTS: List[str] = []           # All text passages for indexing
MAPPING: List[Dict] = []             # Mapping from text to original path
INDEX = None                         # FAISS or numpy index
INDEX_LOCK = threading.Lock()        # Thread safety for index operations

# Tour databases (USING Tour DATACLASS)
TOUR_NAME_TO_INDEX: Dict[str, int] = {}      # Normalized tour name â†’ index
TOURS_DB: Dict[int, Tour] = {}               # Structured tour database using Tour objects
TOUR_TAGS: Dict[int, List[str]] = {}         # Auto-generated tags for filtering

# Session management (USING ConversationContext DATACLASS)
SESSION_CONTEXTS: Dict[str, ConversationContext] = {}
SESSION_LOCK = threading.Lock()
SESSION_TIMEOUT = 1800  # 30 minutes

# Cache system
_response_cache: Dict[str, CacheEntry] = {}
_cache_lock = threading.Lock()

# Embedding cache (memory optimized)
_embedding_cache: Dict[str, Tuple[List[float], int]] = {}
_embedding_cache_lock = threading.Lock()
MAX_EMBEDDING_CACHE_SIZE = UpgradeFlags.get_all_flags()["EMBEDDING_CACHE_SIZE"]

# =========== MEMORY OPTIMIZATION FUNCTIONS ===========
def optimize_for_memory_profile():
    """Apply memory optimizations based on RAM profile"""
    flags = UpgradeFlags.get_all_flags()
    
    if IS_LOW_RAM:
        logger.info("ðŸ§  Low RAM mode (512MB) - optimizing memory usage")
        # Disable heavy preloading
        global FAISS_ENABLED
        FAISS_ENABLED = False
        
        # Reduce cache sizes
        import functools
        functools.lru_cache(maxsize=128)(embed_text)
        
        # Limit tour loading
        global MAX_TOURS_TO_LOAD
        MAX_TOURS_TO_LOAD = 50
        
    elif IS_HIGH_RAM:
        logger.info("ðŸš€ High RAM mode (2GB) - enabling all features")
        # Enable all features
        FAISS_ENABLED = HAS_FAISS
        MAX_TOURS_TO_LOAD = 1000
        
        # Increase cache sizes
        import functools
        functools.lru_cache(maxsize=flags["EMBEDDING_CACHE_SIZE"])(embed_text)

# =========== UPGRADE 1: MANDATORY FILTER SYSTEM (DATACLASS COMPATIBLE) ===========
class MandatoryFilterSystem:
    """
    UPGRADE 1: Extract and apply mandatory filters BEFORE semantic search
    """
    
    FILTER_PATTERNS = {
        'duration': [
            (r'(?:thá»i gian|máº¥y ngÃ y|bao lÃ¢u|kÃ©o dÃ i)\s*(?:lÃ \s*)?(\d+)\s*(?:ngÃ y|Ä‘Ãªm)', 'exact_duration'),
            (r'(\d+)\s*ngÃ y\s*(?:vÃ \s*)?(\d+)?\s*Ä‘Ãªm', 'days_nights'),
            (r'(\d+)\s*ngÃ y\s*(?:trá»Ÿ lÃªn|trá»Ÿ xuá»‘ng)', 'duration_range'),
            (r'(?:tour|hÃ nh trÃ¬nh)\s*(?:khoáº£ng|táº§m|khoáº£ng)?\s*(\d+)\s*ngÃ y', 'approx_duration'),
        ],
        
        'price': [
            (r'dÆ°á»›i\s*(\d[\d,\.]*)\s*(triá»‡u|tr|k|nghÃ¬n)', 'max_price'),
            (r'trÃªn\s*(\d[\d,\.]*)\s*(triá»‡u|tr|k|nghÃ¬n)', 'min_price'),
            (r'khoáº£ng\s*(\d[\d,\.]*)\s*(?:Ä‘áº¿n|-)\s*(\d[\d,\.]*)\s*(triá»‡u|tr|k|nghÃ¬n)', 'price_range'),
            (r'giÃ¡\s*(?:tá»«\s*)?(\d[\d,\.]*)\s*(?:Ä‘áº¿n|-|tá»›i)\s*(\d[\d,\.]*)\s*(triá»‡u|tr|k|nghÃ¬n)', 'price_range'),
            (r'(\d[\d,\.]*)\s*(triá»‡u|tr|k|nghÃ¬n)\s*trá»Ÿ xuá»‘ng', 'max_price'),
            (r'(\d[\d,\.]*)\s*(triá»‡u|tr|k|nghÃ¬n)\s*trá»Ÿ lÃªn', 'min_price'),
        ],
        
        'location': [
            (r'(?:á»Ÿ|táº¡i|vá»|Ä‘áº¿n|thÄƒm)\s+([^.,!?\n]+?)(?:\s|$|\.|,|!|\?)', 'location'),
            (r'(?:Ä‘iá»ƒm Ä‘áº¿n|Ä‘á»‹a Ä‘iá»ƒm|nÆ¡i|vÃ¹ng)\s+(?:lÃ \s*)?([^.,!?\n]+)', 'location'),
            (r'(?:quanh|gáº§n|khu vá»±c)\s+([^.,!?\n]+)', 'near_location'),
        ],
        
        'date_time': [
            (r'(?:thÃ¡ng|vÃ o)\s*(\d{1,2})', 'month'),
            (r'(?:cuá»‘i tuáº§n|weekend)', 'weekend'),
            (r'(?:dá»‹p|lá»…|táº¿t)\s+([^.,!?\n]+)', 'holiday'),
        ],
        
        'group_type': [
            (r'(?:gia Ä‘Ã¬nh|family)', 'family'),
            (r'(?:cáº·p Ä‘Ã´i|couple|Ä‘Ã´i lá»©a)', 'couple'),
            (r'(?:nhÃ³m báº¡n|báº¡n bÃ¨|friends)', 'friends'),
            (r'(?:cÃ´ng ty|doanh nghiá»‡p|team building)', 'corporate'),
            (r'(?:má»™t mÃ¬nh|Ä‘i láº»|solo)', 'solo'),
        ],
    }
    
    @staticmethod
    def extract_filters(message: str) -> FilterSet:
        """
        Extract ALL mandatory filters from user message
        """
        filters = FilterSet()
        
        if not message:
            return filters
        
        message_lower = message.lower()
        
        # 1. DURATION FILTERS
        for pattern, filter_type in MandatoryFilterSystem.FILTER_PATTERNS['duration']:
            matches = list(re.finditer(pattern, message_lower))
            for match in matches:
                if filter_type == 'exact_duration':
                    try:
                        days = int(match.group(1))
                        filters.duration_min = days
                        filters.duration_max = days
                    except (ValueError, IndexError):
                        pass
                elif filter_type == 'days_nights':
                    try:
                        days = int(match.group(1))
                        nights = int(match.group(2)) if match.group(2) else days
                        # Store in appropriate fields
                        filters.duration_min = days
                        filters.duration_max = days
                    except (ValueError, IndexError):
                        pass
        
        # 2. PRICE FILTERS
        for pattern, filter_type in MandatoryFilterSystem.FILTER_PATTERNS['price']:
            matches = list(re.finditer(pattern, message_lower))
            for match in matches:
                try:
                    if filter_type == 'max_price':
                        amount = MandatoryFilterSystem._parse_price(match.group(1), match.group(2))
                        if amount:
                            filters.price_max = amount
                            logger.info(f"ðŸ’° Extracted MAX price filter: {amount} VND")
                    
                    elif filter_type == 'min_price':
                        amount = MandatoryFilterSystem._parse_price(match.group(1), match.group(2))
                        if amount:
                            filters.price_min = amount
                            logger.info(f"ðŸ’° Extracted MIN price filter: {amount} VND")
                    
                    elif filter_type == 'price_range':
                        min_amount = MandatoryFilterSystem._parse_price(match.group(1), match.group(3))
                        max_amount = MandatoryFilterSystem._parse_price(match.group(2), match.group(3))
                        if min_amount and max_amount:
                            filters.price_min = min_amount
                            filters.price_max = max_amount
                            logger.info(f"ðŸ’° Extracted PRICE RANGE: {min_amount} - {max_amount} VND")
                
                except (ValueError, IndexError, AttributeError):
                    continue
        
        # 3. LOCATION FILTERS
        for pattern, filter_type in MandatoryFilterSystem.FILTER_PATTERNS['location']:
            matches = list(re.finditer(pattern, message_lower))
            for match in matches:
                location = match.group(1).strip()
                if location and len(location) > 1:
                    if filter_type == 'location':
                        filters.location = location
                    elif filter_type == 'near_location':
                        filters.near_location = location
        
        # 4. DATE/TIME FILTERS
        for pattern, filter_type in MandatoryFilterSystem.FILTER_PATTERNS['date_time']:
            matches = list(re.finditer(pattern, message_lower))
            for match in matches:
                if filter_type == 'month':
                    try:
                        filters.month = int(match.group(1))
                    except (ValueError, IndexError):
                        pass
                elif filter_type == 'weekend':
                    filters.weekend = True
                elif filter_type == 'holiday':
                    filters.holiday = match.group(1).strip()
        
        # 5. GROUP TYPE FILTERS
        for pattern, filter_type in MandatoryFilterSystem.FILTER_PATTERNS['group_type']:
            if re.search(pattern, message_lower):
                filters.group_type = filter_type
        
        # 6. SPECIAL KEYWORDS
        special_keywords = {
            'ráº»': ('price_max', 1500000),
            'giÃ¡ ráº»': ('price_max', 1500000),
            'tiáº¿t kiá»‡m': ('price_max', 1500000),
            'cao cáº¥p': ('price_min', 3000000),
            'sang trá»ng': ('price_min', 3000000),
            'premium': ('price_min', 3000000),
            'ngáº¯n ngÃ y': ('duration_max', 2),
            'dÃ i ngÃ y': ('duration_min', 3),
        }
        
        for keyword, (filter_key, value) in special_keywords.items():
            if keyword in message_lower:
                if filter_key == 'price_max':
                    filters.price_max = value
                elif filter_key == 'price_min':
                    filters.price_min = value
                elif filter_key == 'duration_max':
                    filters.duration_max = value
                elif filter_key == 'duration_min':
                    filters.duration_min = value
        
        logger.info(f"ðŸŽ¯ Extracted filters: {filters}")
        return filters
    
    @staticmethod
    def _parse_price(amount_str: str, unit: str) -> Optional[int]:
        """Parse price string like '1.5 triá»‡u' to integer VND"""
        if not amount_str:
            return None
        
        try:
            amount_str = amount_str.replace(',', '').replace('.', '')
            if not amount_str.isdigit():
                return None
            
            amount = int(amount_str)
            
            if unit in ['triá»‡u', 'tr']:
                return amount * 1000000
            elif unit == 'k':
                return amount * 1000
            elif unit == 'nghÃ¬n':
                return amount * 1000
            else:
                return amount if amount > 1000 else amount * 1000
        
        except (ValueError, AttributeError):
            return None
    
    @staticmethod
    def apply_filters(tours_db: Dict[int, Tour], filters: FilterSet) -> List[int]:
        """
        Apply mandatory filters to tour database
        Returns list of tour indices that pass ALL filters
        """
        if filters.is_empty() or not tours_db:
            return list(tours_db.keys())
        
        passing_tours = []
        
        try:
            for tour_idx, tour in tours_db.items():
                passes_all = True
                
                # PRICE FILTERING
                if passes_all and (filters.price_max is not None or filters.price_min is not None):
                    tour_price_text = tour.price or ""
                    if not tour_price_text:
                        if filters.price_max is not None or filters.price_min is not None:
                            passes_all = False
                    else:
                        tour_prices = MandatoryFilterSystem._extract_tour_prices(tour_price_text)
                        if not tour_prices:
                            passes_all = False
                        else:
                            min_tour_price = min(tour_prices)
                            max_tour_price = max(tour_prices)
                            
                            if filters.price_max is not None and min_tour_price > filters.price_max:
                                passes_all = False
                            if filters.price_min is not None and max_tour_price < filters.price_min:
                                passes_all = False
                
                # DURATION FILTERING
                if passes_all and (filters.duration_min is not None or filters.duration_max is not None):
                    duration_text = (tour.duration or "").lower()
                    tour_duration = MandatoryFilterSystem._extract_duration_days(duration_text)
                    
                    if tour_duration is not None:
                        if filters.duration_min is not None and tour_duration < filters.duration_min:
                            passes_all = False
                        if filters.duration_max is not None and tour_duration > filters.duration_max:
                            passes_all = False
                    else:
                        if filters.duration_min is not None or filters.duration_max is not None:
                            passes_all = False
                
                # LOCATION FILTERING
                if passes_all and (filters.location is not None or filters.near_location is not None):
                    tour_location = (tour.location or "").lower()
                    if filters.location is not None:
                        filter_location = filters.location.lower()
                        if filter_location not in tour_location:
                            passes_all = False
                    if filters.near_location is not None:
                        near_location = filters.near_location.lower()
                        if near_location not in tour_location:
                            passes_all = False
                
                if passes_all:
                    passing_tours.append(tour_idx)
            
            logger.info(f"ðŸ” After mandatory filtering: {len(passing_tours)}/{len(tours_db)} tours pass")
        except Exception as e:
            logger.error(f"âŒ Error in apply_filters: {e}")
            passing_tours = list(tours_db.keys())
        
        return passing_tours
    
    @staticmethod
    def _extract_tour_prices(price_text: str) -> List[int]:
        """Extract price numbers from tour price text"""
        prices = []
        
        number_patterns = [
            r'(\d[\d,\.]+)\s*(?:triá»‡u|tr)',
            r'(\d[\d,\.]+)\s*(?:k|nghÃ¬n)',
            r'(\d[\d,\.]+)\s*(?:Ä‘á»“ng|vnÄ‘|vnd)',
            r'(\d[\d,\.]+)\s*-\s*(\d[\d,\.]+)',
        ]
        
        for pattern in number_patterns:
            matches = re.finditer(pattern, price_text, re.IGNORECASE)
            for match in matches:
                try:
                    for i in range(1, 3):
                        if match.group(i):
                            num_str = match.group(i).replace(',', '').replace('.', '')
                            if num_str.isdigit():
                                num = int(num_str)
                                
                                if 'triá»‡u' in match.group(0).lower() or 'tr' in match.group(0).lower():
                                    num = num * 1000000
                                elif 'k' in match.group(0).lower() or 'nghÃ¬n' in match.group(0).lower():
                                    num = num * 1000
                                
                                prices.append(num)
                except (ValueError, AttributeError):
                    continue
        
        if not prices:
            raw_numbers = re.findall(r'\d[\d,\.]+', price_text)
            for num_str in raw_numbers[:2]:
                try:
                    num_str = num_str.replace(',', '').replace('.', '')
                    if num_str.isdigit():
                        num = int(num_str)
                        if 100 <= num <= 10000:
                            num = num * 1000
                        prices.append(num)
                except ValueError:
                    continue
        
        return prices
    
    @staticmethod
    def _extract_duration_days(duration_text: str) -> Optional[int]:
        """Extract duration in days from text"""
        if not duration_text:
            return None
        
        patterns = [
            r'(\d+)\s*ngÃ y',
            r'(\d+)\s*ngÃ y\s*\d*\s*Ä‘Ãªm',
            r'(\d+)\s*Ä‘Ãªm',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, duration_text)
            if match:
                try:
                    return int(match.group(1))
                except (ValueError, IndexError):
                    continue
        
        return None

# =========== UPGRADE 2: DEDUPLICATION ENGINE (DATACLASS COMPATIBLE) ===========
class DeduplicationEngine:
    """
    UPGRADE 2: Remove duplicate and highly similar results
    """
    
    SIMILARITY_THRESHOLD = 0.85
    MIN_TEXT_LENGTH = 20
    
    @staticmethod
    def calculate_similarity(text1: str, text2: str) -> float:
        """Calculate text similarity using multiple methods"""
        if not text1 or not text2:
            return 0.0
        
        text1_norm = DeduplicationEngine._normalize_text(text1)
        text2_norm = DeduplicationEngine._normalize_text(text2)
        
        if len(text1_norm) < DeduplicationEngine.MIN_TEXT_LENGTH or len(text2_norm) < DeduplicationEngine.MIN_TEXT_LENGTH:
            return 0.0
        
        seq_ratio = SequenceMatcher(None, text1_norm, text2_norm).ratio()
        
        words1 = set(text1_norm.split())
        words2 = set(text2_norm.split())
        
        if not words1 or not words2:
            jaccard = 0.0
        else:
            intersection = words1.intersection(words2)
            union = words1.union(words2)
            jaccard = len(intersection) / len(union)
        
        prefix_len = min(50, min(len(text1_norm), len(text2_norm)))
        prefix1 = text1_norm[:prefix_len]
        prefix2 = text2_norm[:prefix_len]
        prefix_sim = SequenceMatcher(None, prefix1, prefix2).ratio()
        
        similarity = (seq_ratio * 0.5) + (jaccard * 0.3) + (prefix_sim * 0.2)
        
        return similarity
    
    @staticmethod
    def _normalize_text(text: str) -> str:
        """Normalize text for comparison"""
        if not text:
            return ""
        
        text = text.lower()
        text = unicodedata.normalize('NFD', text)
        text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        stopwords = {'vÃ ', 'cá»§a', 'cho', 'vá»›i', 'táº¡i', 'á»Ÿ', 'nÃ y', 'Ä‘Ã³', 'kia', 'vá»', 'trong'}
        words = [word for word in text.split() if word not in stopwords]
        
        return ' '.join(words)
    
    @staticmethod
    def deduplicate_passages(passages: List[Tuple[float, Dict]], 
                            similarity_threshold: float = None) -> List[Tuple[float, Dict]]:
        """
        Remove duplicate passages from results
        """
        if len(passages) <= 1:
            return passages
        
        threshold = similarity_threshold or DeduplicationEngine.SIMILARITY_THRESHOLD
        unique_passages = []
        seen_passages = []
        
        sorted_passages = sorted(passages, key=lambda x: x[0], reverse=True)
        
        for score, passage in sorted_passages:
            text = passage.get('text', '').strip()
            path = passage.get('path', '')
            
            if not text or len(text) < DeduplicationEngine.MIN_TEXT_LENGTH:
                unique_passages.append((score, passage))
                continue
            
            is_duplicate = False
            for seen_text, seen_path in seen_passages:
                tour_match1 = re.search(r'tours\[(\d+)\]', path)
                tour_match2 = re.search(r'tours\[(\d+)\]', seen_path)
                
                if tour_match1 and tour_match2:
                    if tour_match1.group(1) == tour_match2.group(1):
                        field1 = path.split('.')[-1] if '.' in path else ''
                        field2 = seen_path.split('.')[-1] if '.' in seen_path else ''
                        if field1 == field2:
                            is_duplicate = True
                            break
                
                similarity = DeduplicationEngine.calculate_similarity(text, seen_text)
                if similarity > threshold:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_passages.append((score, passage))
                seen_passages.append((text, path))
        
        logger.info(f"ðŸ”„ Deduplication: {len(passages)} â†’ {len(unique_passages)} passages")
        return unique_passages
    
    @staticmethod
    def merge_similar_tours(tour_indices: List[int], tours_db: Dict[int, Tour]) -> List[int]:
        """Merge tours that are essentially the same"""
        if len(tour_indices) <= 1:
            return tour_indices
        
        tour_groups = []
        processed = set()
        
        for i, idx1 in enumerate(tour_indices):
            if idx1 in processed:
                continue
            
            group = [idx1]
            tour1 = tours_db.get(idx1)
            name1 = (tour1.name if tour1 else "").strip()
            
            if not name1:
                processed.add(idx1)
                tour_groups.append(group)
                continue
            
            for j, idx2 in enumerate(tour_indices[i+1:], i+1):
                if idx2 in processed:
                    continue
                
                tour2 = tours_db.get(idx2)
                name2 = (tour2.name if tour2 else "").strip()
                
                if not name2:
                    continue
                
                similarity = DeduplicationEngine.calculate_similarity(name1, name2)
                if similarity > 0.9:
                    group.append(idx2)
                    processed.add(idx2)
            
            processed.add(idx1)
            tour_groups.append(group)
        
        best_tours = []
        for group in tour_groups:
            if not group:
                continue
            
            if len(group) == 1:
                best_tours.append(group[0])
                continue
            
            best_score = -1
            best_idx = group[0]
            
            for idx in group:
                tour = tours_db.get(idx)
                if not tour:
                    continue
                
                score = 0
                
                if tour.name:
                    score += 2
                if tour.duration:
                    score += 2
                if tour.location:
                    score += 2
                if tour.price:
                    score += 3
                if tour.includes:
                    score += 2
                if tour.summary:
                    score += 1
                
                for field in [tour.includes, tour.summary, tour.notes]:
                    if isinstance(field, str) and len(field) > 50:
                        score += 1
                    elif isinstance(field, list) and field:
                        score += len(field)
                
                if score > best_score:
                    best_score = score
                    best_idx = idx
            
            best_tours.append(best_idx)
        
        logger.info(f"ðŸ”„ Tour merging: {len(tour_indices)} â†’ {len(best_tours)} unique tours")
        return best_tours

# =========== UPGRADE 3: ENHANCED FIELD DETECTION (DATACLASS COMPATIBLE) ===========
class EnhancedFieldDetector:
    """
    UPGRADE 3: Better detection of what user is asking for
    """
    
    FIELD_DETECTION_RULES = [
        # TOUR LIST
        {
            "field": "tour_name",
            "patterns": [
                (r'liá»‡t kÃª.*tour|danh sÃ¡ch.*tour|cÃ¡c tour|cÃ³ nhá»¯ng tour nÃ o', 1.0),
                (r'tour nÃ o.*cÃ³|tour nÃ o.*hiá»‡n|tour nÃ o.*Ä‘ang', 0.9),
                (r'ká»ƒ tÃªn.*tour|nÃªu tÃªn.*tour|tÃªn cÃ¡c tour', 0.9),
                (r'cÃ³ máº¥y.*tour|bao nhiÃªu.*tour|sá»‘ lÆ°á»£ng.*tour', 0.8),
                (r'list tour|show tour|all tour|every tour', 0.8),
            ],
            "keywords": [
                ("liá»‡t kÃª", 0.9), ("danh sÃ¡ch", 0.9), ("cÃ¡c", 0.7),
                ("táº¥t cáº£", 0.8), ("má»i", 0.7), ("máº¥y", 0.6),
                ("bao nhiÃªu", 0.7), ("sá»‘ lÆ°á»£ng", 0.7),
            ]
        },
        
        # PRICE
        {
            "field": "price",
            "patterns": [
                (r'giÃ¡.*bao nhiÃªu|bao nhiÃªu tiá»n|chi phÃ­.*bao nhiÃªu', 1.0),
                (r'giÃ¡ tour|giÃ¡ cáº£|giÃ¡ thÃ nh|chi phÃ­ tour', 0.9),
                (r'tour.*giÃ¡.*bao nhiÃªu|tour.*bao nhiÃªu tiá»n', 0.95),
                (r'pháº£i tráº£.*bao nhiÃªu|tá»‘n.*bao nhiÃªu|máº¥t.*bao nhiÃªu', 0.8),
                (r'Ä‘Ã³ng.*bao nhiÃªu|thanh toÃ¡n.*bao nhiÃªu', 0.8),
            ],
            "keywords": [
                ("giÃ¡", 0.8), ("tiá»n", 0.7), ("chi phÃ­", 0.8),
                ("Ä‘Ã³ng", 0.6), ("tráº£", 0.6), ("tá»‘n", 0.6),
                ("phÃ­", 0.7), ("kinh phÃ­", 0.7), ("tá»•ng chi", 0.7),
            ]
        },
        
        # DURATION
        {
            "field": "duration",
            "patterns": [
                (r'thá»i gian.*bao lÃ¢u|máº¥y ngÃ y.*Ä‘i|bao lÃ¢u.*tour', 1.0),
                (r'tour.*bao nhiÃªu ngÃ y|máº¥y ngÃ y.*tour', 0.9),
                (r'Ä‘i trong.*bao lÃ¢u|kÃ©o dÃ i.*bao lÃ¢u', 0.9),
                (r'thá»i lÆ°á»£ng.*bao nhiÃªu|thá»i gian.*dÃ i bao lÃ¢u', 0.8),
            ],
            "keywords": [
                ("bao lÃ¢u", 0.9), ("máº¥y ngÃ y", 0.9), ("thá»i gian", 0.8),
                ("kÃ©o dÃ i", 0.7), ("thá»i lÆ°á»£ng", 0.8), ("ngÃ y", 0.6),
                ("Ä‘Ãªm", 0.6), ("thá»i háº¡n", 0.7),
            ]
        },
        
        # LOCATION
        {
            "field": "location",
            "patterns": [
                (r'á»Ÿ Ä‘Ã¢u|Ä‘i Ä‘Ã¢u|Ä‘áº¿n Ä‘Ã¢u|tá»›i Ä‘Ã¢u|thÄƒm quan Ä‘Ã¢u', 1.0),
                (r'Ä‘á»‹a Ä‘iá»ƒm.*nÃ o|nÆ¡i nÃ o|vÃ¹ng nÃ o|khu vá»±c nÃ o', 0.9),
                (r'tour.*á»Ÿ.*Ä‘Ã¢u|hÃ nh trÃ¬nh.*Ä‘i.*Ä‘Ã¢u', 0.9),
                (r'khÃ¡m phÃ¡.*Ä‘Ã¢u|thÄƒm.*Ä‘Ã¢u|ghÃ©.*Ä‘Ã¢u', 0.8),
            ],
            "keywords": [
                ("á»Ÿ Ä‘Ã¢u", 1.0), ("Ä‘i Ä‘Ã¢u", 1.0), ("Ä‘áº¿n Ä‘Ã¢u", 0.9),
                ("tá»›i Ä‘Ã¢u", 0.9), ("Ä‘á»‹a Ä‘iá»ƒm", 0.8), ("nÆ¡i", 0.7),
                ("vÃ¹ng", 0.7), ("khu vá»±c", 0.7),
            ]
        },
        
        # SUMMARY
        {
            "field": "summary",
            "patterns": [
                (r'cÃ³ gÃ¬ hay|cÃ³ gÃ¬ Ä‘áº·c biá»‡t|cÃ³ gÃ¬ thÃº vá»‹', 0.9),
                (r'tour nÃ y tháº¿ nÃ o|hÃ nh trÃ¬nh ra sao|chuyáº¿n Ä‘i nhÆ° nÃ o', 0.8),
                (r'giá»›i thiá»‡u.*tour|mÃ´ táº£.*tour|nÃ³i vá».*tour', 0.8),
                (r'tour.*cÃ³ gÃ¬|Ä‘i.*Ä‘Æ°á»£c gÃ¬|tráº£i nghiá»‡m.*gÃ¬', 0.7),
                (r'Ä‘iá»ƒm nháº¥n.*tour|ná»•i báº­t.*gÃ¬|Ä‘áº·c sáº¯c.*gÃ¬', 0.7),
            ],
            "keywords": [
                ("cÃ³ gÃ¬", 0.7), ("tháº¿ nÃ o", 0.6), ("ra sao", 0.6),
                ("giá»›i thiá»‡u", 0.7), ("mÃ´ táº£", 0.7), ("nÃ³i vá»", 0.6),
                ("Ä‘iá»ƒm nháº¥n", 0.7), ("ná»•i báº­t", 0.7), ("Ä‘áº·c sáº¯c", 0.7),
            ]
        },
        
        # INCLUDES
        {
            "field": "includes",
            "patterns": [
                (r'lá»‹ch trÃ¬nh.*chi tiáº¿t|chÆ°Æ¡ng trÃ¬nh.*chi tiáº¿t', 0.9),
                (r'lÃ m gÃ¬.*tour|hoáº¡t Ä‘á»™ng.*gÃ¬|sinh hoáº¡t.*gÃ¬', 0.8),
                (r'tour.*gá»“m.*gÃ¬|bao gá»“m.*gÃ¬|gá»“m nhá»¯ng gÃ¬', 0.8),
                (r'Ä‘i Ä‘Ã¢u.*lÃ m gÃ¬|thÄƒm quan.*gÃ¬|khÃ¡m phÃ¡.*gÃ¬', 0.7),
            ],
            "keywords": [
                ("lá»‹ch trÃ¬nh", 0.8), ("chÆ°Æ¡ng trÃ¬nh", 0.8), ("lÃ m gÃ¬", 0.7),
                ("hoáº¡t Ä‘á»™ng", 0.7), ("sinh hoáº¡t", 0.6), ("gá»“m", 0.6),
                ("bao gá»“m", 0.7), ("gá»“m nhá»¯ng", 0.7),
            ]
        },
    ]
    
    @staticmethod
    def detect_field_with_confidence(message: str) -> Tuple[Optional[str], float, Dict[str, float]]:
        """
        Detect which field user is asking about with confidence scores
        """
        if not message:
            return None, 0.0, {}
        
        message_lower = message.lower()
        scores = {}
        
        for rule in EnhancedFieldDetector.FIELD_DETECTION_RULES:
            field = rule["field"]
            field_score = 0.0
            
            for pattern, weight in rule["patterns"]:
                if re.search(pattern, message_lower):
                    field_score = max(field_score, weight)
            
            for keyword, weight in rule["keywords"]:
                if keyword in message_lower:
                    position = message_lower.find(keyword)
                    position_factor = 1.0 - (position / max(len(message_lower), 1))
                    adjusted_weight = weight * (0.7 + 0.3 * position_factor)
                    field_score = max(field_score, adjusted_weight)
            
            if field_score > 0:
                field_score = min(field_score * 1.1, 1.0)
            
            scores[field] = field_score
        
        best_field = None
        best_score = 0.0
        
        for field, score in scores.items():
            if score > best_score:
                best_score = score
                best_field = field
        
        if (best_score < 0.3 and 
            ("cÃ³ gÃ¬" in message_lower or "tháº¿ nÃ o" in message_lower) and
            "tour" in message_lower):
            best_field = "summary"
            best_score = 0.6
        
        logger.info(f"ðŸ” Field detection: '{message}' â†’ {best_field} (confidence: {best_score:.2f})")
        return best_field, best_score, scores

# =========== UPGRADE 4: QUESTION PIPELINE (DATACLASS COMPATIBLE) ===========
class QuestionPipeline:
    """
    UPGRADE 4: Process different types of questions differently
    """
    
    @staticmethod
    def classify_question(message: str) -> Tuple[QuestionType, float, Dict]:
        """
        Classify question type with confidence and metadata
        """
        message_lower = message.lower()
        type_scores = defaultdict(float)
        metadata = {}
        
        # LISTING detection - CHá»ˆ khi yÃªu cáº§u rÃµ rÃ ng liá»‡t kÃª DANH SÃCH
        listing_patterns = [
            (r'liá»‡t kÃª.*táº¥t cáº£.*tour|danh sÃ¡ch.*táº¥t cáº£.*tour|táº¥t cáº£.*tour', 0.95),
            (r'liá»‡t kÃª.*tour|danh sÃ¡ch.*tour|list.*tour', 0.9),
            (r'ká»ƒ tÃªn.*tour|nÃªu tÃªn.*tour', 0.9),
            (r'cÃ³ nhá»¯ng.*tour nÃ o|cÃ³ máº¥y.*tour|máº¥y.*tour', 0.7),
            (r'bÃªn báº¡n.*cÃ³.*tour|hiá»‡n cÃ³.*tour', 0.75),
        ]
        
        for pattern, weight in listing_patterns:
            if re.search(pattern, message_lower):
                type_scores[QuestionType.LISTING] = max(
                    type_scores[QuestionType.LISTING], weight
                )
        
        # COMPARISON detection
        comparison_patterns = [
            (r'so sÃ¡nh.*vÃ |Ä‘á»‘i chiáº¿u.*vÃ ', 0.95),
            (r'khÃ¡c nhau.*nÃ o|giá»‘ng nhau.*nÃ o', 0.9),
            (r'nÃªn chá»n.*nÃ o|tá»‘t hÆ¡n.*nÃ o|hÆ¡n kÃ©m.*nÃ o', 0.85),
            (r'tour.*vÃ .*tour', 0.8),
            (r'sÃ¡nh.*vá»›i|Ä‘á»‘i chiáº¿u.*vá»›i', 0.8),
        ]
        
        for pattern, weight in comparison_patterns:
            if re.search(pattern, message_lower):
                type_scores[QuestionType.COMPARISON] = max(
                    type_scores[QuestionType.COMPARISON], weight
                )
                metadata['comparison_type'] = 'direct'
        
        # RECOMMENDATION detection
        recommendation_patterns = [
            (r'phÃ¹ há»£p.*vá»›i|nÃªn Ä‘i.*nÃ o|gá»£i Ã½.*tour', 0.95),
            (r'tour nÃ o.*phÃ¹ há»£p|phÃ¹ há»£p.*tour nÃ o', 0.95),
            (r'tour.*tá»‘t.*nháº¥t|hÃ nh trÃ¬nh.*hay nháº¥t|tour.*lÃ½ tÆ°á»Ÿng', 0.9),
            (r'Ä‘á» xuáº¥t.*tour|tÆ° váº¥n.*tour|chá»n.*tour nÃ o', 0.9),
            (r'tour nÃ o.*cho.*gia Ä‘Ã¬nh|tour.*gia Ä‘Ã¬nh|gia Ä‘Ã¬nh.*tour', 0.9),
            (r'tour nÃ o.*cho|dÃ nh cho.*tour|tour.*dÃ nh cho', 0.85),
            (r'nÃªn.*tour nÃ o|nÃªn chá»n.*tour|tour.*nÃªn', 0.85),
            (r'tour.*nháº¹ nhÃ ng|tour.*dá»…|tour.*phÃ¹ há»£p.*ngÆ°á»i', 0.85),
            (r'tour.*tráº» em|tour.*con nÃ­t|tour.*bÃ©', 0.85),
            (r'tour.*ngÆ°á»i lá»›n tuá»•i|tour.*cao tuá»•i|tour.*nghá»‰ dÆ°á»¡ng', 0.85),
            (r'chi phÃ­.*vá»«a pháº£i|giÃ¡.*phÃ¹ há»£p|giÃ¡.*há»£p lÃ½', 0.8),
            (r'cho.*tÃ´i|dÃ nh cho.*tÃ´i|há»£p vá»›i.*tÃ´i', 0.75),
            (r'náº¿u.*thÃ¬.*nÃªn.*tour|nÃªn chá»n.*tour', 0.8),
        ]
        
        for pattern, weight in recommendation_patterns:
            if re.search(pattern, message_lower):
                type_scores[QuestionType.RECOMMENDATION] = max(
                    type_scores[QuestionType.RECOMMENDATION], weight
                )
        
        # GREETING detection
        greeting_words = ['xin chÃ o', 'chÃ o', 'hello', 'hi', 'helo', 'chao']
        greeting_score = 0.0
        for word in greeting_words:
            if word in message_lower:
                if message_lower.startswith(word) or f" {word} " in message_lower or message_lower.endswith(f" {word}"):
                    greeting_score += 0.3
        
        other_intent_score = max([score for qtype, score in type_scores.items() 
                                 if qtype != QuestionType.GREETING], default=0.0)
        
        if greeting_score > 0.8 and other_intent_score < 0.3:
            type_scores[QuestionType.GREETING] = min(greeting_score, 1.0)
        
        # FAREWELL detection
        farewell_words = ['táº¡m biá»‡t', 'cáº£m Æ¡n', 'thanks', 'thank you', 'bye', 'goodbye']
        if any(word in message_lower for word in farewell_words):
            type_scores[QuestionType.FAREWELL] = 0.95
        
        # CALCULATION detection
        calculation_patterns = [
            (r'tÃ­nh toÃ¡n|tÃ­nh.*bao nhiÃªu|tá»•ng.*bao nhiÃªu', 0.9),
            (r'cá»™ng.*láº¡i|nhÃ¢n.*lÃªn|chia.*ra', 0.8),
            (r'bao nhiÃªu.*ngÆ°á»i|máº¥y.*ngÆ°á»i|sá»‘ lÆ°á»£ng.*ngÆ°á»i', 0.7),
        ]
        
        for pattern, weight in calculation_patterns:
            if re.search(pattern, message_lower):
                type_scores[QuestionType.CALCULATION] = max(
                    type_scores[QuestionType.CALCULATION], weight
                )
        
        # COMPLEX question detection
        complex_indicators = [
            ('vÃ ', 0.3), ('rá»“i', 0.4), ('sau Ä‘Ã³', 0.5),
            ('tiáº¿p theo', 0.5), ('ngoÃ i ra', 0.4), ('thÃªm ná»¯a', 0.4),
        ]
        
        complex_score = 0.0
        for indicator, weight in complex_indicators:
            if indicator in message_lower:
                complex_score += weight
        
        if complex_score > 0.8:
            type_scores[QuestionType.COMPLEX] = min(complex_score / 2, 1.0)
            metadata['complex_parts'] = QuestionPipeline._split_complex_question(message)
        
        # DEFAULT: INFORMATION request
        if not type_scores:
            type_scores[QuestionType.INFORMATION] = 0.6
        else:
            info_keywords = ['lÃ  gÃ¬', 'bao nhiÃªu', 'á»Ÿ Ä‘Ã¢u', 'khi nÃ o', 'tháº¿ nÃ o', 'ai', 'táº¡i sao']
            if any(keyword in message_lower for keyword in info_keywords):
                type_scores[QuestionType.INFORMATION] = max(
                    type_scores.get(QuestionType.INFORMATION, 0),
                    0.5
                )
        
        # Determine best type
        best_type = QuestionType.INFORMATION
        best_score = 0.0
        
        for qtype, score in type_scores.items():
            if score > best_score:
                best_score = score
                best_type = qtype
        
        if best_score < 0.5:
            best_type = QuestionType.INFORMATION
            best_score = 0.5
        
        logger.info(f"ðŸŽ¯ Question classification: '{message}' â†’ {best_type.value} (score: {best_score:.2f})")
        return best_type, best_score, metadata
    
    @staticmethod
    def _split_complex_question(message: str) -> List[str]:
        """Split complex multi-part question into simpler parts"""
        split_patterns = [
            r'\s+vÃ \s+',
            r'\s+rá»“i\s+',
            r'\s+sau Ä‘Ã³\s+',
            r'\s+tiáº¿p theo\s+',
            r'\s+ngoÃ i ra\s+',
            r'\s+thÃªm ná»¯a\s+',
            r'\s+Ä‘á»“ng thá»i\s+',
            r'\s+cuá»‘i cÃ¹ng\s+',
        ]
        
        parts = [message]
        
        for pattern in split_patterns:
            new_parts = []
            for part in parts:
                split_result = re.split(pattern, part, flags=re.IGNORECASE)
                new_parts.extend([p.strip() for p in split_result if p.strip()])
            parts = new_parts
        
        return parts
    
    @staticmethod
    def process_comparison_question(tour_indices: List[int], tours_db: Dict[int, Tour], 
                                  aspect: str = "", context: Dict = None) -> str:
        """
        Process comparison question between tours
        """
        if len(tour_indices) < 2:
            return "Cáº§n Ã­t nháº¥t 2 tour Ä‘á»ƒ so sÃ¡nh."
        
        tours_to_compare = []
        for idx in tour_indices[:3]:
            tour = tours_db.get(idx)
            if tour:
                tours_to_compare.append((idx, tour))
        
        if len(tours_to_compare) < 2:
            return "KhÃ´ng tÃ¬m tháº¥y Ä‘á»§ thÃ´ng tin tour Ä‘á»ƒ so sÃ¡nh."
        
        if not aspect:
            aspect = 'price'
        
        result_lines = []
        
        headers = ["TIÃŠU CHÃ"]
        for idx, tour in tours_to_compare:
            tour_name = tour.name or f'Tour #{idx}'
            headers.append(tour_name[:25])
        
        result_lines.append(" | ".join(headers))
        result_lines.append("-" * (len(headers) * 30))
        
        comparison_fields = [
            ('duration', 'â±ï¸ Thá»i gian'),
            ('location', 'ðŸ“ Äá»‹a Ä‘iá»ƒm'),
            ('price', 'ðŸ’° GiÃ¡ tour'),
            ('accommodation', 'ðŸ¨ Chá»— á»Ÿ'),
            ('meals', 'ðŸ½ï¸ Ä‚n uá»‘ng'),
            ('transport', 'ðŸš— Di chuyá»ƒn'),
            ('summary', 'ðŸ“ MÃ´ táº£'),
        ]
        
        for field, display_name in comparison_fields:
            if aspect and field != aspect and aspect not in ['all', 'táº¥t cáº£']:
                continue
            
            row = [display_name]
            all_values = []
            
            for idx, tour in tours_to_compare:
                value = getattr(tour, field, 'N/A')
                if isinstance(value, list):
                    value = ', '.join(value[:2])
                row.append(str(value)[:30])
                all_values.append(str(value).lower())
            
            if len(set(all_values)) > 1 or aspect == field:
                result_lines.append(" | ".join(row))
        
        result_lines.append("\n" + "="*50)
        result_lines.append("**ÄÃNH GIÃ & Gá»¢I Ã:**")
        
        durations = [tour.duration for _, tour in tours_to_compare]
        if any('1 ngÃ y' in d for d in durations) and any('2 ngÃ y' in d for d in durations):
            result_lines.append("â€¢ Náº¿u báº¡n cÃ³ Ã­t thá»i gian: Chá»n tour 1 ngÃ y")
            result_lines.append("â€¢ Náº¿u muá»‘n tráº£i nghiá»‡m sÃ¢u: Chá»n tour 2 ngÃ y")
        
        prices = []
        for _, tour in tours_to_compare:
            price_text = tour.price or ''
            price_nums = re.findall(r'\d[\d,\.]+', price_text)
            if price_nums:
                try:
                    price = int(price_nums[0].replace(',', '').replace('.', ''))
                    prices.append(price)
                except:
                    pass
        
        if len(prices) >= 2:
            min_price_idx = prices.index(min(prices))
            max_price_idx = prices.index(max(prices))
            
            if prices[max_price_idx] > prices[min_price_idx] * 1.5:
                result_lines.append(f"â€¢ Tiáº¿t kiá»‡m chi phÃ­: {headers[min_price_idx + 1]}")
                result_lines.append(f"â€¢ Tráº£i nghiá»‡m cao cáº¥p: {headers[max_price_idx + 1]}")
        
        result_lines.append("\nðŸ’¡ *LiÃªn há»‡ hotline 0332510486 Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n chi tiáº¿t*")
        
        return "\n".join(result_lines)

# =========== UPGRADE 5: COMPLEX QUERY SPLITTER (DATACLASS COMPATIBLE) ===========
class ComplexQueryProcessor:
    """
    UPGRADE 5: Handle complex multi-condition queries
    """
    
    @staticmethod
    def split_query(query: str) -> List[Dict[str, Any]]:
        """
        Split complex query into sub-queries with priorities
        """
        sub_queries = []
        
        complexity_score = ComplexQueryProcessor._calculate_complexity(query)
        if complexity_score < 1.5:
            return [{
                'query': query,
                'priority': 1.0,
                'filters': {},
                'focus': 'general'
            }]
        
        conditions = ComplexQueryProcessor._extract_conditions(query)
        
        if len(conditions) <= 1:
            return [{
                'query': query,
                'priority': 1.0,
                'filters': conditions[0] if conditions else {},
                'focus': 'general'
            }]
        
        sub_queries.append({
            'query': query,
            'priority': 1.0,
            'filters': ComplexQueryProcessor._merge_conditions(conditions),
            'focus': 'specific'
        })
        
        location_conds = [c for c in conditions if 'location' in c]
        other_conds = [c for c in conditions if 'location' not in c]
        
        if location_conds and other_conds:
            for other_cond in other_conds[:2]:
                merged = ComplexQueryProcessor._merge_conditions(location_conds + [other_cond])
                sub_queries.append({
                    'query': f"{query} (focus on location + {list(other_cond.keys())[0]})",
                    'priority': 0.8,
                    'filters': merged,
                    'focus': list(other_cond.keys())[0]
                })
        
        important_conds = ['price', 'duration', 'location']
        for cond_type in important_conds:
            conds_of_type = [c for c in conditions if cond_type in c]
            if conds_of_type:
                sub_queries.append({
                    'query': f"{query} (focus on {cond_type})",
                    'priority': 0.6,
                    'filters': conds_of_type[0],
                    'focus': cond_type
                })
        
        sub_queries.sort(key=lambda x: x['priority'], reverse=True)
        
        logger.info(f"ðŸ”€ Split query into {len(sub_queries)} sub-queries")
        return sub_queries[:3]
    
    @staticmethod
    def _calculate_complexity(query: str) -> float:
        """Calculate how complex a query is"""
        complexity = 0.0
        
        aspects = {
            'price': ['giÃ¡', 'tiá»n', 'chi phÃ­', 'Ä‘áº¯t', 'ráº»'],
            'duration': ['ngÃ y', 'Ä‘Ãªm', 'bao lÃ¢u', 'thá»i gian'],
            'location': ['á»Ÿ', 'táº¡i', 'Ä‘áº¿n', 'vá»', 'Ä‘á»‹a Ä‘iá»ƒm'],
            'quality': ['tá»‘t', 'hay', 'Ä‘áº¹p', 'háº¥p dáº«n', 'thÃº vá»‹'],
            'type': ['thiá»n', 'khÃ­ cÃ´ng', 'retreat', 'chá»¯a lÃ nh'],
        }
        
        query_lower = query.lower()
        
        distinct_aspects = 0
        for aspect, keywords in aspects.items():
            if any(keyword in query_lower for keyword in keywords):
                distinct_aspects += 1
        
        complexity += distinct_aspects * 0.5
        complexity += min(len(query.split()) / 10, 1.0)
        
        conjunctions = ['vÃ ', 'vá»›i', 'cÃ³', 'cho', 'mÃ ', 'nhÆ°ng']
        for conj in conjunctions:
            if conj in query_lower:
                complexity += 0.3
        
        return complexity
    
    @staticmethod
    def _extract_conditions(query: str) -> List[Dict[str, Any]]:
        """Extract individual conditions from query"""
        conditions = []
        
        filters = MandatoryFilterSystem.extract_filters(query)
        
        if filters.price_min is not None or filters.price_max is not None:
            price_cond = {'price': {}}
            if filters.price_min is not None:
                price_cond['price']['min'] = filters.price_min
            if filters.price_max is not None:
                price_cond['price']['max'] = filters.price_max
            conditions.append(price_cond)
        
        if filters.duration_min is not None or filters.duration_max is not None:
            duration_cond = {'duration': {}}
            if filters.duration_min is not None:
                duration_cond['duration']['min'] = filters.duration_min
            if filters.duration_max is not None:
                duration_cond['duration']['max'] = filters.duration_max
            conditions.append(duration_cond)
        
        if filters.location:
            conditions.append({'location': filters.location})
        if filters.near_location:
            conditions.append({'near_location': filters.near_location})
        
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['ráº»', 'giÃ¡ ráº»', 'tiáº¿t kiá»‡m']):
            conditions.append({'price_quality': 'budget'})
        if any(word in query_lower for word in ['cao cáº¥p', 'sang', 'premium']):
            conditions.append({'price_quality': 'premium'})
        
        if 'thiá»n' in query_lower:
            conditions.append({'activity_type': 'meditation'})
        if 'khÃ­ cÃ´ng' in query_lower:
            conditions.append({'activity_type': 'qigong'})
        if 'retreat' in query_lower:
            conditions.append({'activity_type': 'retreat'})
        if 'chá»¯a lÃ nh' in query_lower:
            conditions.append({'activity_type': 'healing'})
        
        tour_name_patterns = [
            r'tour\s+([^vÃ \s,]+)\s+vÃ \s+tour\s+([^\s,]+)',
            r'tour\s+([^\s,]+)\s+vá»›i\s+tour\s+([^\s,]+)',
            r'tour\s+([^\s,]+)\s+.*tour\s+([^\s,]+)',
        ]
        
        for pattern in tour_name_patterns:
            matches = re.finditer(pattern, query_lower)
            for match in matches:
                for i in range(1, 3):
                    if match.group(i):
                        tour_name = match.group(i).strip()
                        normalized_name = FuzzyMatcher.normalize_vietnamese(tour_name)
                        for name, idx in TOUR_NAME_TO_INDEX.items():
                            if normalized_name in name or name in normalized_name:
                                conditions.append({'specific_tour': idx})
                                logger.info(f"ðŸ” Extracted tour name from complex query: {tour_name} â†’ index {idx}")
        
        return conditions
    
    @staticmethod
    def _merge_conditions(conditions: List[Dict]) -> Dict[str, Any]:
        """Merge multiple conditions into one filter dict"""
        merged = {}
        
        for condition in conditions:
            for key, value in condition.items():
                if key in merged:
                    if isinstance(merged[key], dict) and isinstance(value, dict):
                        merged[key].update(value)
                    elif isinstance(merged[key], list) and isinstance(value, list):
                        merged[key].extend(value)
                    else:
                        if isinstance(value, dict) or (isinstance(value, str) and len(value) > len(str(merged[key]))):
                            merged[key] = value
                else:
                    merged[key] = value
        
        return merged

# =========== UPGRADE 6: FUZZY MATCHING (DATACLASS COMPATIBLE) ===========
class FuzzyMatcher:
    """
    UPGRADE 6: Handle misspellings and variations in tour names
    """
    
    SIMILARITY_THRESHOLD = 0.75
    
    @staticmethod
    def normalize_vietnamese(text: str) -> str:
        """
        Normalize Vietnamese text for fuzzy matching
        """
        if not text:
            return ""
        
        text = text.lower()
        text = unicodedata.normalize('NFD', text)
        text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')
        
        replacements = {
            'Ä‘': 'd',
            'khÃ´ng': 'ko',
            'khong': 'ko',
            'rá»“i': 'roi',
            'vá»›i': 'voi',
            'Ä‘Æ°á»£c': 'duoc',
            'má»™t': 'mot',
            'hai': '2',
            'ba': '3',
            'bá»‘n': '4',
            'nÄƒm': '5',
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    @staticmethod
    def find_similar_tours(query: str, tour_names: Dict[str, int]) -> List[Tuple[int, float]]:
        """
        Find tours with names similar to query
        """
        if not query or not tour_names:
            return []
        
        query_norm = FuzzyMatcher.normalize_vietnamese(query)
        if not query_norm:
            return []
        
        matches = []
        
        for tour_name, tour_idx in tour_names.items():
            tour_norm = FuzzyMatcher.normalize_vietnamese(tour_name)
            if not tour_norm:
                continue
            
            similarity = SequenceMatcher(None, query_norm, tour_norm).ratio()
            
            if query_norm in tour_norm or tour_norm in query_norm:
                similarity = min(similarity + 0.2, 1.0)
            
            query_words = set(query_norm.split())
            tour_words = set(tour_norm.split())
            common_words = query_words.intersection(tour_words)
            
            if common_words:
                word_boost = len(common_words) * 0.1
                similarity = min(similarity + word_boost, 1.0)
            
            if similarity >= FuzzyMatcher.SIMILARITY_THRESHOLD:
                matches.append((tour_idx, similarity))
        
        matches.sort(key=lambda x: x[1], reverse=True)
        
        logger.info(f"ðŸ” Fuzzy matching: '{query}' â†’ {len(matches)} matches")
        return matches
    
    @staticmethod
    def find_tour_by_partial_name(partial_name: str, tours_db: Dict[int, Tour]) -> List[int]:
        """
        Find tours by partial name match
        """
        if not partial_name or not tours_db:
            return []
        
        partial_norm = FuzzyMatcher.normalize_vietnamese(partial_name)
        matches = []
        
        for tour_idx, tour in tours_db.items():
            tour_name = tour.name or ""
            if not tour_name:
                continue
            
            tour_norm = FuzzyMatcher.normalize_vietnamese(tour_name)
            
            if partial_norm in tour_norm:
                match_ratio = len(partial_norm) / len(tour_norm) if tour_norm else 0
                matches.append((tour_idx, match_ratio))
        
        matches.sort(key=lambda x: x[1], reverse=True)
        
        return [idx for idx, _ in matches[:3]]

# =========== UPGRADE 7: STATE MACHINE (DATACLASS COMPATIBLE) ===========
class ConversationStateMachine:
    """
    UPGRADE 7: Track conversation state for better context
    """
    
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.state = ConversationState.INITIAL
        self.context = ConversationContext(session_id=session_id)
        self.transitions = []
        self.created_at = datetime.utcnow()
        self.last_updated = datetime.utcnow()
    
    def update(self, user_message: str, bot_response: str, tour_indices: List[int] = None):
        """Update state based on new interaction"""
        self.last_updated = datetime.utcnow()
        self.context.update(user_message, bot_response, tour_indices)
        
        new_state = self._determine_state(user_message, bot_response)
        
        self.transitions.append({
            'timestamp': datetime.utcnow().isoformat(),
            'from': self.state.value,
            'to': new_state.value,
            'message': user_message[:100]
        })
        
        self.state = new_state
        
        logger.info(f"ðŸ”„ State update: {self.state.value} for session {self.session_id}")
    
    def _determine_state(self, user_message: str, bot_response: str) -> ConversationState:
        """Determine new state based on current interaction"""
        message_lower = user_message.lower()
        
        farewell_words = ['táº¡m biá»‡t', 'cáº£m Æ¡n', 'thanks', 'bye', 'goodbye']
        if any(word in message_lower for word in farewell_words):
            return ConversationState.FAREWELL
        
        tour_ref_patterns = [
            r'tour nÃ y', r'tour Ä‘Ã³', r'tour Ä‘ang nÃ³i', r'cÃ¡i tour',
            r'nÃ³', r'cÃ¡i Ä‘Ã³', r'cÃ¡i nÃ y', r'Ä‘áº¥y'
        ]
        
        if any(re.search(pattern, message_lower) for pattern in tour_ref_patterns):
            if self.context.current_tours:
                return ConversationState.TOUR_SELECTED
            elif self.context.last_successful_tours:
                self.context.current_tours = self.context.last_successful_tours
                return ConversationState.TOUR_SELECTED
        
        if 'so sÃ¡nh' in message_lower or 'sÃ¡nh' in message_lower:
            return ConversationState.COMPARING
        
        if any(word in message_lower for word in ['phÃ¹ há»£p', 'gá»£i Ã½', 'Ä‘á» xuáº¥t', 'tÆ° váº¥n', 'nÃªn chá»n']):
            return ConversationState.RECOMMENDATION
        
        if any(word in message_lower for word in ['Ä‘áº·t', 'booking', 'Ä‘Äƒng kÃ½', 'giá»¯ chá»—']):
            return ConversationState.BOOKING
        
        if self.context.current_tours:
            return ConversationState.ASKING_DETAILS
        
        return ConversationState.INITIAL
    
    def get_context_hint(self) -> str:
        """Get hint about current context for LLM prompt"""
        hints = []
        
        if self.state == ConversationState.TOUR_SELECTED and self.context.current_tours:
            tour_indices = self.context.current_tours
            if len(tour_indices) == 1:
                hints.append(f"User is asking about tour index {tour_indices[0]}")
            else:
                hints.append(f"User is asking about tours {tour_indices}")
        
        if self.context.user_preferences:
            prefs = []
            for key, value in self.context.user_preferences.items():
                prefs.append(f"{key}: {value}")
            if prefs:
                hints.append(f"User preferences: {', '.join(prefs)}")
        
        return "; ".join(hints) if hints else "No specific context"
    
    def extract_reference(self, message: str) -> List[int]:
        """Extract tour reference from message using conversation context"""
        message_lower = message.lower()
        
        if self.context.current_tours:
            for tour_idx in self.context.current_tours:
                tour = TOURS_DB.get(tour_idx)
                if not tour:
                    continue
                tour_name = (tour.name or "").lower()
                if tour_name:
                    tour_words = set(tour_name.split())
                    msg_words = set(message_lower.split())
                    common = tour_words.intersection(msg_words)
                    if common and len(common) >= 1:
                        logger.info(f"ðŸ”„ State machine: Using current tour {tour_idx}")
                        return self.context.current_tours
        
        ref_patterns = [
            (r'tour nÃ y', 1.0),
            (r'tour Ä‘Ã³', 0.9),
            (r'tour Ä‘ang nÃ³i', 0.9),
            (r'cÃ¡i tour', 0.8),
            (r'nÃ³', 0.7),
            (r'Ä‘áº¥y', 0.7),
            (r'cÃ¡i Ä‘Ã³', 0.7),
        ]
        
        for pattern, confidence in ref_patterns:
            if re.search(pattern, message_lower):
                if self.context.current_tours:
                    logger.info(f"ðŸ”„ State machine: Resolved reference to {self.context.current_tours}")
                    return self.context.current_tours
                elif self.context.last_successful_tours:
                    logger.info(f"ðŸ”„ State machine: Using last successful tours {self.context.last_successful_tours}")
                    return self.context.last_successful_tours
        
        if self.context.mentioned_tours:
            recent_tours = list(self.context.mentioned_tours)
            for tour_idx in recent_tours[-3:]:
                tour = TOURS_DB.get(tour_idx)
                if not tour:
                    continue
                tour_name = (tour.name or "").lower()
                if tour_name:
                    tour_words = set(tour_name.split())
                    msg_words = set(message_lower.split())
                    common = tour_words.intersection(msg_words)
                    if common and len(common) >= 1:
                        logger.info(f"ðŸ”„ State machine: Matched to recently mentioned tour {tour_idx}")
                        return [tour_idx]
        
        return []

# =========== UPGRADE 8: DEEP SEMANTIC ANALYSIS (DATACLASS COMPATIBLE) ===========
class SemanticAnalyzer:
    """
    UPGRADE 8: Deep understanding of user intent beyond keywords
    """
    
    USER_PROFILE_PATTERNS = {
        'age_group': [
            (r'ngÆ°á»i giÃ |ngÆ°á»i lá»›n tuá»•i|cao tuá»•i', 'senior'),
            (r'thanh niÃªn|tráº»|sinh viÃªn|há»c sinh', 'young'),
            (r'trung niÃªn|trung tuá»•i', 'middle_aged'),
            (r'gia Ä‘Ã¬nh.*tráº» em|tráº» nhá»|con nÃ­t', 'family_with_kids'),
        ],
        
        'group_type': [
            (r'má»™t mÃ¬nh|Ä‘i láº»|solo', 'solo'),
            (r'cáº·p Ä‘Ã´i|Ä‘Ã´i lá»©a|ngÆ°á»i yÃªu', 'couple'),
            (r'gia Ä‘Ã¬nh|bá»‘ máº¹ con', 'family'),
            (r'báº¡n bÃ¨|nhÃ³m báº¡n|há»™i báº¡n', 'friends'),
            (r'cÃ´ng ty|doanh nghiá»‡p|Ä‘á»“ng nghiá»‡p', 'corporate'),
        ],
        
        'interest_type': [
            (r'thiÃªn nhiÃªn|rá»«ng|cÃ¢y|cáº£nh quan', 'nature'),
            (r'lá»‹ch sá»­|di tÃ­ch|chiáº¿n tranh|tri Ã¢n', 'history'),
            (r'vÄƒn hÃ³a|cá»™ng Ä‘á»“ng|dÃ¢n tá»™c|truyá»n thá»‘ng', 'culture'),
            (r'thiá»n|tÃ¢m linh|tÄ©nh tÃ¢m|yoga', 'spiritual'),
            (r'khÃ­ cÃ´ng|sá»©c khá»e|chá»¯a lÃ nh|wellness', 'wellness'),
            (r'áº©m thá»±c|Ä‘á»“ Äƒn|mÃ³n ngon|Ä‘áº·c sáº£n', 'food'),
            (r'phiÃªu lÆ°u|máº¡o hiá»ƒm|khÃ¡m phÃ¡|tráº£i nghiá»‡m', 'adventure'),
        ],
        
        'budget_level': [
            (r'kinh táº¿|tiáº¿t kiá»‡m|ráº»|giÃ¡ tháº¥p', 'budget'),
            (r'trung bÃ¬nh|vá»«a pháº£i|pháº£i chÄƒng', 'midrange'),
            (r'cao cáº¥p|sang trá»ng|premium|Ä‘áº¯t', 'premium'),
        ],
        
        'physical_level': [
            (r'nháº¹ nhÃ ng|dá»… dÃ ng|khÃ´ng má»‡t', 'easy'),
            (r'vá»«a pháº£i|trung bÃ¬nh|bÃ¬nh thÆ°á»ng', 'moderate'),
            (r'thá»­ thÃ¡ch|khÃ³|má»‡t|leo nÃºi', 'challenging'),
        ],
    }
    
    @staticmethod
    def analyze_user_profile(message: str, current_context: ConversationContext = None) -> UserProfile:
        """
        Analyze message to build user profile
        """
        if current_context and hasattr(current_context, 'user_profile') and current_context.user_profile:
            profile = current_context.user_profile
        else:
            profile = UserProfile()
        
        message_lower = message.lower()
        
        for category, patterns in SemanticAnalyzer.USER_PROFILE_PATTERNS.items():
            for pattern, value in patterns:
                if re.search(pattern, message_lower):
                    if category == 'interests':
                        if value not in profile.interests:
                            profile.interests.append(value)
                            profile.confidence_scores[f'interest_{value}'] = 0.8
                    else:
                        setattr(profile, category, value)
                        profile.confidence_scores[category] = 0.8
        
        SemanticAnalyzer._infer_attributes(profile, message_lower)
        profile.overall_confidence = SemanticAnalyzer._calculate_confidence(profile)
        
        logger.info(f"ðŸ‘¤ User profile analysis: {profile}")
        return profile
    
    @staticmethod
    def _infer_attributes(profile: UserProfile, message_lower: str):
        """Infer additional attributes from context"""
        if not profile.age_group:
            if profile.group_type and 'family_with_kids' in profile.group_type:
                profile.age_group = 'middle_aged'
                profile.confidence_scores['age_group'] = 0.6
            elif 'senior' in message_lower or 'giÃ ' in message_lower:
                profile.age_group = 'senior'
                profile.confidence_scores['age_group'] = 0.7
        
        if not profile.physical_level:
            if 'adventure' in profile.interests:
                profile.physical_level = 'challenging'
                profile.confidence_scores['physical_level'] = 0.6
            elif 'spiritual' in profile.interests or 'wellness' in profile.interests:
                profile.physical_level = 'easy'
                profile.confidence_scores['physical_level'] = 0.6
        
        if not profile.budget_level:
            budget_keywords = {
                'budget': ['ráº»', 'tiáº¿t kiá»‡m', 'Ã­t tiá»n', 'kinh táº¿'],
                'premium': ['cao cáº¥p', 'sang', 'Ä‘áº¯t', 'premium']
            }
            
            for level, keywords in budget_keywords.items():
                if any(keyword in message_lower for keyword in keywords):
                    profile.budget_level = level
                    profile.confidence_scores['budget_level'] = 0.7
                    break
    
    @staticmethod
    def _calculate_confidence(profile: UserProfile) -> float:
        """Calculate overall confidence in user profile"""
        if not profile.confidence_scores:
            return 0.0
        
        total = 0.0
        count = 0
        
        for key, score in profile.confidence_scores.items():
            total += score
            count += 1
        
        return total / max(count, 1)
    
    @staticmethod
    def match_tours_to_profile(profile: UserProfile, tours_db: Dict[int, Tour], 
                              max_results: int = 5) -> List[Tuple[int, float, List[str]]]:
        """
        Match tours to user profile with explanation
        """
        matches = []
        
        for tour_idx, tour in tours_db.items():
            score = 0.0
            reasons = []
            
            tour_tags = tour.tags or []
            
            if profile.age_group:
                if profile.age_group == 'senior':
                    if any('easy' in tag for tag in tour_tags):
                        score += 0.3
                        reasons.append("phÃ¹ há»£p ngÆ°á»i lá»›n tuá»•i")
                    if any('nature' in tag for tag in tour_tags):
                        score += 0.2
                        reasons.append("thiÃªn nhiÃªn nháº¹ nhÃ ng")
            
            if profile.interests:
                for interest in profile.interests:
                    tour_summary = (tour.summary or "").lower()
                    if (interest in tour_summary or 
                        any(interest in tag for tag in tour_tags)):
                        score += 0.4
                        reasons.append(f"cÃ³ yáº¿u tá»‘ {interest}")
            
            if profile.budget_level:
                tour_price = tour.price or ""
                price_nums = re.findall(r'\d[\d,\.]+', tour_price)
                
                if price_nums:
                    try:
                        first_price = int(price_nums[0].replace(',', '').replace('.', ''))
                        
                        if profile.budget_level == 'budget' and first_price < 2000000:
                            score += 0.3
                            reasons.append("giÃ¡ há»£p lÃ½")
                        elif profile.budget_level == 'premium' and first_price > 2500000:
                            score += 0.3
                            reasons.append("cao cáº¥p")
                        elif profile.budget_level == 'midrange' and 1500000 <= first_price <= 3000000:
                            score += 0.3
                            reasons.append("giÃ¡ vá»«a pháº£i")
                    except:
                        pass
            
            if profile.physical_level:
                if profile.physical_level == 'easy':
                    if any('easy' in tag or 'meditation' in tag for tag in tour_tags):
                        score += 0.2
                        reasons.append("hoáº¡t Ä‘á»™ng nháº¹ nhÃ ng")
            
            if score > 0:
                matches.append((tour_idx, score, reasons))
        
        matches.sort(key=lambda x: x[1], reverse=True)
        
        return matches[:max_results]

# =========== UPGRADE 9: AUTO-VALIDATION SYSTEM (DATACLASS COMPATIBLE) ===========
class AutoValidator:
    """
    UPGRADE 9: Validate and correct information before returning
    """
    
    VALIDATION_RULES = {
        'duration': {
            'patterns': [
                r'(\d+)\s*ngÃ y\s*(\d+)\s*Ä‘Ãªm',
                r'(\d+)\s*ngÃ y',
                r'(\d+)\s*Ä‘Ãªm',
            ],
            'constraints': {
                'max_days': 7,
                'max_nights': 7,
                'valid_day_night_combos': [(1,0), (1,1), (2,1), (2,2), (3,2), (3,3)],
                'common_durations': ['1 ngÃ y', '2 ngÃ y 1 Ä‘Ãªm', '3 ngÃ y 2 Ä‘Ãªm']
            }
        },
        
        'price': {
            'patterns': [
                r'(\d[\d,\.]*)\s*(triá»‡u|tr|k|nghÃ¬n)',
                r'(\d[\d,\.]*)\s*-\s*(\d[\d,\.]*)\s*(triá»‡u|tr|k|nghÃ¬n)?',
                r'(\d[\d,\.]*)\s*(Ä‘á»“ng|vnÄ‘|vnd)',
            ],
            'constraints': {
                'min_tour_price': 500000,
                'max_tour_price': 10000000,
                'common_ranges': [
                    (800000, 1500000),
                    (1500000, 2500000),
                    (2500000, 4000000),
                ]
            }
        },
        
        'location': {
            'patterns': [
                r'á»Ÿ\s+([^.,!?]+)',
                r'táº¡i\s+([^.,!?]+)',
                r'Ä‘áº¿n\s+([^.,!?]+)',
            ],
            'constraints': {
                'valid_locations': ['Huáº¿', 'Quáº£ng Trá»‹', 'Báº¡ch MÃ£', 'TrÆ°á»ng SÆ¡n', 'ÄÃ´ng HÃ ', 'Khe Sanh'],
                'max_length': 100
            }
        },
    }
    
    @staticmethod
    def validate_response(response: str) -> str:
        """
        Validate and correct response content
        """
        if not response:
            return response
        
        validated = response
        
        validated = AutoValidator._validate_duration(validated)
        validated = AutoValidator._validate_price(validated)
        validated = AutoValidator._validate_locations(validated)
        validated = AutoValidator._check_unrealistic_info(validated)
        
        if validated != response:
            validated = AutoValidator._add_validation_note(validated)
        
        return validated
    
    @staticmethod
    def _validate_duration(text: str) -> str:
        """Validate and correct duration information"""
        for pattern in AutoValidator.VALIDATION_RULES['duration']['patterns']:
            matches = list(re.finditer(pattern, text))
            for match in matches:
                try:
                    if match.lastindex == 2:
                        days = int(match.group(1))
                        nights = int(match.group(2))
                        
                        constraints = AutoValidator.VALIDATION_RULES['duration']['constraints']
                        
                        valid_combos = constraints['valid_day_night_combos']
                        is_valid_combo = any(d == d2 and n == n2 for d2, n2 in valid_combos)
                        
                        if days > constraints['max_days'] or nights > constraints['max_nights']:
                            replacement = random.choice(constraints['common_durations'])
                            text = text.replace(match.group(0), replacement)
                            logger.warning(f"âš ï¸ Corrected unrealistic duration: {days} ngÃ y {nights} Ä‘Ãªm â†’ {replacement}")
                        
                        elif not is_valid_combo:
                            valid_days = min(days, constraints['max_days'])
                            valid_nights = min(nights, constraints['max_nights'])
                            if abs(valid_days - valid_nights) > 1:
                                valid_nights = valid_days
                            
                            replacement = f"{valid_days} ngÃ y {valid_nights} Ä‘Ãªm"
                            text = text.replace(match.group(0), replacement)
                            logger.info(f"ðŸ”„ Fixed duration combo: {replacement}")
                    
                    elif match.lastindex == 1:
                        num = int(match.group(1))
                        constraints = AutoValidator.VALIDATION_RULES['duration']['constraints']
                        
                        if num > constraints['max_days']:
                            replacement = f"{constraints['max_days']} ngÃ y"
                            text = text.replace(match.group(0), replacement)
                            logger.warning(f"âš ï¸ Capped long duration: {num} â†’ {constraints['max_days']}")
                
                except (ValueError, IndexError):
                    continue
        
        return text
    
    @staticmethod
    def _validate_price(text: str) -> str:
        """Validate and correct price information"""
        for pattern in AutoValidator.VALIDATION_RULES['price']['patterns']:
            matches = list(re.finditer(pattern, text, re.IGNORECASE))
            for match in matches:
                try:
                    amount_str = match.group(1).replace(',', '').replace('.', '')
                    if not amount_str.isdigit():
                        continue
                    
                    amount = int(amount_str)
                    
                    unit = match.group(2).lower() if match.lastindex >= 2 else ''
                    
                    if unit in ['triá»‡u', 'tr']:
                        amount = amount * 1000000
                    elif unit in ['k', 'nghÃ¬n']:
                        amount = amount * 1000
                    
                    constraints = AutoValidator.VALIDATION_RULES['price']['constraints']
                    
                    if amount < constraints['min_tour_price']:
                        replacement = "giÃ¡ há»£p lÃ½"
                        text = text.replace(match.group(0), replacement)
                        logger.warning(f"âš ï¸ Corrected too-low price: {amount} â†’ {replacement}")
                    
                    elif amount > constraints['max_tour_price']:
                        replacement = "giÃ¡ cao cáº¥p"
                        text = text.replace(match.group(0), replacement)
                        logger.warning(f"âš ï¸ Corrected too-high price: {amount} â†’ {replacement}")
                
                except (ValueError, IndexError, AttributeError):
                    continue
        
        return text
    
    @staticmethod
    def _validate_locations(text: str) -> str:
        """Validate location names"""
        wrong_locations = {
            'hÃ  ná»™i': 'Huáº¿',
            'há»“ chÃ­ minh': 'Quáº£ng Trá»‹',
            'Ä‘Ã  náºµng': 'Báº¡ch MÃ£',
            'nha trang': 'TrÆ°á»ng SÆ¡n',
        }
        
        for wrong, correct in wrong_locations.items():
            if wrong in text.lower():
                text = text.replace(wrong, correct)
                text = text.replace(wrong.capitalize(), correct)
                logger.info(f"ðŸ”„ Corrected location: {wrong} â†’ {correct}")
        
        return text
    
    @staticmethod
    def _check_unrealistic_info(text: str) -> str:
        """Check for other unrealistic information"""
        unrealistic_patterns = [
            (r'\d+\s*giá»\s*bay', "thá»i gian di chuyá»ƒn"),
            (r'\d+\s*sao', "cháº¥t lÆ°á»£ng dá»‹ch vá»¥"),
            (r'\d+\s*táº§ng', "chá»— á»Ÿ"),
            (r'\d+\s*m\s*cao', "Ä‘á»‹a hÃ¬nh"),
        ]
        
        for pattern, replacement in unrealistic_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
                logger.info(f"ðŸ”„ Replaced unrealistic info with: {replacement}")
        
        return text
    
    @staticmethod
    def _add_validation_note(text: str) -> str:
        """Add note about information validation"""
        note = "\n\n*ThÃ´ng tin Ä‘Æ°á»£c cung cáº¥p dá»±a trÃªn dá»¯ liá»‡u hiá»‡n cÃ³. " \
               "Vui lÃ²ng liÃªn há»‡ hotline 0332510486 Ä‘á»ƒ xÃ¡c nháº­n chi tiáº¿t chÃ­nh xÃ¡c nháº¥t.*"
        
        if note not in text:
            text += note
        
        return text

# =========== UPGRADE 10: TEMPLATE SYSTEM (DATACLASS COMPATIBLE) ===========
class TemplateSystem:
    """
    UPGRADE 10: Beautiful, structured responses for different question types
    """
    
    TEMPLATES = {
        'tour_list': {
            'header': "âœ¨ **DANH SÃCH TOUR RUBY WINGS** âœ¨\n\n",
            'item': "**{index}. {tour_name}** {emoji}\n"
                   "   ðŸ“… {duration}\n"
                   "   ðŸ“ {location}\n"
                   "   ðŸ’° {price}\n"
                   "   {summary}\n",
            'footer': "\nðŸ“ž **LiÃªn há»‡ Ä‘áº·t tour:** 0332510486\n"
                     "ðŸ“ **Ruby Wings Travel** - HÃ nh trÃ¬nh tráº£i nghiá»‡m Ä‘áº·c sáº¯c\n"
                     "ðŸ’¡ *Há»i chi tiáº¿t vá» báº¥t ká»³ tour nÃ o báº±ng cÃ¡ch nháº­p tÃªn tour*",
            'emoji_map': {
                '1 ngÃ y': 'ðŸŒ…',
                '2 ngÃ y': 'ðŸŒ„',
                '3 ngÃ y': 'ðŸ”ï¸',
                'default': 'âœ¨'
            }
        },
        
        'tour_detail': {
            'header': "ðŸŽ¯ **{tour_name}**\n\n",
            'sections': {
                'overview': "ðŸ“‹ **THÃ”NG TIN CHÃNH:**\n"
                          "   â±ï¸ Thá»i gian: {duration}\n"
                          "   ðŸ“ Äá»‹a Ä‘iá»ƒm: {location}\n"
                          "   ðŸ’° GiÃ¡ tour: {price}\n\n",
                'description': "ðŸ“– **MÃ” Táº¢ TOUR:**\n{summary}\n\n",
                'includes': "ðŸŽª **Lá»ŠCH TRÃŒNH & Dá»ŠCH Vá»¤:**\n{includes}\n\n",
                'accommodation': "ðŸ¨ **CHá»– á»ž:**\n{accommodation}\n\n",
                'meals': "ðŸ½ï¸ **Ä‚N Uá»NG:**\n{meals}\n\n",
                'transport': "ðŸš— **DI CHUYá»‚N:**\n{transport}\n\n",
                'notes': "ðŸ“ **GHI CHÃš:**\n{notes}\n\n",
            },
            'footer': "ðŸ“ž **Äáº¶T TOUR & TÆ¯ Váº¾N:** 0332510486\n"
                     "â­ *Tour phÃ¹ há»£p cho: {suitable_for}*",
            'default_values': {
                'duration': 'Äang cáº­p nháº­t',
                'location': 'Äang cáº­p nháº­t',
                'price': 'LiÃªn há»‡ Ä‘á»ƒ biáº¿t giÃ¡',
                'summary': 'HÃ nh trÃ¬nh tráº£i nghiá»‡m Ä‘áº·c sáº¯c cá»§a Ruby Wings',
                'includes': 'Chi tiáº¿t lá»‹ch trÃ¬nh liÃªn há»‡ tÆ° váº¥n',
                'accommodation': 'Äang cáº­p nháº­t',
                'meals': 'Äang cáº­p nháº­t',
                'transport': 'Äang cáº­p nháº­t',
                'notes': 'Vui lÃ²ng liÃªn há»‡ Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t',
                'suitable_for': 'má»i Ä‘á»‘i tÆ°á»£ng',
            }
        },
        
        'comparison': {
            'header': "ðŸ“Š **SO SÃNH TOUR**\n\n",
            'table_header': "| TiÃªu chÃ­ | {tour1} | {tour2} |\n|----------|----------|----------|\n",
            'table_row': "| {criterion} | {value1} | {value2} |\n",
            'recommendation': "\nðŸ’¡ **Gá»¢I Ã Lá»°A CHá»ŒN:**\n{recommendations}\n",
            'footer': "\nðŸ“ž **TÆ° váº¥n chi tiáº¿t:** 0332510486\n"
                     "ðŸ¤” *Cáº§n so sÃ¡nh thÃªm tiÃªu chÃ­ nÃ o?*",
        },
        
        'recommendation': {
            'header': "ðŸŽ¯ **Äá»€ XUáº¤T TOUR PHÃ™ Há»¢P**\n\n",
            'top_recommendation': "ðŸ† **PHÃ™ Há»¢P NHáº¤T ({score}%)**\n"
                                "**{tour_name}**\n"
                                "   âœ… {reasons}\n"
                                "   ðŸ“… {duration} | ðŸ“ {location} | ðŸ’° {price}\n\n",
            'other_recommendations': "ðŸ“‹ **Lá»°A CHá»ŒN KHÃC:**\n",
            'other_item': "   â€¢ **{tour_name}** ({score}%)\n"
                         "     ðŸ“… {duration} | ðŸ“ {location}\n",
            'criteria': "\nðŸ” **TIÃŠU CHÃ Äá»€ XUáº¤T:**\n{criteria}\n",
            'footer': "\nðŸ“ž **LiÃªn há»‡ tÆ° váº¥n cÃ¡ nhÃ¢n hÃ³a:** 0332510486\n"
                     "ðŸ’¬ *Cho tÃ´i biáº¿t thÃªm sá»Ÿ thÃ­ch cá»§a báº¡n Ä‘á»ƒ Ä‘á» xuáº¥t chÃ­nh xÃ¡c hÆ¡n*",
        },
        
        'information': {
            'header': "â„¹ï¸ **THÃ”NG TIN:**\n\n",
            'content': "{content}\n",
            'sources': "\nðŸ“š *Nguá»“n thÃ´ng tin tá»« dá»¯ liá»‡u Ruby Wings*",
            'footer': "\nðŸ“ž **Hotline há»— trá»£:** 0332510486",
        },
        
        'greeting': {
            'template': "ðŸ‘‹ **Xin chÃ o! TÃ´i lÃ  trá»£ lÃ½ AI cá»§a Ruby Wings**\n\n"
                       "TÃ´i cÃ³ thá»ƒ giÃºp báº¡n:\n"
                       "â€¢ TÃ¬m hiá»ƒu vá» cÃ¡c tour tráº£i nghiá»‡m\n"
                       "â€¢ So sÃ¡nh cÃ¡c hÃ nh trÃ¬nh\n"
                       "â€¢ Äá» xuáº¥t tour phÃ¹ há»£p vá»›i báº¡n\n"
                       "â€¢ Cung cáº¥p thÃ´ng tin chi tiáº¿t vá» tour\n\n"
                       "ðŸ’¡ **VÃ­ dá»¥ báº¡n cÃ³ thá»ƒ há»i:**\n"
                       "- 'CÃ³ nhá»¯ng tour nÃ o?'\n"
                       "- 'Tour Báº¡ch MÃ£ giÃ¡ bao nhiÃªu?'\n"
                       "- 'Tour nÃ o phÃ¹ há»£p cho gia Ä‘Ã¬nh?'\n\n"
                       "HÃ£y cho tÃ´i biáº¿t báº¡n cáº§n gÃ¬ nhÃ©! ðŸ˜Š",
        },
        
        'farewell': {
            'template': "ðŸ™ **Cáº£m Æ¡n báº¡n Ä‘Ã£ trÃ² chuyá»‡n cÃ¹ng Ruby Wings!**\n\n"
                       "ChÃºc báº¡n má»™t ngÃ y trÃ n Ä‘áº§y nÄƒng lÆ°á»£ng vÃ  bÃ¬nh an.\n"
                       "Hy vá»ng sá»›m Ä‘Æ°á»£c Ä‘á»“ng hÃ nh cÃ¹ng báº¡n trong hÃ nh trÃ¬nh tráº£i nghiá»‡m sáº¯p tá»›i!\n\n"
                       "ðŸ“ž **LiÃªn há»‡ Ä‘áº·t tour:** 0332510486\n"
                       "ðŸŒ **Website:** rubywings.vn\n\n"
                       "Háº¹n gáº·p láº¡i! âœ¨",
        },
    }
    
    @staticmethod
    def render(template_name: str, **kwargs) -> str:
        """Render template with provided variables"""
        template_data = TemplateSystem.TEMPLATES.get(template_name)
        if not template_data:
            return kwargs.get('content', '')
        
        if template_name in ['greeting', 'farewell']:
            return template_data['template']
        
        response_parts = []
        
        if 'header' in template_data:
            header = template_data['header']
            for key, value in kwargs.items():
                header = header.replace(f'{{{key}}}', str(value))
            response_parts.append(header)
        
        if template_name == 'tour_list':
            response_parts.append(TemplateSystem._render_tour_list(template_data, kwargs))
        
        elif template_name == 'tour_detail':
            response_parts.append(TemplateSystem._render_tour_detail(template_data, kwargs))
        
        elif template_name == 'comparison':
            response_parts.append(TemplateSystem._render_comparison(template_data, kwargs))
        
        elif template_name == 'recommendation':
            response_parts.append(TemplateSystem._render_recommendation(template_data, kwargs))
        
        elif template_name == 'information':
            response_parts.append(TemplateSystem._render_information(template_data, kwargs))
        
        if 'footer' in template_data:
            footer = template_data['footer']
            for key, value in kwargs.items():
                footer = footer.replace(f'{{{key}}}', str(value))
            response_parts.append(footer)
        
        return '\n'.join(response_parts)
    
    @staticmethod
    def _render_tour_list(template_data: Dict, kwargs: Dict) -> str:
        """Render tour list template"""
        tours = kwargs.get('tours', [])
        if not tours:
            return "Hiá»‡n chÆ°a cÃ³ thÃ´ng tin tour."
        
        items = []
        for i, tour in enumerate(tours[:10], 1):
            duration = tour.duration or ''
            emoji = template_data['emoji_map'].get('default')
            for dur_pattern, dur_emoji in template_data['emoji_map'].items():
                if dur_pattern in duration.lower():
                    emoji = dur_emoji
                    break
            
            item_template = template_data['item']
            item = item_template.format(
                index=i,
                tour_name=tour.name or f'Tour #{i}',
                emoji=emoji or 'âœ¨',
                duration=duration or 'Äang cáº­p nháº­t',
                location=tour.location or 'Äang cáº­p nháº­t',
                price=tour.price or 'LiÃªn há»‡ Ä‘á»ƒ biáº¿t giÃ¡',
                summary=(tour.summary or 'Tour tráº£i nghiá»‡m Ä‘áº·c sáº¯c')[:100] + '...'
            )
            items.append(item)
        
        return '\n'.join(items)
    
    @staticmethod
    def _render_tour_detail(template_data: Dict, kwargs: Dict) -> str:
        """Render tour detail template"""
        sections = []
        
        for section_name, section_template in template_data['sections'].items():
            value = kwargs.get(section_name, template_data['default_values'].get(section_name, ''))
            
            if value and value != template_data['default_values'].get(section_name):
                if isinstance(value, list):
                    if section_name == 'includes':
                        value = '\n'.join([f'   â€¢ {item}' for item in value[:5]])
                    else:
                        value = ', '.join(value[:3])
                
                section = section_template.format(**{section_name: value})
                sections.append(section)
        
        return '\n'.join(sections)
    
    @staticmethod
    def _render_comparison(template_data: Dict, kwargs: Dict) -> str:
        """Render comparison template"""
        comparison_table = []
        
        tour1_name = kwargs.get('tour1_name', 'Tour 1')[:20]
        tour2_name = kwargs.get('tour2_name', 'Tour 2')[:20]
        table_header = template_data['table_header'].format(tour1=tour1_name, tour2=tour2_name)
        comparison_table.append(table_header)
        
        criteria = kwargs.get('criteria', [])
        for criterion in criteria[:8]:
            row = template_data['table_row'].format(
                criterion=criterion.get('name', ''),
                value1=criterion.get('value1', 'N/A')[:20],
                value2=criterion.get('value2', 'N/A')[:20]
            )
            comparison_table.append(row)
        
        return '\n'.join(comparison_table)
    
    @staticmethod
    def _render_recommendation(template_data: Dict, kwargs: Dict) -> str:
        """Render recommendation template"""
        recommendation_text = []
        
        top_tour = kwargs.get('top_tour')
        if top_tour:
            top_text = template_data['top_recommendation'].format(
                score=int(top_tour.get('score', 0) * 100),
                tour_name=top_tour.get('name', ''),
                reasons=', '.join(top_tour.get('reasons', ['phÃ¹ há»£p'])[:3]),
                duration=top_tour.get('duration', ''),
                location=top_tour.get('location', ''),
                price=top_tour.get('price', 'LiÃªn há»‡ Ä‘á»ƒ biáº¿t giÃ¡')
            )
            recommendation_text.append(top_text)
        
        other_tours = kwargs.get('other_tours', [])
        if other_tours:
            recommendation_text.append(template_data['other_recommendations'])
            
            for tour in other_tours[:2]:
                other_item = template_data['other_item'].format(
                    tour_name=tour.get('name', ''),
                    score=int(tour.get('score', 0) * 100),
                    duration=tour.get('duration', ''),
                    location=tour.get('location', '')
                )
                recommendation_text.append(other_item)
        
        return '\n'.join(recommendation_text)
    
    @staticmethod
    def _render_information(template_data: Dict, kwargs: Dict) -> str:
        """Render information template"""
        content = kwargs.get('content', '')
        if not content:
            return ""
        
        info_text = template_data['content'].format(content=content)
        
        if kwargs.get('has_sources'):
            info_text += template_data['sources']
        
        return info_text

# =========== TOUR DATABASE BUILDER (USING Tour DATACLASS) ===========
def load_knowledge(path: str = KNOWLEDGE_PATH):
    """Load knowledge base from JSON file"""
    global KNOW, FLAT_TEXTS, MAPPING
    
    try:
        with open(path, "r", encoding="utf-8") as f:
            KNOW = json.load(f)
        logger.info(f"âœ… Loaded knowledge from {path}")
    except Exception as e:
        logger.error(f"âŒ Could not open {path}: {e}")
        KNOW = {}
    
    FLAT_TEXTS = []
    MAPPING = []
    
    def scan(obj, prefix="root"):
        if isinstance(obj, dict):
            for k, v in obj.items():
                scan(v, f"{prefix}.{k}")
        elif isinstance(obj, list):
            for i, v in enumerate(obj):
                scan(v, f"{prefix}[{i}]")
        elif isinstance(obj, str):
            t = obj.strip()
            if t:
                FLAT_TEXTS.append(t)
                MAPPING.append({"path": prefix, "text": t})
        else:
            try:
                s = str(obj).strip()
                if s:
                    FLAT_TEXTS.append(s)
                    MAPPING.append({"path": prefix, "text": s})
            except Exception:
                pass
    
    scan(KNOW)
    logger.info(f"ðŸ“Š Knowledge scanned: {len(FLAT_TEXTS)} passages")

def index_tour_names():
    """Build tour name to index mapping"""
    global TOUR_NAME_TO_INDEX
    TOUR_NAME_TO_INDEX = {}
    
    for m in MAPPING:
        path = m.get("path", "")
        if path.endswith(".tour_name"):
            txt = m.get("text", "") or ""
            norm = normalize_text_simple(txt)
            if not norm:
                continue
            
            match = re.search(r"\[(\d+)\]", path)
            if match:
                idx = int(match.group(1))
                prev = TOUR_NAME_TO_INDEX.get(norm)
                if prev is None:
                    TOUR_NAME_TO_INDEX[norm] = idx
                else:
                    existing_txt = MAPPING[next(
                        i for i, m2 in enumerate(MAPPING) 
                        if re.search(rf"\[{prev}\]", m2.get('path','')) and ".tour_name" in m2.get('path','')
                    )].get("text","")
                    if len(txt) > len(existing_txt):
                        TOUR_NAME_TO_INDEX[norm] = idx
    
    logger.info(f"ðŸ“ Indexed {len(TOUR_NAME_TO_INDEX)} tour names")

def build_tours_db():
    """Build structured tour database from MAPPING using Tour dataclass"""
    global TOURS_DB, TOUR_TAGS
    
    TOURS_DB.clear()
    TOUR_TAGS.clear()
    
    # First pass: collect all fields for each tour
    for m in MAPPING:
        path = m.get("path", "")
        text = m.get("text", "")
        
        if not path or not text:
            continue
        
        tour_match = re.search(r'tours\[(\d+)\]', path)
        if not tour_match:
            continue
        
        tour_idx = int(tour_match.group(1))
        
        field_match = re.search(r'tours\[\d+\]\.(\w+)(?:\[\d+\])?', path)
        if not field_match:
            continue
        
        field_name = field_match.group(1)
        
        # Initialize tour entry
        if tour_idx not in TOURS_DB:
            TOURS_DB[tour_idx] = Tour(index=tour_idx)
        
        # Update field in Tour object
        tour_obj = TOURS_DB[tour_idx]
        if field_name == 'tour_name':
            tour_obj.name = text
        elif field_name == 'duration':
            tour_obj.duration = text
        elif field_name == 'location':
            tour_obj.location = text
        elif field_name == 'price':
            tour_obj.price = text
        elif field_name == 'summary':
            tour_obj.summary = text
        elif field_name == 'includes':
            if isinstance(tour_obj.includes, list):
                tour_obj.includes.append(text)
            else:
                tour_obj.includes = [text]
        elif field_name == 'accommodation':
            tour_obj.accommodation = text
        elif field_name == 'meals':
            tour_obj.meals = text
        elif field_name == 'transport':
            tour_obj.transport = text
        elif field_name == 'notes':
            tour_obj.notes = text
        elif field_name == 'style':
            tour_obj.style = text
    
    # Second pass: generate tags and metadata
    for tour_idx, tour_obj in TOURS_DB.items():
        tags = []
        
        # Location tags
        if tour_obj.location:
            locations = [loc.strip() for loc in tour_obj.location.split(",") if loc.strip()]
            tags.extend([f"location:{loc}" for loc in locations[:2]])
        
        # Duration tags
        if tour_obj.duration:
            duration_lower = tour_obj.duration.lower()
            if "1 ngÃ y" in duration_lower:
                tags.append("duration:1day")
            elif "2 ngÃ y" in duration_lower:
                tags.append("duration:2day")
            elif "3 ngÃ y" in duration_lower:
                tags.append("duration:3day")
            else:
                day_match = re.search(r'(\d+)\s*ngÃ y', duration_lower)
                if day_match:
                    days = int(day_match.group(1))
                    tags.append(f"duration:{days}day")
        
        # Price tags
        if tour_obj.price:
            price_nums = re.findall(r'[\d,\.]+', tour_obj.price)
            if price_nums:
                try:
                    clean_nums = []
                    for p in price_nums[:2]:
                        p_clean = p.replace(',', '').replace('.', '')
                        if p_clean.isdigit():
                            clean_nums.append(int(p_clean))
                    
                    if clean_nums:
                        avg_price = sum(clean_nums) / len(clean_nums)
                        if avg_price < 1000000:
                            tags.append("price:budget")
                        elif avg_price < 2000000:
                            tags.append("price:midrange")
                        else:
                            tags.append("price:premium")
                except:
                    pass
        
        # Style/theme tags
        text_to_check = (tour_obj.style + " " + (tour_obj.summary or '')).lower()
        
        theme_keywords = {
            'meditation': ['thiá»n', 'chÃ¡nh niá»‡m', 'tÃ¢m linh'],
            'history': ['lá»‹ch sá»­', 'di tÃ­ch', 'chiáº¿n tranh', 'tri Ã¢n'],
            'nature': ['thiÃªn nhiÃªn', 'rá»«ng', 'nÃºi', 'cÃ¢y'],
            'culture': ['vÄƒn hÃ³a', 'cá»™ng Ä‘á»“ng', 'dÃ¢n tá»™c'],
            'wellness': ['khÃ­ cÃ´ng', 'sá»©c khá»e', 'chá»¯a lÃ nh'],
            'adventure': ['phiÃªu lÆ°u', 'máº¡o hiá»ƒm', 'khÃ¡m phÃ¡'],
        }
        
        for theme, keywords in theme_keywords.items():
            if any(keyword in text_to_check for keyword in keywords):
                tags.append(f"theme:{theme}")
        
        # Destination tags from tour name
        if tour_obj.name:
            name_lower = tour_obj.name.lower()
            if "báº¡ch mÃ£" in name_lower:
                tags.append("destination:bachma")
            if "trÆ°á»ng sÆ¡n" in name_lower:
                tags.append("destination:truongson")
            if "quáº£ng trá»‹" in name_lower:
                tags.append("destination:quangtri")
            if "huáº¿" in name_lower:
                tags.append("destination:hue")
        
        # Update Tour object tags
        tour_obj.tags = list(set(tags))
        TOUR_TAGS[tour_idx] = tour_obj.tags
        
        # Calculate completeness score
        completeness = 0
        important_fields = ['name', 'duration', 'location', 'price', 'summary']
        for field in important_fields:
            if getattr(tour_obj, field, None):
                completeness += 1
        
        tour_obj.completeness_score = completeness / len(important_fields)
    
    logger.info(f"âœ… Built tours database: {len(TOURS_DB)} tours with tags")

def get_passages_by_field(field_name: str, limit: int = 50, 
                         tour_indices: Optional[List[int]] = None) -> List[Tuple[float, Dict]]:
    """
    Get passages for a specific field
    """
    exact_matches = []
    global_matches = []
    
    for m in MAPPING:
        path = m.get("path", "")
        if path.endswith(f".{field_name}") or f".{field_name}" in path:
            is_exact_match = False
            if tour_indices:
                for ti in tour_indices:
                    if f"[{ti}]" in path:
                        is_exact_match = True
                        break
            
            if is_exact_match:
                exact_matches.append((2.0, m))
            elif not tour_indices:
                global_matches.append((1.0, m))
    
    all_results = exact_matches + global_matches
    all_results.sort(key=lambda x: x[0], reverse=True)
    return all_results[:limit]

# =========== CACHE SYSTEM (DATACLASS COMPATIBLE) ===========
class CacheSystem:
    """Simple caching system for responses"""
    
    @staticmethod
    def get_cache_key(query: str, context_hash: str = "") -> str:
        """Generate cache key"""
        key_parts = [query]
        if context_hash:
            key_parts.append(context_hash)
        return hashlib.md5("|".join(key_parts).encode()).hexdigest()
    
    @staticmethod
    def get(key: str, ttl_seconds: int = 300):
        """Get item from cache"""
        with _cache_lock:
            if key in _response_cache:
                cache_entry = _response_cache[key]
                if not cache_entry.is_expired():
                    logger.info(f"ðŸ’¾ Cache hit for key: {key[:20]}...")
                    return cache_entry.value
                else:
                    del _response_cache[key]
            return None
    
    @staticmethod
    def set(key: str, value: Any):
        """Set item in cache"""
        with _cache_lock:
            cache_entry = CacheEntry(
                key=key,
                value=value,
                created_at=datetime.utcnow(),
                ttl_seconds=UpgradeFlags.get_all_flags().get("CACHE_TTL_SECONDS", 300)
            )
            _response_cache[key] = cache_entry
            
            if len(_response_cache) > 1000:
                sorted_items = sorted(_response_cache.items(), 
                                     key=lambda x: x[1].created_at)
                for old_key in [k for k, _ in sorted_items[:200]]:
                    if old_key in _response_cache:
                        del _response_cache[old_key]

# =========== EMBEDDING FUNCTIONS (MEMORY OPTIMIZED) ===========
@lru_cache(maxsize=128 if IS_LOW_RAM else 1000)
def embed_text(text: str) -> Tuple[List[float], int]:
    """Embed text using OpenAI or fallback (with memory optimization)"""
    if not text:
        return [], 0
    
    text = text[:2000]
    
    if client:
        try:
            response = client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=text
            )
            if response.data:
                embedding = response.data[0].embedding
                return embedding, len(embedding)
        except Exception as e:
            logger.error(f"OpenAI embedding error: {e}")
    
    # Fallback: deterministic hash-based embedding
    h = hash(text) % (10 ** 12)
    dim = 1536
    embedding = [(float((h >> (i % 32)) & 0xFF) + (i % 7)) / 255.0 
                 for i in range(dim)]
    
    return embedding, dim

def query_index(query: str, top_k: int = TOP_K) -> List[Tuple[float, Dict]]:
    """Query the index"""
    if not query or INDEX is None:
        return []
    
    emb, _ = embed_text(query)
    if not emb:
        return []
    
    vec = np.array(emb, dtype="float32").reshape(1, -1)
    vec = vec / (np.linalg.norm(vec) + 1e-12)
    
    try:
        if HAS_FAISS and isinstance(INDEX, faiss.Index):
            D, I = INDEX.search(vec, top_k)
        else:
            D, I = INDEX.search(vec, top_k)
        
        results = []
        for score, idx in zip(D[0], I[0]):
            if 0 <= idx < len(MAPPING):
                results.append((float(score), MAPPING[idx]))
        
        return results
    except Exception as e:
        logger.error(f"Index search error: {e}")
        return []

class NumpyIndex:
    """Simple numpy-based index with safe numpy handling"""

    def __init__(self, mat=None):
        if NUMPY_AVAILABLE:
            if mat is None:
                self.mat = np.empty((0, 0), dtype="float32")
            else:
                # Force numpy float32 2D
                self.mat = np.asarray(mat, dtype="float32")
                if self.mat.ndim == 1:
                    self.mat = self.mat.reshape(1, -1)
        else:
            self.mat = mat if mat is not None else []

        # SAFE dimension detection (no numpy truth check)
        if NUMPY_AVAILABLE:
            if self.mat.shape[0] > 0 and self.mat.ndim == 2:
                self.dim = int(self.mat.shape[1])
            else:
                self.dim = 0
            self.size = int(self.mat.shape[0])
        else:
            self.size = len(self.mat)
            self.dim = len(self.mat[0]) if self.size > 0 else 0

    def is_empty(self):
        if NUMPY_AVAILABLE:
            return self.mat.shape[0] == 0
        return len(self.mat) == 0

    def search(self, query_vec, k=5):
        if self.is_empty():
            return [], []

        if NUMPY_AVAILABLE:
            q = np.asarray(query_vec, dtype="float32").reshape(1, -1)
            sims = np.dot(self.mat, q.T).reshape(-1)
            topk = np.argsort(-sims)[:k]
            return sims[topk].tolist(), topk.tolist()
        else:
            return [], []

    
    def search(self, qvec, k):
        if not self.mat or (NUMPY_AVAILABLE and self.mat.shape[0] == 0) or (not NUMPY_AVAILABLE and len(self.mat) == 0):
            # Return empty results
            return np.array([[]]), np.array([[]], dtype=np.int64)
        
        q = np.array(qvec).flatten()
        
        if NUMPY_AVAILABLE:
            # Use numpy if available
            q = q / (np.linalg.norm(q) + 1e-12)
            m = self.mat / (np.linalg.norm(self.mat, axis=1, keepdims=True) + 1e-12)
            sims = np.dot(q, m.T)
            idx = np.argsort(-sims)[:k]
            scores = sims[idx]
        else:
            # Fallback calculation
            q_norm = q / (sum(x*x for x in q)**0.5 + 1e-12)
            scores = []
            for i, row in enumerate(self.mat):
                row_norm = row / (sum(x*x for x in row)**0.5 + 1e-12)
                sim = sum(q_norm[j] * row_norm[j] for j in range(min(len(q_norm), len(row_norm))))
                scores.append((sim, i))
            
            scores.sort(key=lambda x: x[0], reverse=True)
            top_k = scores[:k]
            if top_k:
                scores_arr = np.array([s[0] for s in top_k])
                idx_arr = np.array([s[1] for s in top_k])
            else:
                scores_arr = np.array([])
                idx_arr = np.array([], dtype=np.int64)
            
            return scores_arr.reshape(1, -1), idx_arr.reshape(1, -1)
        
        return scores.reshape(1, -1), idx.reshape(1, -1)
    
    def save(self, path):
        if NUMPY_AVAILABLE:
            np.savez_compressed(path, mat=self.mat)
        else:
            logger.warning(f"âš ï¸ Cannot save index without NumPy: {path}")
    
    @classmethod
    def load(cls, path):
        if NUMPY_AVAILABLE:
            try:
                arr = np.load(path)
                return cls(arr['mat'])
            except Exception as e:
                logger.error(f"Failed to load numpy index: {e}")
                return cls()
        else:
            logger.warning(f"âš ï¸ Cannot load index without NumPy: {path}")
            return cls()

def build_index(force_rebuild: bool = False) -> bool:
    """Build or load FAISS/numpy index"""
    global INDEX, EMBEDDING_MODEL
    
    with INDEX_LOCK:
        # Try to load existing index
        if not force_rebuild:
            if FAISS_ENABLED and HAS_FAISS and os.path.exists(FAISS_INDEX_PATH):
                try:
                    INDEX = faiss.read_index(FAISS_INDEX_PATH)
                    logger.info(f"âœ… Loaded FAISS index from {FAISS_INDEX_PATH}")
                    return True
                except Exception as e:
                    logger.warning(f"Failed to load FAISS index: {e}")
            
            # Try numpy fallback
            if os.path.exists(FALLBACK_VECTORS_PATH):
                try:
                    arr = np.load(FALLBACK_VECTORS_PATH)
                    INDEX = NumpyIndex(arr['mat'])
                    logger.info("âœ… Loaded numpy index")
                    return True
                except Exception as e:
                    logger.warning(f"Failed to load numpy index: {e}")
        
        # Build new index
        if not FLAT_TEXTS:
            logger.warning("No texts to index")
            return False
        
        logger.info(f"ðŸ”¨ Building index for {len(FLAT_TEXTS)} passages...")
        
        # Generate embeddings
        vectors = []
        dims = None
        
        for text in FLAT_TEXTS:
            emb, d = embed_text(text)
            if emb:
                if dims is None:
                    dims = len(emb)
                vectors.append(np.array(emb, dtype="float32"))
        
        if not vectors:
            logger.error("No embeddings generated")
            return False
        
        # Create index
        mat = np.vstack(vectors)
        mat = mat / (np.linalg.norm(mat, axis=1, keepdims=True) + 1e-12)
        
        if FAISS_ENABLED and HAS_FAISS:
            INDEX = faiss.IndexFlatIP(dims)
            INDEX.add(mat)
            try:
                faiss.write_index(INDEX, FAISS_INDEX_PATH)
                logger.info(f"âœ… Saved FAISS index to {FAISS_INDEX_PATH}")
            except Exception as e:
                logger.error(f"Failed to save FAISS index: {e}")
        else:
            INDEX = NumpyIndex(mat)
            try:
                INDEX.save(FALLBACK_VECTORS_PATH)
                logger.info(f"âœ… Saved numpy index to {FALLBACK_VECTORS_PATH}")
            except Exception as e:
                logger.error(f"Failed to save numpy index: {e}")
        
        logger.info(f"âœ… Index built: {len(vectors)} vectors, {dims} dimensions")
        return True

# =========== HELPER FUNCTIONS ===========
def normalize_text_simple(s: str) -> str:
    """Basic text normalization"""
    if not s:
        return ""
    s = s.lower()
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    s = re.sub(r"[^\w\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def get_session_context(session_id: str) -> ConversationContext:
    """Get or create context for session using ConversationContext dataclass with auto-repair"""

    ctx = SESSION_CONTEXTS.get(session_id)

    if ctx is None:
        ctx = ConversationContext(session_id=session_id)
        SESSION_CONTEXTS[session_id] = ctx
        return ctx

    # ===== AUTO REPAIR FOR OLD CONTEXT OBJECTS =====
    if not hasattr(ctx, "last_tour_indices"):
        ctx.last_tour_indices = []

    if not hasattr(ctx, "current_tours"):
        ctx.current_tours = []

    if not hasattr(ctx, "mentioned_tours"):
        ctx.mentioned_tours = set()

    if not hasattr(ctx, "last_successful_tours"):
        ctx.last_successful_tours = []

    if not hasattr(ctx, "conversation_history"):
        ctx.conversation_history = []

    if not hasattr(ctx, "user_preferences"):
        ctx.user_preferences = {}

    return ctx


def _prepare_llm_prompt(user_message: str, search_results: List, context: Dict) -> str:
    """Prepare prompt for LLM vá»›i cáº£i tiáº¿n xá»­ lÃ½ cÃ¢u há»i chung"""
    user_message_lower = user_message.lower()
    
    # Kiá»ƒm tra náº¿u lÃ  cÃ¢u há»i vá» chÃ­nh sÃ¡ch chung
    is_general_policy_question = any(phrase in user_message_lower for phrase in [
        'giÃ¡ tour Ä‘Ã£ bao gá»“m', 'bao gá»“m Äƒn uá»‘ng', 'bao gá»“m xe Ä‘Æ°a Ä‘Ã³n', 
        'bao gá»“m khÃ¡ch sáº¡n', 'Ä‘Ã£ bao gá»“m nhá»¯ng gÃ¬', 'cÃ³ bao gá»“m',
        'Ä‘Ã£ cÃ³ Äƒn uá»‘ng chÆ°a', 'Ä‘Ã£ cÃ³ xe chÆ°a', 'Ä‘Ã£ cÃ³ khÃ¡ch sáº¡n chÆ°a'
    ])
    
    prompt_parts = [
        "Báº¡n lÃ  trá»£ lÃ½ AI thÃ´ng minh cá»§a Ruby Wings - chuyÃªn tÆ° váº¥n du lá»‹ch tráº£i nghiá»‡m.",
        "HÆ¯á»šNG DáºªN QUAN TRá»ŒNG:",
        "1. LUÃ”N sá»­ dá»¥ng thÃ´ng tin tá»« dá»¯ liá»‡u ná»™i bá»™ Ä‘Æ°á»£c cung cáº¥p bÃªn dÆ°á»›i",
        "2. Náº¿u thiáº¿u thÃ´ng tin chi tiáº¿t, tá»•ng há»£p tá»« thÃ´ng tin chung cÃ³ sáºµn",
        "3. KHÃ”NG BAO GIá»œ nÃ³i 'khÃ´ng cÃ³ thÃ´ng tin', 'khÃ´ng biáº¿t', 'khÃ´ng rÃµ'",
        "4. LuÃ´n giá»¯ thÃ¡i Ä‘á»™ nhiá»‡t tÃ¬nh, há»¯u Ã­ch, chuyÃªn nghiá»‡p, thÃ´ng minh",
        "5. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin chÃ­nh xÃ¡c, Ä‘Æ°a ra thÃ´ng tin tá»•ng quÃ¡t dá»±a trÃªn kiáº¿n thá»©c chung vá» tour du lá»‹ch",
        "6. KHÃ”NG tá»± Ã½ bá»‹a thÃ´ng tin khÃ´ng cÃ³ trong dá»¯ liá»‡u",
        "7. Äá»‘i vá»›i cÃ¢u há»i vá» CHÃNH SÃCH CHUNG (giÃ¡ tour bao gá»“m gÃ¬, dá»‹ch vá»¥ Ä‘i kÃ¨m):",
        "   - Chá»‰ cáº§n tráº£ lá»i ngáº¯n gá»n, tá»•ng quÃ¡t, thÃ´ng minh",
        "   - KHÃ”NG cáº§n liá»‡t kÃª táº¥t cáº£ cÃ¡c tour",
        "   - Táº­p trung vÃ o thÃ´ng tin chung tá»« dá»¯ liá»‡u cÃ³ sáºµn",
        "   - Náº¿u cáº§n, Ä‘á» cáº­p ráº±ng cÃ³ thá»ƒ Ä‘iá»u chá»‰nh theo yÃªu cáº§u thá»±c táº¿",
        "8. Äá»‘i vá»›i cÃ¢u há»i vá» TOUR Cá»¤ THá»‚:",
        "   - Tráº£ lá»i chi tiáº¿t vá» tour Ä‘Ã³",
        "   - Chá»‰ liá»‡t kÃª tour khÃ¡c náº¿u cáº§n so sÃ¡nh hoáº·c Ä‘á» xuáº¥t",
        "",
        "THÃ”NG TIN NGá»® Cáº¢NH:",
    ]
    
    if context.get('user_preferences'):
        prefs = []
        if context['user_preferences'].get('duration_pref'):
            prefs.append(f"ThÃ­ch tour {context['user_preferences']['duration_pref']}")
        if context['user_preferences'].get('interests'):
            prefs.append(f"Quan tÃ¢m: {', '.join(context['user_preferences']['interests'])}")
        if prefs:
            prompt_parts.append(f"- Sá»Ÿ thÃ­ch ngÆ°á»i dÃ¹ng: {'; '.join(prefs)}")
    
    if context.get('current_tours'):
        tours_info = []
        for tour in context['current_tours']:
            tours_info.append(f"{tour['name']} ({tour.get('duration', '?')})")
        if tours_info:
            prompt_parts.append(f"- Tour Ä‘ang tháº£o luáº­n: {', '.join(tours_info)}")
    
    if context.get('filters'):
        filters = context['filters']
        filter_strs = []
        if filters.get('price_max'):
            filter_strs.append(f"giÃ¡ dÆ°á»›i {filters['price_max']:,} VND")
        if filters.get('price_min'):
            filter_strs.append(f"giÃ¡ trÃªn {filters['price_min']:,} VND")
        if filters.get('location'):
            filter_strs.append(f"Ä‘á»‹a Ä‘iá»ƒm: {filters['location']}")
        if filter_strs:
            prompt_parts.append(f"- Bá»™ lá»c: {', '.join(filter_strs)}")
    
    prompt_parts.append("")
    prompt_parts.append("Dá»® LIá»†U Ná»˜I Bá»˜ RUBY WINGS:")
    
    if search_results:
        # Æ¯u tiÃªn hiá»ƒn thá»‹ thÃ´ng tin vá» dá»‹ch vá»¥ bao gá»“m
        includes_results = []
        other_results = []
        
        for score, passage in search_results:
            text = passage.get('text', '').lower()
            path = passage.get('path', '').lower()
            
            # Æ¯u tiÃªn thÃ´ng tin vá» includes, meals, accommodation, transport
            if any(keyword in text or keyword in path for keyword in 
                  ['includes', 'bao gá»“m', 'Äƒn uá»‘ng', 'meal', 'accommodation', 
                   'khÃ¡ch sáº¡n', 'hotel', 'transport', 'xe', 'Ä‘Æ°a Ä‘Ã³n']):
                includes_results.append((score, passage))
            else:
                other_results.append((score, passage))
        
        # Hiá»ƒn thá»‹ thÃ´ng tin dá»‹ch vá»¥ bao gá»“m trÆ°á»›c
        displayed_count = 0
        max_display = 5
        
        for i, (score, passage) in enumerate(includes_results[:max_display], 1):
            text = passage.get('text', '')[:300]
            prompt_parts.append(f"\n[{i}] (Äá»™ liÃªn quan: {score:.2f}) - Dá»‹ch vá»¥ bao gá»“m")
            prompt_parts.append(f"{text}")
            displayed_count += 1
        
        # Hiá»ƒn thá»‹ cÃ¡c káº¿t quáº£ khÃ¡c
        remaining_slots = max_display - displayed_count
        for i, (score, passage) in enumerate(other_results[:remaining_slots], displayed_count + 1):
            text = passage.get('text', '')[:300]
            prompt_parts.append(f"\n[{i}] (Äá»™ liÃªn quan: {score:.2f})")
            prompt_parts.append(f"{text}")
    else:
        prompt_parts.append("KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u liÃªn quan trá»±c tiáº¿p.")
    
    prompt_parts.append("")
    prompt_parts.append("HÆ¯á»šNG DáºªN TRáº¢ Lá»œI Äáº¶C BIá»†T:")
    
    if is_general_policy_question:
        prompt_parts.append("ðŸ”¹ ÄÃ‚Y LÃ€ CÃ‚U Há»ŽI Vá»€ CHÃNH SÃCH CHUNG:")
        prompt_parts.append("1. Tráº£ lá»i NGáº®N Gá»ŒN, Tá»”NG QUÃT vá» chÃ­nh sÃ¡ch giÃ¡ tour bao gá»“m")
        prompt_parts.append("2. KHÃ”NG liá»‡t kÃª táº¥t cáº£ cÃ¡c tour")
        prompt_parts.append("3. Táº­p trung vÃ o thÃ´ng tin chung: Äƒn uá»‘ng, xe Ä‘Æ°a Ä‘Ã³n, khÃ¡ch sáº¡n")
        prompt_parts.append("4. Äá» cáº­p ráº±ng cÃ³ thá»ƒ Ä‘iá»u chá»‰nh theo yÃªu cáº§u thá»±c táº¿")
        prompt_parts.append("5. Káº¿t thÃºc báº±ng lá»i má»i liÃªn há»‡ hotline Ä‘á»ƒ biáº¿t chi tiáº¿t cá»¥ thá»ƒ")
    else:
        prompt_parts.append("1. Tráº£ lá»i dá»±a trÃªn dá»¯ liá»‡u trÃªn")
        prompt_parts.append("2. Náº¿u cÃ³ thÃ´ng tin tá»« dá»¯ liá»‡u, trÃ­ch dáº«n nÃ³")
        prompt_parts.append("3. Giá»¯ cÃ¢u tráº£ lá»i rÃµ rÃ ng, há»¯u Ã­ch")
        prompt_parts.append("4. Káº¿t thÃºc báº±ng lá»i má»i liÃªn há»‡ hotline 0332510486 náº¿u cáº§n thÃªm thÃ´ng tin")
    
    prompt_parts.append("")
    prompt_parts.append("TRáº¢ Lá»œI Cá»¦A Báº N (báº±ng tiáº¿ng Viá»‡t, thÃ¢n thiá»‡n, chuyÃªn nghiá»‡p):")
    
    return "\n".join(prompt_parts)

def _prepare_llm_prompt(user_message: str, search_results: List, context: Dict) -> str:
    """Prepare prompt for LLM"""
    prompt_parts = [
        # ... ná»™i dung hiá»‡n táº¡i ...
    ]
    
    return "\n".join(prompt_parts)


# =========== THÃŠM HÃ€M Má»šI ===========
def _generate_fallback_response(user_message: str, search_results: List, tour_indices: List[int] = None) -> str:
    """Generate fallback response when LLM is unavailable"""
    message_lower = user_message.lower()
    
    # 1. Xá»­ lÃ½ cÃ¢u há»i vá» chÃ­nh sÃ¡ch bao gá»“m (Æ¯U TIÃŠN)
    if any(phrase in message_lower for phrase in [
        'giÃ¡ tour Ä‘Ã£ bao gá»“m', 'bao gá»“m Äƒn uá»‘ng', 'bao gá»“m xe Ä‘Æ°a Ä‘Ã³n', 
        'bao gá»“m khÃ¡ch sáº¡n', 'Ä‘Ã£ bao gá»“m nhá»¯ng gÃ¬', 'cÃ³ bao gá»“m',
        'Ä‘Ã£ cÃ³ Äƒn uá»‘ng chÆ°a', 'Ä‘Ã£ cÃ³ xe chÆ°a', 'Ä‘Ã£ cÃ³ khÃ¡ch sáº¡n chÆ°a'
    ]):
        return "ThÃ´ng thÆ°á»ng, giÃ¡ tour Ruby Wings Ä‘Ã£ bao gá»“m cÃ¡c dá»‹ch vá»¥ cÆ¡ báº£n nhÆ°:\n" \
               "â€¢ Ä‚n uá»‘ng theo chÆ°Æ¡ng trÃ¬nh\n" \
               "â€¢ Xe Ä‘Æ°a Ä‘Ã³n trong suá»‘t hÃ nh trÃ¬nh\n" \
               "â€¢ KhÃ¡ch sáº¡n/chá»— á»Ÿ tiÃªu chuáº©n\n\n" \
               "Tuy nhiÃªn, Ä‘á»ƒ biáº¿t chÃ­nh xÃ¡c dá»‹ch vá»¥ bao gá»“m trong tá»«ng tour cá»¥ thá»ƒ, " \
               "vui lÃ²ng liÃªn há»‡ hotline **0332510486** Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n chi tiáº¿t vÃ  " \
               "Ä‘iá»u chá»‰nh theo yÃªu cáº§u riÃªng cá»§a báº¡n! ðŸ˜Š"
    
    # 2. Xá»­ lÃ½ cÃ¢u há»i vá» giÃ¡
    if 'dÆ°á»›i' in message_lower and ('triá»‡u' in message_lower or 'tiá»n' in message_lower):
        if not tour_indices and TOURS_DB:
            all_tours = list(TOURS_DB.items())[:3]
            response = "Dá»±a trÃªn yÃªu cáº§u cá»§a báº¡n, tÃ´i Ä‘á» xuáº¥t cÃ¡c tour cÃ³ giÃ¡ há»£p lÃ½:\n"
            for idx, tour in all_tours:
                tour_name = tour.name or f'Tour #{idx}'
                price = tour.price or 'LiÃªn há»‡ Ä‘á»ƒ biáº¿t giÃ¡'
                response += f"â€¢ **{tour_name}**: {price}\n"
            response += "\nðŸ’¡ *LiÃªn há»‡ hotline 0332510486 Ä‘á»ƒ biáº¿t giÃ¡ chÃ­nh xÃ¡c vÃ  Æ°u Ä‘Ã£i*"
            return response
    
    # 3. Xá»­ lÃ½ khi khÃ´ng cÃ³ káº¿t quáº£ tÃ¬m kiáº¿m
    if not search_results:
        if tour_indices and TOURS_DB:
            response = "ThÃ´ng tin vá» tour báº¡n quan tÃ¢m:\n"
            for idx in tour_indices[:2]:
                tour = TOURS_DB.get(idx)
                if tour:
                    response += f"\n**{tour.name or f'Tour #{idx}'}**\n"
                    if tour.duration:
                        response += f"â±ï¸ {tour.duration}\n"
                    if tour.location:
                        response += f"ðŸ“ {tour.location}\n"
                    if tour.price:
                        response += f"ðŸ’° {tour.price}\n"
            response += "\nðŸ’¡ *LiÃªn há»‡ hotline 0332510486 Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t*"
            return response
        else:
            return "Xin lá»—i, hiá»‡n khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong dá»¯ liá»‡u. " \
                   "Vui lÃ²ng liÃªn há»‡ hotline 0332510486 Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n trá»±c tiáº¿p."
    
    # 4. Xá»­ lÃ½ cÃ³ káº¿t quáº£ tÃ¬m kiáº¿m
    top_results = search_results[:3]
    response_parts = ["TÃ´i tÃ¬m tháº¥y má»™t sá»‘ thÃ´ng tin liÃªn quan:"]
    
    for i, (score, passage) in enumerate(top_results, 1):
        text = passage.get('text', '')[:150]
        if text:
            response_parts.append(f"\n{i}. {text}")
    
    response_parts.append("\nðŸ’¡ *LiÃªn há»‡ hotline 0332510486 Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t*")
    
    return "".join(response_parts)


# =========== MAIN CHAT ENDPOINT WITH ALL UPGRADES ===========
@app.route("/chat", methods=["POST"])
def chat_endpoint():
    """
    Main chat endpoint with all 10 upgrades integrated
    """
    start_time = time.time()
    
    try:
        data = request.get_json() or {}
        user_message = (data.get("message") or "").strip()
        
        if not user_message:
            return jsonify({
                "reply": "Xin chÃ o! TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n vá» cÃ¡c tour cá»§a Ruby Wings?",
                "sources": [],
                "context": {},
                "processing_time": 0
            })
        
        session_id = extract_session_id(data, request.remote_addr)
        context = get_session_context(session_id)
        
        # Kiá»ƒm tra náº¿u lÃ  cÃ¢u há»i vá» chÃ­nh sÃ¡ch chung
        user_message_lower_check = user_message.lower()
        is_general_policy_question = any(phrase in user_message_lower_check for phrase in [
            'giÃ¡ tour Ä‘Ã£ bao gá»“m', 'bao gá»“m Äƒn uá»‘ng', 'bao gá»“m xe Ä‘Æ°a Ä‘Ã³n', 
            'bao gá»“m khÃ¡ch sáº¡n', 'Ä‘Ã£ bao gá»“m nhá»¯ng gÃ¬', 'cÃ³ bao gá»“m',
            'Ä‘Ã£ cÃ³ Äƒn uá»‘ng chÆ°a', 'Ä‘Ã£ cÃ³ xe chÆ°a', 'Ä‘Ã£ cÃ³ khÃ¡ch sáº¡n chÆ°a',
            'tour Ä‘Ã£ bao gá»“m', 'Ä‘Ã£ bao gá»“m gÃ¬ trong giÃ¡', 'giÃ¡ Ä‘Ã£ bao gá»“m nhá»¯ng gÃ¬'
        ])
        
        # Náº¿u lÃ  cÃ¢u há»i vá» chÃ­nh sÃ¡ch chung, Æ°u tiÃªn xá»­ lÃ½ Ä‘áº·c biá»‡t
        if is_general_policy_question:
            logger.info("ðŸŽ¯ PhÃ¡t hiá»‡n cÃ¢u há»i chÃ­nh sÃ¡ch chung, xá»­ lÃ½ Ä‘áº·c biá»‡t")
            
            # TÃ¬m thÃ´ng tin vá» dá»‹ch vá»¥ bao gá»“m
            includes_keywords = ['includes', 'bao gá»“m', 'Äƒn uá»‘ng', 'meal', 'accommodation', 
                               'khÃ¡ch sáº¡n', 'transport', 'xe', 'Ä‘Æ°a Ä‘Ã³n']
            
            # TÃ¬m kiáº¿m táº­p trung vÃ o thÃ´ng tin bao gá»“m
            includes_results = []
            for keyword in includes_keywords:
                keyword_results = query_index(keyword, top_k=3)
                includes_results.extend(keyword_results)
            
            # Loáº¡i bá» trÃ¹ng láº·p
            unique_includes = []
            seen_texts = set()
            for score, passage in includes_results:
                text = passage.get('text', '')
                if text and text not in seen_texts:
                    seen_texts.add(text)
                    unique_includes.append((score, passage))
            
            # Táº¡o cÃ¢u tráº£ lá»i thÃ´ng minh
            if unique_includes:
                # TrÃ­ch xuáº¥t thÃ´ng tin chung
                services_included = []
                for score, passage in unique_includes[:5]:
                    text = passage.get('text', '')
                    if 'bao gá»“m' in text.lower() or 'includes' in text.lower():
                        services_included.append(text[:150])
                
                if services_included:
                    reply = "ThÃ´ng thÆ°á»ng, giÃ¡ tour Ruby Wings Ä‘Ã£ bao gá»“m:\n\n"
                    for i, service in enumerate(services_included[:3], 1):
                        reply += f"â€¢ {service}\n"
                    
                    reply += "\nTuy nhiÃªn, tuá»³ vÃ o tá»«ng tour cá»¥ thá»ƒ vÃ  yÃªu cáº§u thá»±c táº¿, " \
                            "chÃºng tÃ´i cÃ³ thá»ƒ Ä‘iá»u chá»‰nh cÃ¡c dá»‹ch vá»¥ bao gá»“m cho phÃ¹ há»£p.\n\n" \
                            "ðŸ’¡ *Äá»ƒ biáº¿t chÃ­nh xÃ¡c dá»‹ch vá»¥ bao gá»“m trong tour báº¡n quan tÃ¢m, " \
                            "vui lÃ²ng liÃªn há»‡ hotline 0332510486 Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n chi tiáº¿t!*"
                else:
                    reply = "ThÃ´ng thÆ°á»ng, cÃ¡c tour cá»§a Ruby Wings Ä‘Ã£ bao gá»“m Ä‘áº§y Ä‘á»§ dá»‹ch vá»¥ " \
                           "nhÆ° Äƒn uá»‘ng, xe Ä‘Æ°a Ä‘Ã³n vÃ  chá»— á»Ÿ. Tuy nhiÃªn, tuá»³ vÃ o tá»«ng tour cá»¥ thá»ƒ " \
                           "vÃ  yÃªu cáº§u thá»±c táº¿, chÃºng tÃ´i cÃ³ thá»ƒ Ä‘iá»u chá»‰nh cho phÃ¹ há»£p.\n\n" \
                           "ðŸ’¡ *Vui lÃ²ng liÃªn há»‡ hotline 0332510486 Ä‘á»ƒ biáº¿t chÃ­nh xÃ¡c " \
                           "dá»‹ch vá»¥ bao gá»“m trong tour báº¡n quan tÃ¢m!*"
            else:
                reply = "GiÃ¡ tour Ruby Wings thÆ°á»ng Ä‘Ã£ bao gá»“m cÃ¡c dá»‹ch vá»¥ cÆ¡ báº£n nhÆ°:\n" \
                       "â€¢ Ä‚n uá»‘ng theo chÆ°Æ¡ng trÃ¬nh\n" \
                       "â€¢ Xe Ä‘Æ°a Ä‘Ã³n trong suá»‘t hÃ nh trÃ¬nh\n" \
                       "â€¢ KhÃ¡ch sáº¡n/chá»— á»Ÿ tiÃªu chuáº©n\n\n" \
                       "Tuy nhiÃªn, Ä‘á»ƒ biáº¿t chÃ­nh xÃ¡c dá»‹ch vá»¥ bao gá»“m trong tá»«ng tour cá»¥ thá»ƒ, " \
                       "vui lÃ²ng liÃªn há»‡ hotline **0332510486** Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n chi tiáº¿t vÃ  " \
                       "Ä‘iá»u chá»‰nh theo yÃªu cáº§u riÃªng cá»§a báº¡n! ðŸ˜Š"
            
            # Bá» qua xá»­ lÃ½ thÃ´ng thÆ°á»ng, tráº£ vá» cÃ¢u tráº£ lá»i Ä‘áº·c biá»‡t
            processing_time = time.time() - start_time
            
            chat_response = ChatResponse(
                reply=reply,
                sources=[],
                context={
                    "session_id": session_id,
                    "special_handling": "general_policy_question",
                    "processing_time_ms": int(processing_time * 1000)
                },
                tour_indices=[],
                processing_time_ms=int(processing_time * 1000),
                from_memory=False
            )
            
            return jsonify(chat_response.to_dict())
        
        # Check memory cache
        recent_response = None
        if hasattr(context, 'get_recent_response') and hasattr(context, 'check_recent_question'):
            recent_response = context.get_recent_response(user_message)
            if recent_response and context.check_recent_question(user_message):
                logger.info("ðŸ’­ Using cached response from recent conversation")
                processing_time = time.time() - start_time
                chat_response = ChatResponse(
                    reply=recent_response,
                    sources=[],
                    context={
                        "session_id": session_id,
                        "from_memory": True,
                        "processing_time_ms": int(processing_time * 1000)
                    },
                    tour_indices=[],
                    processing_time_ms=int(processing_time * 1000),
                    from_memory=True
                )
                return jsonify(chat_response.to_dict())
        
        # ... [pháº§n xá»­ lÃ½ bÃ¬nh thÆ°á»ng tiáº¿p theo] ...
        
        # Initialize state machine
        if UpgradeFlags.is_enabled("7_STATE_MACHINE"):
            if not hasattr(context, 'state_machine') or context.state_machine is None:
                context.state_machine = ConversationStateMachine(session_id)
        
        state_tour_indices = []
        if UpgradeFlags.is_enabled("7_STATE_MACHINE") and context.state_machine:
            state_tour_indices = context.state_machine.extract_reference(user_message)
            if state_tour_indices:
                logger.info(f"ðŸ”„ State machine injected tours: {state_tour_indices}")
                context.last_tour_indices = state_tour_indices
        
        # UPGRADE 5: COMPLEX QUERY SPLITTER
        sub_queries = []
        if UpgradeFlags.is_enabled("5_QUERY_SPLITTER"):
            sub_queries = ComplexQueryProcessor.split_query(user_message)
        
        # UPGRADE 1: MANDATORY FILTER EXTRACTION
        mandatory_filters = FilterSet()
        if UpgradeFlags.is_enabled("1_MANDATORY_FILTER"):
            mandatory_filters = MandatoryFilterSystem.extract_filters(user_message)
            
            if not mandatory_filters.is_empty() and TOURS_DB:
                filtered_indices = MandatoryFilterSystem.apply_filters(TOURS_DB, mandatory_filters)
                if filtered_indices:
                    if state_tour_indices:
                        combined = [idx for idx in state_tour_indices if idx in filtered_indices]
                        context.last_tour_indices = combined if combined else filtered_indices
                    else:
                        context.last_tour_indices = filtered_indices
                    logger.info(f"ðŸ” Applied mandatory filters")
        
        # UPGRADE 6: FUZZY MATCHING
        fuzzy_matches = []
        if UpgradeFlags.is_enabled("6_FUZZY_MATCHING"):
            fuzzy_matches = FuzzyMatcher.find_similar_tours(user_message, TOUR_NAME_TO_INDEX)
            if fuzzy_matches:
                fuzzy_indices = [idx for idx, _ in fuzzy_matches]
                logger.info(f"ðŸ” Fuzzy matches found: {fuzzy_indices}")
                
                if context.last_tour_indices:
                    context.last_tour_indices = list(set(context.last_tour_indices + fuzzy_indices))
                else:
                    context.last_tour_indices = fuzzy_indices
        
        # UPGRADE 3: ENHANCED FIELD DETECTION
        requested_field = None
        field_confidence = 0.0
        if UpgradeFlags.is_enabled("3_ENHANCED_FIELDS"):
            requested_field, field_confidence, _ = EnhancedFieldDetector.detect_field_with_confidence(user_message)
        
        # UPGRADE 4: QUESTION CLASSIFICATION
        question_type = QuestionType.INFORMATION
        question_confidence = 0.0
        question_metadata = {}
        
        if UpgradeFlags.is_enabled("4_QUESTION_PIPELINE"):
            question_type, question_confidence, question_metadata = QuestionPipeline.classify_question(user_message)
        
        # UPGRADE 8: SEMANTIC ANALYSIS
        user_profile = UserProfile()
        if UpgradeFlags.is_enabled("8_SEMANTIC_ANALYSIS"):
            current_profile = getattr(context, 'user_profile', None)
            user_profile = SemanticAnalyzer.analyze_user_profile(user_message, current_profile)
            context.user_profile = user_profile
        
        # UPGRADE 7: STATE MACHINE PROCESSING
        if UpgradeFlags.is_enabled("7_STATE_MACHINE") and context.state_machine:
            placeholder_response = "Processing your request..."
            context.state_machine.update(user_message, placeholder_response, tour_indices if 'tour_indices' in locals() else [])
        
        # TOUR RESOLUTION
        tour_indices = context.last_tour_indices or []
        
        # Handle comparison questions
        if question_type == QuestionType.COMPARISON and not tour_indices:
            comparison_tour_names = []
            name_patterns = [
                r'tour\s+([^\s,]+)\s+vÃ \s+tour\s+([^\s,]+)',
                r'tour\s+([^\s,]+)\s+vá»›i\s+tour\s+([^\s,]+)',
            ]
            
            for pattern in name_patterns:
                matches = re.finditer(pattern, user_message.lower())
                for match in matches:
                    for i in range(1, 3):
                        if match.group(i):
                            tour_name = match.group(i).strip()
                            for norm_name, idx in TOUR_NAME_TO_INDEX.items():
                                if tour_name in norm_name or FuzzyMatcher.normalize_vietnamese(tour_name) in norm_name:
                                    comparison_tour_names.append(idx)
                                    break
            
            if len(comparison_tour_names) >= 2:
                tour_indices = comparison_tour_names[:2]
                context.last_tour_indices = tour_indices
                logger.info(f"ðŸ” Extracted tours for comparison: {tour_indices}")
        
        # Check cache
        cache_key = None
        if UpgradeFlags.get_all_flags().get("ENABLE_CACHING", True):
            context_hash = hashlib.md5(json.dumps({
                'tour_indices': tour_indices,
                'field': requested_field,
                'question_type': question_type.value,
                'filters': mandatory_filters.to_dict()
            }, sort_keys=True).encode()).hexdigest()
            
            cache_key = CacheSystem.get_cache_key(user_message, context_hash)
            cached_response = CacheSystem.get(cache_key)
            
            if cached_response:
                logger.info("ðŸ’¾ Using cached response")
                return jsonify(cached_response)
        
        # PROCESS BY QUESTION TYPE
        reply = ""
        sources = []
        
        # GREETING
        if question_type == QuestionType.GREETING:
            reply = TemplateSystem.render('greeting')
        
        # FAREWELL
        elif question_type == QuestionType.FAREWELL:
            reply = TemplateSystem.render('farewell')
        
        # COMPARISON
        elif question_type == QuestionType.COMPARISON:
            if len(tour_indices) >= 2:
                comparison_result = QuestionPipeline.process_comparison_question(
                    tour_indices, TOURS_DB, "", question_metadata
                )
                reply = comparison_result
            else:
                if TOURS_DB:
                    all_tours = list(TOURS_DB.items())
                    if len(all_tours) >= 2:
                        tour1_idx, tour1 = all_tours[0]
                        tour2_idx, tour2 = all_tours[1]
                        reply = f"Báº¡n cÃ³ thá»ƒ so sÃ¡nh:\n1. {tour1.name or f'Tour #{tour1_idx}'}\n2. {tour2.name or f'Tour #{tour2_idx}'}\n\nHÃ£y cho tÃ´i biáº¿t báº¡n muá»‘n so sÃ¡nh tour nÃ o cá»¥ thá»ƒ."
                    else:
                        reply = "Hiá»‡n chá»‰ cÃ³ 1 tour trong há»‡ thá»‘ng, khÃ´ng thá»ƒ so sÃ¡nh."
                else:
                    reply = "Báº¡n muá»‘n so sÃ¡nh tour nÃ o vá»›i nhau? Vui lÃ²ng nÃªu tÃªn 2 tour trá»Ÿ lÃªn."
        
        # RECOMMENDATION
        elif question_type == QuestionType.RECOMMENDATION:
            # QUAN TRá»ŒNG: Chá»‰ chuyá»ƒn sang COMPARISON khi cÃ³ rÃµ rÃ ng tá»« "so sÃ¡nh" 
            # KHÃ”NG chuyá»ƒn khi cÃ³ "phÃ¹ há»£p vá»›i", "tour nÃ o vá»›i", etc.
            if 'so sÃ¡nh' in user_message.lower() and 'phÃ¹ há»£p' not in user_message.lower():
                question_type = QuestionType.COMPARISON
                if not tour_indices and TOURS_DB:
                    tour_indices = list(TOURS_DB.keys())[:2]
                    reply = f"TÃ´i tháº¥y báº¡n muá»‘n so sÃ¡nh. Báº¡n cÃ³ thá»ƒ so sÃ¡nh:\n1. {TOURS_DB[tour_indices[0]].name or f'Tour #{tour_indices[0]}'}\n2. {TOURS_DB[tour_indices[1]].name or f'Tour #{tour_indices[1]}'}"
                else:
                    reply = "Báº¡n muá»‘n so sÃ¡nh tour nÃ o vá»›i nhau?"
            elif UpgradeFlags.is_enabled("8_SEMANTIC_ANALYSIS"):
                profile_matches = SemanticAnalyzer.match_tours_to_profile(
                    user_profile, TOURS_DB, max_results=3
                )
                
                if profile_matches:
                    recommendations = []
                    for idx, score, reasons in profile_matches:
                        tour = TOURS_DB.get(idx)
                        if tour:
                            recommendations.append({
                                'name': tour.name or f'Tour #{idx}',
                                'score': score,
                                'reasons': reasons,
                                'duration': tour.duration or '',
                                'location': tour.location or '',
                                'price': tour.price or '',
                            })
                    
                    if recommendations:
                        reply = TemplateSystem.render('recommendation',
                            top_tour=recommendations[0] if recommendations else None,
                            other_tours=recommendations[1:] if len(recommendations) > 1 else [],
                            criteria=user_profile.to_summary()
                        )
                    else:
                        reply = "Hiá»‡n chÆ°a tÃ¬m tháº¥y tour phÃ¹ há»£p vá»›i yÃªu cáº§u cá»§a báº¡n."
                else:
                    reply = "Xin lá»—i, tÃ´i chÆ°a hiá»ƒu rÃµ báº¡n cáº§n tour nhÆ° tháº¿ nÃ o. " \
                           "Báº¡n cÃ³ thá»ƒ nÃ³i cá»¥ thá»ƒ hÆ¡n vá» sá»Ÿ thÃ­ch vÃ  yÃªu cáº§u cá»§a mÃ¬nh khÃ´ng?"
            else:
                if TOURS_DB:
                    top_tours = list(TOURS_DB.items())[:2]
                    reply = "Dá»±a trÃªn thÃ´ng tin hiá»‡n cÃ³, tÃ´i Ä‘á» xuáº¥t báº¡n tham kháº£o:\n"
                    for idx, tour in top_tours:
                        reply += f"â€¢ {tour.name or f'Tour #{idx}'}\n"
                    reply += "\nðŸ’¡ Báº¡n cÃ³ thá»ƒ há»i chi tiáº¿t vá» tá»«ng tour cá»¥ thá»ƒ."
                else:
                    reply = "Hiá»‡n chÆ°a cÃ³ thÃ´ng tin tour Ä‘á»ƒ Ä‘á» xuáº¥t."
        
        # LISTING
        elif question_type == QuestionType.LISTING or requested_field == "tour_name":
            all_tours = []
            for idx, tour in TOURS_DB.items():
                all_tours.append(tour)
            
            # UPGRADE 2: DEDUPLICATION
            if UpgradeFlags.is_enabled("2_DEDUPLICATION") and all_tours:
                seen_names = set()
                unique_tours = []
                for tour in all_tours:
                    name = tour.name
                    if name not in seen_names:
                        seen_names.add(name)
                        unique_tours.append(tour)
                all_tours = unique_tours
            
            all_tours = all_tours[:15]
            
            # UPGRADE 10: TEMPLATE SYSTEM
            if UpgradeFlags.is_enabled("10_TEMPLATE_SYSTEM"):
                reply = TemplateSystem.render('tour_list', tours=all_tours)
            else:
                if all_tours:
                    reply = "âœ¨ **Danh sÃ¡ch tour Ruby Wings:** âœ¨\n\n"
                    for i, tour in enumerate(all_tours[:10], 1):
                        reply += f"{i}. **{tour.name or f'Tour #{i}'}**\n"
                        if tour.duration:
                            reply += f"   â±ï¸ {tour.duration}\n"
                        if tour.location:
                            reply += f"   ðŸ“ {tour.location}\n"
                        reply += "\n"
                    reply += "ðŸ’¡ *Há»i chi tiáº¿t vá» báº¥t ká»³ tour nÃ o báº±ng cÃ¡ch nháº­p tÃªn tour*"
                else:
                    reply = "Hiá»‡n chÆ°a cÃ³ thÃ´ng tin tour trong há»‡ thá»‘ng."
        
        # FIELD-SPECIFIC QUERY
        elif requested_field and field_confidence > 0.3:
            if tour_indices:
                field_info = []
                for idx in tour_indices:
                    tour = TOURS_DB.get(idx)
                    if tour:
                        field_value = getattr(tour, requested_field, None)
                        if field_value:
                            if isinstance(field_value, list):
                                field_text = "\n".join([f"â€¢ {item}" for item in field_value])
                            else:
                                field_text = field_value
                            
                            tour_name = tour.name or f'Tour #{idx}'
                            field_info.append(f"**{tour_name}**:\n{field_text}")
                
                if field_info:
                    reply = "\n\n".join(field_info)
                    field_passages = get_passages_by_field(requested_field, tour_indices=tour_indices)
                    sources = [m for _, m in field_passages]
                else:
                    reply = f"KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin vá» {requested_field} cho tour Ä‘Ã£ chá»n."
            else:
                field_passages = get_passages_by_field(requested_field, limit=5)
                if field_passages:
                    field_texts = [m.get('text', '') for _, m in field_passages]
                    reply = "**ThÃ´ng tin chung:**\n" + "\n".join([f"â€¢ {text}" for text in field_texts[:3]])
                    sources = [m for _, m in field_passages]
                else:
                    reply = f"Hiá»‡n khÃ´ng cÃ³ thÃ´ng tin vá» {requested_field} trong dá»¯ liá»‡u."
        
        # DEFAULT: SEMANTIC SEARCH + LLM
        else:
            search_results = query_index(user_message, TOP_K)
            
            # UPGRADE 2: DEDUPLICATION
            if UpgradeFlags.is_enabled("2_DEDUPLICATION") and search_results:
                search_results = DeduplicationEngine.deduplicate_passages(search_results)
            
            # Prepare context for LLM
            current_tours = []
            if tour_indices:
                for idx in tour_indices[:2]:
                    tour = TOURS_DB.get(idx)
                    if tour:
                        current_tours.append({
                            'index': idx,
                            'name': tour.name or f'Tour #{idx}',
                            'duration': tour.duration or '',
                            'location': tour.location or '',
                            'price': tour.price or '',
                        })
            
            # Prepare prompt
            prompt = _prepare_llm_prompt(user_message, search_results, {
                'user_message': user_message,
                'tour_indices': tour_indices,
                'question_type': question_type.value,
                'requested_field': requested_field,
                'user_preferences': getattr(context, 'user_preferences', {}),
                'current_tours': current_tours,
                'filters': mandatory_filters.to_dict()
            })
            
            # Get LLM response
            if client and HAS_OPENAI:
                try:
                    messages = [
                        {"role": "system", "content": prompt},
                        {"role": "user", "content": user_message}
                    ]
                    
                    response = client.chat.completions.create(
                        model=CHAT_MODEL,
                        messages=messages,
                        temperature=0.2,
                        max_tokens=800,
                        top_p=0.95
                    )
                    
                    if response.choices and len(response.choices) > 0:
                        reply = response.choices[0].message.content or ""
                    else:
                        reply = "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ táº¡o pháº£n há»“i ngay lÃºc nÃ y."
                
                except Exception as e:
                    logger.error(f"OpenAI API error: {e}")
                    reply = _generate_fallback_response(user_message, search_results, tour_indices)
            else:
                reply = _generate_fallback_response(user_message, search_results, tour_indices)
            
            sources = [m for _, m in search_results]
        
        # UPGRADE 9: AUTO-VALIDATION
        if UpgradeFlags.is_enabled("9_AUTO_VALIDATION"):
            reply = (lambda _v: _v if _v is not None else reply)(safe_validate(reply))
        
        # Update context
        context.last_action = "chat_response"
        context.timestamp = datetime.utcnow()
        
        if tour_indices and tour_indices[0] in TOURS_DB:
            tour = TOURS_DB[tour_indices[0]]
            context.last_tour_name = tour.name
        
        # Update state machine
        if UpgradeFlags.is_enabled("7_STATE_MACHINE") and context.state_machine:
            context.state_machine.update(user_message, reply, tour_indices)
        
        # Add to memory
        if hasattr(context, 'add_to_history'):
            context.add_to_history(user_message, reply)
        
        # Prepare response
        processing_time = time.time() - start_time
        
        chat_response = ChatResponse(
            reply=reply,
            sources=sources,
            context={
                "session_id": session_id,
                "last_tour_name": getattr(context, 'last_tour_name', None),
                "user_preferences": getattr(context, 'user_preferences', {}),
                "question_type": question_type.value,
                "requested_field": requested_field,
                "processing_time_ms": int(processing_time * 1000),
                "from_memory": False
            },
            tour_indices=tour_indices,
            processing_time_ms=int(processing_time * 1000),
            from_memory=False
        )
        
        response_data = chat_response.to_dict()
        
        # Cache the response
        if cache_key and UpgradeFlags.get_all_flags().get("ENABLE_CACHING", True):
            CacheSystem.set(cache_key, response_data)
        
        logger.info(f"â±ï¸ Processing time: {processing_time:.2f}s | "
                   f"Question: {question_type.value} | "
                   f"Tours: {len(tour_indices)} | "
                   f"Reply length: {len(reply)}")
        
        return jsonify(response_data)
    
    except Exception as e:
        logger.error(f"âŒ Chat endpoint error: {e}\n{traceback.format_exc()}")
        
        processing_time = time.time() - start_time
        
        return jsonify({
            "reply": "Xin lá»—i, cÃ³ lá»—i xáº£y ra khi xá»­ lÃ½ yÃªu cáº§u cá»§a báº¡n. "
                    "Vui lÃ²ng thá»­ láº¡i sau hoáº·c liÃªn há»‡ hotline 0332510486.",
            "sources": [],
            "context": {
                "error": str(e),
                "processing_time_ms": int(processing_time * 1000)
            }
        }), 500

# =========== OTHER ENDPOINTS ===========
@app.route("/")
def home():
    """Home endpoint"""
    return jsonify({
        "status": "ok",
        "version": "4.0",
        "upgrades": UpgradeFlags.get_all_flags(),
        "services": {
            "openai": "available" if client else "unavailable",
            "faiss": "available" if HAS_FAISS else "unavailable",
            "google_sheets": "available" if HAS_GOOGLE_SHEETS else "unavailable",
            "meta_capi": "available" if HAS_META_CAPI else "unavailable",
        },
        "counts": {
            "tours": len(TOURS_DB),
            "passages": len(FLAT_TEXTS),
            "tour_names": len(TOUR_NAME_TO_INDEX),
        }
    })

@app.route("/reindex", methods=["POST"])
def reindex():
    """Rebuild index endpoint"""
    secret = request.headers.get("X-RBW-ADMIN", "")
    if not secret and os.environ.get("RBW_ALLOW_REINDEX", "") != "1":
        return jsonify({"error": "reindex not allowed"}), 403
    
    load_knowledge()
    build_index(force_rebuild=True)
    
    return jsonify({
        "ok": True,
        "count": len(FLAT_TEXTS),
        "tours": len(TOURS_DB)
    })

# =========== GOOGLE SHEETS INTEGRATION ===========
_gsheet_client = None
_gsheet_client_lock = threading.Lock()

def get_gspread_client(force_refresh: bool = False):
    """Get Google Sheets client"""
    global _gsheet_client
    
    if not GOOGLE_SERVICE_ACCOUNT_JSON:
        logger.error("GOOGLE_SERVICE_ACCOUNT_JSON not set")
        return None
    
    with _gsheet_client_lock:
        if _gsheet_client is not None and not force_refresh:
            return _gsheet_client
        
        try:
            info = json.loads(GOOGLE_SERVICE_ACCOUNT_JSON)
            scopes = [
                "https://www.googleapis.com/auth/spreadsheets",
                "https://www.googleapis.com/auth/drive",
            ]
            creds = Credentials.from_service_account_info(info, scopes=scopes)
            _gsheet_client = gspread.authorize(creds)
            logger.info("âœ… Google Sheets client initialized")
            return _gsheet_client
        except Exception as e:
            logger.error(f"âŒ Google Sheets client failed: {e}")
            return None

@app.route('/api/save-lead', methods=['POST', 'OPTIONS'])
def save_lead():
    """Save lead from form submission - FIXED vá»›i Ä‘áº§y Ä‘á»§ 9 trÆ°á»ng KHÃ”NG Lá»–I"""
    if request.method == 'OPTIONS':
        return jsonify({'status': 'ok'}), 200
    
    try:
        data = request.get_json() or {}
        
        # Extract data vá»›i Ä‘áº§y Ä‘á»§ trÆ°á»ng
        phone = data.get('phone', '').strip()
        name = data.get('name', '').strip()
        email = data.get('email', '').strip()
        tour_interest = data.get('tour_interest', '').strip()
        page_url = data.get('page_url', request.referrer or '')
        note = data.get('note', '').strip()
        
        if not phone:
            return jsonify({'error': 'Phone number is required'}), 400
        
        # Clean phone
        phone_clean = re.sub(r'[^\d+]', '', phone)
        
        # Validate phone
        if not re.match(r'^(0|\+?84)\d{9,10}$', phone_clean):
            return jsonify({'error': 'Invalid phone number format'}), 400
        
        # XÃ¡c Ä‘á»‹nh source channel vÃ  action type
        source_channel = 'Website Form'
        action_type = 'Lead Submission'
        raw_status = 'New'
        
        # Táº¡o lead data
        lead_data = {
            'timestamp': datetime.now().isoformat(),
            'phone': phone_clean,
            'name': name,
            'email': email,
            'tour_interest': tour_interest,
            'source': 'Lead Form',
            'page_url': page_url,
            'note': note,
            'source_channel': source_channel,
            'action_type': action_type,
            'raw_status': raw_status
        }
        
        # Send to Meta CAPI
        if ENABLE_META_CAPI_CALL and HAS_META_CAPI:
            try:
                result = send_meta_lead(
                    request,
                    phone=phone_clean,
                    contact_name=name,
                    email=email,
                    content_name=f"Tour: {tour_interest}" if tour_interest else "General Inquiry",
                    value=200000,
                    currency="VND"
                )
                increment_stat('meta_capi_calls')
                logger.info(f"âœ… Form lead sent to Meta CAPI: {phone_clean[:4]}***")
                if DEBUG and HAS_META_CAPI:
                    logger.debug(f"Meta CAPI result: {result}")
            except Exception as e:
                increment_stat('meta_capi_errors')
                logger.error(f"Meta CAPI error: {e}")
        
        # Save to Google Sheets vá»›i Ä‘áº§y Ä‘á»§ 9 cá»™t CHÃNH XÃC
        if ENABLE_GOOGLE_SHEETS:
            try:
                import gspread
                from google.oauth2.service_account import Credentials
                
                if GOOGLE_SERVICE_ACCOUNT_JSON and GOOGLE_SHEET_ID:
                    creds_json = json.loads(GOOGLE_SERVICE_ACCOUNT_JSON)
                    creds = Credentials.from_service_account_info(
                        creds_json,
                        scopes=['https://www.googleapis.com/auth/spreadsheets']
                    )
                    
                    gc = gspread.authorize(creds)
                    sh = gc.open_by_key(GOOGLE_SHEET_ID)
                    ws = sh.worksheet(GOOGLE_SHEET_NAME)
                    
                    # ===== CHUáº¨N Bá»Š DÃ’NG Vá»šI 9 Cá»˜T CHÃNH XÃC =====
                    # Cá»™t A -> I: created_at, source_channel, action_type, page_url, contact_name, phone, service_interest, note, raw_status
                    row = [
                        lead_data['timestamp'],           # A: created_at (timestamp)
                        source_channel,                   # B: source_channel
                        action_type,                      # C: action_type
                        page_url[:100],                   # D: page_url (giá»›i háº¡n Ä‘á»™ dÃ i)
                        name[:50],                        # E: contact_name (giá»›i háº¡n Ä‘á»™ dÃ i)
                        phone_clean,                      # F: phone
                        tour_interest[:100],              # G: service_interest (giá»›i háº¡n Ä‘á»™ dÃ i)
                        note[:200],                       # H: note (giá»›i háº¡n Ä‘á»™ dÃ i)
                        raw_status                        # I: raw_status (status)
                    ]
                    
                    # LOG Ä‘á»ƒ debug
                    logger.info(f"ðŸ“Š Preparing to save lead with {len(row)} columns")
                    logger.info(f"ðŸ“‹ Row data: {row}")
                    
                    # Ghi vÃ o sheet
                    ws.append_row(row)
                    logger.info("âœ… Form lead saved to Google Sheets vá»›i 9 cá»™t chÃ­nh xÃ¡c")
                    
                    # XÃ¡c minh dÃ²ng cuá»‘i cÃ¹ng
                    all_records = ws.get_all_values()
                    last_row = all_records[-1] if all_records else []
                    logger.info(f"ðŸ“ Last row in sheet has {len(last_row)} columns: {last_row}")
                    
            except Exception as e:
                logger.error(f"âŒ Google Sheets error: {e}")
                logger.error(f"âŒ Error details: {traceback.format_exc()}")
        
        # Fallback storage
        if ENABLE_FALLBACK_STORAGE:
            try:
                if os.path.exists(FALLBACK_STORAGE_PATH):
                    with open(FALLBACK_STORAGE_PATH, 'r', encoding='utf-8') as f:
                        leads = json.load(f)
                else:
                    leads = []
                
                leads.append(lead_data)
                leads = leads[-1000:]
                
                with open(FALLBACK_STORAGE_PATH, 'w', encoding='utf-8') as f:
                    json.dump(leads, f, ensure_ascii=False, indent=2)
                
                logger.info("âœ… Form lead saved to fallback storage")
            except Exception as e:
                logger.error(f"Fallback storage error: {e}")
        
        # Update stats
        increment_stat('leads')
        
        return jsonify({
            'success': True,
            'message': 'Lead Ä‘Ã£ Ä‘Æ°á»£c lÆ°u! Äá»™i ngÅ© Ruby Wings sáº½ liÃªn há»‡ sá»›m nháº¥t. ðŸ“ž',
            'data': {
                'phone': phone_clean[:3] + '***' + phone_clean[-2:],
                'timestamp': lead_data['timestamp'],
                'columns_saved': 9
            }
        })
        
    except Exception as e:
        logger.error(f"âŒ Save lead error: {e}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/call-button', methods=['POST', 'OPTIONS'])
def call_button():
    """Track call button click"""
    if request.method == 'OPTIONS':
        return jsonify({'status': 'ok'}), 200
    
    try:
        data = request.get_json() or {}
        
        page_url = data.get('page_url', '')
        call_type = data.get('call_type', 'regular')
        
        # Send to Meta CAPI
        if ENABLE_META_CAPI_CALL and HAS_META_CAPI:
            try:
                result = send_meta_call_button(
                    request,
                    page_url=page_url,
                    call_type=call_type,
                    button_location='fixed_bottom_left',
                    button_text='Gá»i ngay'
                )
                increment_stat('meta_capi_calls')
                logger.info(f"ðŸ“ž Call button tracked: {call_type}")
                if DEBUG and HAS_META_CAPI:
                    logger.debug(f"Meta CAPI result: {result}")
            except Exception as e:
                increment_stat('meta_capi_errors')
                logger.error(f"Meta CAPI call error: {e}")
        
        return jsonify({
            'success': True,
            'message': 'Call tracked',
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Call button error: {e}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    try:
        return jsonify({
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "services": {
                "chatbot": "running",
                "openai": "available" if client else "unavailable",
                "faiss": "available" if INDEX else "unavailable",
                "tours_db": len(TOURS_DB),
                "upgrades": {k: v for k, v in UpgradeFlags.get_all_flags().items() 
                           if k.startswith("UPGRADE_")}
            },
            "memory_profile": {
                "ram_profile": RAM_PROFILE,
                "is_low_ram": IS_LOW_RAM,
                "is_high_ram": IS_HIGH_RAM,
                "tour_count": len(TOURS_DB),
                "context_count": len(SESSION_CONTEXTS)
            }
        })
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "error": str(e)
        }), 500

# =========== INITIALIZATION ===========
def initialize_app():
    """Initialize the application"""
    logger.info("ðŸš€ Starting Ruby Wings Chatbot v4.0 (Dataclass Rewrite)...")
    
    # Apply memory optimizations
    optimize_for_memory_profile()
    
    # Load knowledge base
    load_knowledge()
    
    # Load or build tours database
    if os.path.exists(FAISS_MAPPING_PATH):
        try:
            with open(FAISS_MAPPING_PATH, 'r', encoding='utf-8') as f:
                MAPPING[:] = json.load(f)
            FLAT_TEXTS[:] = [m.get('text', '') for m in MAPPING]
            logger.info(f"ðŸ“ Loaded {len(MAPPING)} mappings from disk")
        except Exception as e:
            logger.error(f"Failed to load mappings: {e}")
    
    # Build tour databases
    index_tour_names()
    build_tours_db()
    
    # Build index in background
    def build_index_background():
        time.sleep(2)
        success = build_index(force_rebuild=False)
        if success:
            logger.info("âœ… Index ready")
        else:
            logger.warning("âš ï¸ Index building failed")
    
    threading.Thread(target=build_index_background, daemon=True).start()
    
    # Initialize Google Sheets client
    if ENABLE_GOOGLE_SHEETS:
        threading.Thread(target=get_gspread_client, daemon=True).start()
    
    # Log active upgrades
    active_upgrades = [name for name, enabled in UpgradeFlags.get_all_flags().items() 
                      if enabled and name.startswith("UPGRADE_")]
    logger.info(f"ðŸ”§ Active upgrades: {len(active_upgrades)}")
    for upgrade in active_upgrades:
        logger.info(f"   â€¢ {upgrade}")
    
    # Log memory profile
    logger.info(f"ðŸ§  Memory Profile: {RAM_PROFILE}MB | Low RAM: {IS_LOW_RAM} | High RAM: {IS_HIGH_RAM}")
    logger.info(f"ðŸ“Š Tours Database: {len(TOURS_DB)} tours loaded")
    
    logger.info("âœ… Application initialized successfully with dataclasses")

# =========== APPLICATION START ===========
if __name__ == "__main__":
    initialize_app()
    
    # Save mappings if not exists
    if MAPPING and not os.path.exists(FAISS_MAPPING_PATH):
        try:
            with open(FAISS_MAPPING_PATH, 'w', encoding='utf-8') as f:
                json.dump(MAPPING, f, ensure_ascii=False, indent=2)
            logger.info(f"ðŸ’¾ Saved mappings to {FAISS_MAPPING_PATH}")
        except Exception as e:
            logger.error(f"Failed to save mappings: {e}")
    
    # Start server
    logger.info(f"ðŸŒ Starting server on {HOST}:{PORT}")
    app.run(host=HOST, port=PORT, debug=DEBUG, threaded=True)

else:
    # For WSGI
    initialize_app()